# Mathematical derivations {#math}

This section regroups optional derivations which are provided for the sake of completeness.

## Derivation of the ordinary least squares estimator {#ols}

Consider the optimization problem
\begin{align*}
\widehat{\boldsymbol{\beta}}=\min_{\boldsymbol{\beta} \in \mathbb{R}^{p+1}}(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})^\top(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta}).
\end{align*}
We can compute the derivative of the right hand side with respect to $\boldsymbol{\beta}$, set it to zero and solve for $\widehat{\boldsymbol{\beta}}$,  
\begin{align*}
\mathbf{0}_n&=\frac{\partial}{\partial\boldsymbol{\beta}}(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})^\top(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})\\
\\&=\frac{\partial (\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})}{\partial \boldsymbol{\beta}}\frac{\partial (\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})^\top(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})}{\partial (\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})}\\
 \\&=\mathbf{X}^\top (\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})
\end{align*}
using the [chain rule](http://www.stat.rice.edu/~dobelman/notes_papers/math/Matrix.Calculus.AppD.pdf). Distributing the terms leads to the so-called *normal equation*
\begin{align*}
 \mathbf{X}^\top \mathbf{X}\boldsymbol{\beta}&=\mathbf{X}^\top \boldsymbol{y}.
\end{align*}
If the $n \times p$ matrix  $\mathbf{X}$ is full-rank, the quadratic form $\mathbf{X}^\top \mathbf{X}$ is invertible and we obtain the solution to the least square problems provided in Equation \@ref(eq:ols).

## Derivation of the coefficient of determination {#derivationR2}

Because of the orthogonal decomposition $\boldsymbol{y}=\widehat{\boldsymbol{y}} + \boldsymbol{e}$ and provided that the design matrix includes an intercept of $\mathbf{1}_n \in \mathcal{S}(\mathbf{X})$, then $\overline{\boldsymbol{e}}=0$ and the average of the response and of the fitted values is the same. Since $n^{-1}\sum_{i=1}^n \hat{y}_i = n^{-1}\sum_{i=1}^n ({y}_i-e_i)=\overline{y}$,
\begin{align*}
\widehat{\mathsf{Cor}}\left(\hat{\boldsymbol{y}}, \boldsymbol{y}\right)
&= \frac{(\boldsymbol{y} - \overline{y}\mathbf{1}_n)^\top(\widehat{\boldsymbol{y}} - \overline{y}\mathbf{1}_n)}
{\|\boldsymbol{y} - \overline{y}\mathbf{1}_n\|\|\widehat{\boldsymbol{y}} - \overline{y}\mathbf{1}_n\|}
\\&= \frac{(\widehat{\boldsymbol{y}} - \overline{y}\mathbf{1}_n)^\top(\widehat{\boldsymbol{y}} - \overline{y}\mathbf{1}_n) +
\boldsymbol{e}^\top(\widehat{\boldsymbol{y}} - \overline{y}\mathbf{1}_n)}
{\|\boldsymbol{y} - \overline{y}\mathbf{1}_n\|\|\widehat{\boldsymbol{y}} - \overline{y}\mathbf{1}_n\|}
\\&= \frac{\|\widehat{\boldsymbol{y}} - \overline{y}\mathbf{1}_n\|}
{\|\boldsymbol{y} - \overline{y}\mathbf{1}_n\|}
\\&= \frac{\|\boldsymbol{y} - \overline{y}\mathbf{1}_n\| - \|\boldsymbol{e}\|}
{\|\boldsymbol{y} - \overline{y}\mathbf{1}_n\|}
\\&= \sqrt{\frac{\mathsf{SS}_c-\mathsf{SS}_e}{\mathsf{SS}_c}}= \mathrm{R}.
\end{align*}
This justifies the claim of [Section 2.5](#coefR2) that the squared correlation between the fitted values and the response is equal to $R^2$.