# Mathematical derivations {#math}

This section regroups optional derivations which are provided for the sake of completeness.

## Derivation of the ordinary least squares estimator {#ols}

Consider the optimization problem
\begin{align*}
\widehat{\boldsymbol{\beta}}=\min_{\boldsymbol{\beta} \in \mathbb{R}^{p+1}}(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})^\top(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta}).
\end{align*}
We can compute the derivative of the right hand side with respect to $\boldsymbol{\beta}$, set it to zero and solve for $\widehat{\boldsymbol{\beta}}$,  
\begin{align*}
\mathbf{0}_n&=\frac{\partial}{\partial\boldsymbol{\beta}}(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})^\top(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})\\
\\&=\frac{\partial (\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})}{\partial \boldsymbol{\beta}}\frac{\partial (\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})^\top(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})}{\partial (\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})}\\
 \\&=\mathbf{X}^\top (\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})
\end{align*}
using the [chain rule](http://www.stat.rice.edu/~dobelman/notes_papers/math/Matrix.Calculus.AppD.pdf). Distributing the terms leads to the so-called *normal equation*
\begin{align*}
 \mathbf{X}^\top \mathbf{X}\boldsymbol{\beta}&=\mathbf{X}^\top \boldsymbol{y}.
\end{align*}
If the $n \times p$ matrix  $\mathbf{X}$ is full-rank, the quadratic form $\mathbf{X}^\top \mathbf{X}$ is invertible and we obtain the solution to the least square problems provided in Equation \@ref(eq:ols).

## Derivation of the coefficient of determination {#derivationR2}

Because of the orthogonal decomposition $\boldsymbol{y}=\widehat{\boldsymbol{y}} + \boldsymbol{e}$ and provided that the design matrix includes an intercept of $\mathbf{1}_n \in \mathcal{S}(\mathbf{X})$, then $\overline{\boldsymbol{e}}=0$ and the average of the response and of the fitted values is the same. Since $n^{-1}\sum_{i=1}^n \widehat{y}_i = n^{-1}\sum_{i=1}^n ({y}_i-e_i)=\overline{y}$,
\begin{align*}
\widehat{\mathsf{Cor}}\left(\widehat{\boldsymbol{y}}, \boldsymbol{y}\right)
&= \frac{(\boldsymbol{y} - \overline{y}\mathbf{1}_n)^\top(\widehat{\boldsymbol{y}} - \overline{y}\mathbf{1}_n)}
{\|\boldsymbol{y} - \overline{y}\mathbf{1}_n\|\|\widehat{\boldsymbol{y}} - \overline{y}\mathbf{1}_n\|}
\\&= \frac{(\widehat{\boldsymbol{y}} - \overline{y}\mathbf{1}_n)^\top(\widehat{\boldsymbol{y}} - \overline{y}\mathbf{1}_n) +
\boldsymbol{e}^\top(\widehat{\boldsymbol{y}} - \overline{y}\mathbf{1}_n)}
{\|\boldsymbol{y} - \overline{y}\mathbf{1}_n\|\|\widehat{\boldsymbol{y}} - \overline{y}\mathbf{1}_n\|}
\\&= \frac{\|\widehat{\boldsymbol{y}} - \overline{y}\mathbf{1}_n\|}
{\|\boldsymbol{y} - \overline{y}\mathbf{1}_n\|}
\\&= \frac{\|\boldsymbol{y} - \overline{y}\mathbf{1}_n\| - \|\boldsymbol{e}\|}
{\|\boldsymbol{y} - \overline{y}\mathbf{1}_n\|}
\\&= \sqrt{\frac{\mathsf{SS}_c-\mathsf{SS}_e}{\mathsf{SS}_c}}= \mathrm{R}.
\end{align*}
This justifies the claim of [Section 2.5](#coefR2) that the squared correlation between the fitted values and the response is equal to $R^2$.

## Restricted estimation maximum likelihood

The most common estimation technique for fitting linear mixed models or fixed effect models in which the variance of the error term is not $\sigma^2\mathbf{I}_n$ is the restricted estimation maximum likelihood (REML) method. 

For the linear mixed model, the marginal distribution of the $n$ response vector is 
$\boldsymbol{Y} \sim \mathsf{No}_n\{\mathbf{X} \boldsymbol{\beta}, \boldsymbol{\Upsilon}^{-1}(\boldsymbol{\psi})\}$, where $\boldsymbol{\Upsilon}^{-1}(\boldsymbol{\psi}) = \mathbf{Z}\boldsymbol{\Omega}\mathbf{Z}^\top + \mathbf{R}$ and $\mathbf{R} = \mathrm{blockdiag}(\mathbf{R}_1, \ldots, \mathbf{R}_m)$, the covariance matrix of the errors, is block-diagonal. The multivariate normal distribution has covariance parameters $\boldsymbol{\psi}$ and mean parameters $\boldsymbol{\beta}$.

The log likelihood for the model is, up to proportionality constants
\begin{align*}
\ell(\boldsymbol{\beta},\boldsymbol{\psi}; \boldsymbol{y}) \propto  \frac{1}{2} \ln |\boldsymbol{\Upsilon}|- \frac{1}{2} (\boldsymbol{y}- \mathbf{X}\boldsymbol{\beta})^\top\boldsymbol{\Upsilon}(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta}) 
\end{align*}

For known $\boldsymbol{\psi}$, the precision matrix $\boldsymbol{\Upsilon}$ is fully resolved; the restricted maximum likelihood for the parameters of the mean vector $\boldsymbol{\beta}$ is
\begin{align*}
\widehat{\boldsymbol{\beta}}_{\boldsymbol{\psi}} = (\mathbf{X}^\top\boldsymbol{\Upsilon}\mathbf{X})^{-1}\mathbf{X}^\top\boldsymbol{\Upsilon}\boldsymbol{y}.
\end{align*}
We can show that $\widehat{\boldsymbol{\beta}}_{\boldsymbol{\psi}} \sim \mathsf{No}_{p+1}\{\boldsymbol{\beta}, (\mathbf{X}^\top\boldsymbol{\Upsilon}\mathbf{X})^{-1}\}$ and thus write the joint density of the response given mean and variance parameters as
\begin{align*}
f(\boldsymbol{y}; \boldsymbol{\psi}, \boldsymbol{\beta}) = f(\boldsymbol{y}; \widehat{\boldsymbol{\beta}}_{\boldsymbol{\psi}}, \boldsymbol{\beta}, \boldsymbol{\psi}) f(\widehat{\boldsymbol{\beta}}_{\boldsymbol{\psi}}; \boldsymbol{\psi}, \boldsymbol{\beta}). 
\end{align*}
Given $\boldsymbol{\psi}$, $\widehat{\boldsymbol{\beta}}_{\boldsymbol{\psi}}$ is a sufficient statistic for $\boldsymbol{\beta}$, the conditional density depends on $\boldsymbol{\beta}$ only through $\widehat{\boldsymbol{\beta}}_{\boldsymbol{\psi}}$, so $f(\boldsymbol{y}; \widehat{\boldsymbol{\beta}}_{\boldsymbol{\psi}}, \boldsymbol{\beta}, \boldsymbol{\psi}) = f(\boldsymbol{y}; \widehat{\boldsymbol{\beta}}_{\boldsymbol{\psi}},  \boldsymbol{\psi})$.
Rather than using the full likelihood, the REML maximizes the conditional log likelihood
\begin{align*}
\ell_{\mathrm{r}}(\boldsymbol{\psi})= \ln f(\boldsymbol{y}; \widehat{\boldsymbol{\beta}}_{\boldsymbol{\psi}},  \boldsymbol{\psi}) = \ln f(\boldsymbol{y}; \boldsymbol{\psi}, \boldsymbol{\beta}) - \ln f(\widehat{\boldsymbol{\beta}}_{\boldsymbol{\psi}}; \boldsymbol{\psi}, \boldsymbol{\beta}).
\end{align*}
The solution thus differs from that of the maximum log likelihood. The log conditional density is
\begin{align*}
\ell_{\mathrm{r}}(\boldsymbol{\psi})
 &= \frac{1}{2} \ln |\boldsymbol{\Upsilon}| - \frac{1}{2} (\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})^\top\boldsymbol{\Upsilon}(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta}) 
 \\& \quad - \frac{1}{2} \ln \left|\mathbf{X}^\top\boldsymbol{\Upsilon}\mathbf{X}\right| + \frac{1}{2} \left(\widehat{\boldsymbol{\beta}}_{\boldsymbol{\psi}}-\boldsymbol{\beta}\right)^\top \mathbf{X}^\top\boldsymbol{\Upsilon}\mathbf{X} \left(\widehat{\boldsymbol{\beta}}_{\boldsymbol{\psi}}-\boldsymbol{\beta}\right)
\end{align*}
and upon simplifying terms using the fact $\boldsymbol{y} - \mathbf{X}\boldsymbol{\beta} = \boldsymbol{y} - \mathbf{X}\widehat{\boldsymbol{\beta}}_{\boldsymbol{\psi}} + \mathbf{X}(\widehat{\boldsymbol{\beta}}_{\boldsymbol{\psi}} - \boldsymbol{\beta})$, we retrieve the REML log likelihood,
\begin{align*}
\ell_{\mathrm{r}}(\boldsymbol{\psi}) = \frac{1}{2}|\boldsymbol{\Upsilon}| - \frac{1}{2} \ln \left|\mathbf{X}^\top\boldsymbol{\Upsilon}\mathbf{X}\right|  - \frac{1}{2}(\boldsymbol{y} -\mathbf{X}\widehat{\boldsymbol{\beta}}_{\boldsymbol{\psi}})^\top \boldsymbol{\Upsilon}(\boldsymbol{y} -\mathbf{X}\widehat{\boldsymbol{\beta}}_{\boldsymbol{\psi}})
\end{align*}
Once we have obtained $\widehat{\boldsymbol{\psi}}_{\mathrm{r}}= \max_{\boldsymbol{\psi}} \ell_{\mathrm{r}}(\boldsymbol{\psi})$, we replace the resulting $\widehat{\boldsymbol{\Upsilon}}$ value in 
$\widehat{\boldsymbol{\beta}}_{\widehat{\boldsymbol{\psi}}_{\mathrm{r}}}$
to get the REML estimates.
Because the estimation of $\boldsymbol{\psi}$ is based on only part of the full likelihood, estimators for the covariance parameter differ from their counterpart based on the full likelihood; in practice, the latter are often more biased, which explains the popularity of REML. The sufficient statistic $\widehat{\boldsymbol{\beta}}_{\widehat{\boldsymbol{\psi}}}$ depends on the particular model matrix $\mathbf{X}$, so models with different fixed effects cannot be compared based on their maximum restricted likelihood values.

It should be noted that optimisation for linear mixed model is challenging for large $n$ and direct matrix inversion must be avoided, as inverting a square $n \times n$ matrix requires $\mathrm{O}(n^3)$ flops. Software for mixed models must use clever tricks to capitalize on the sparsity of $\boldsymbol{\Upsilon}$, which often follows from that of $\mathbf{R}$; oftentimes $\mathbf{R}  = \sigma^2 \mathbf{I}_n$.