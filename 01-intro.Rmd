# Introduction 

The goal of this course is to introduce the fundamental principles of statistical *inference* as well as advanced statistical methods used in business intelligence problems. We will also see applications of these methods using (mostly) simulated and real data applications using dedicated software. The **SAS** material is presented throughout the course slides, whereas **R**  will be presented in this online book.

This introductory chapter touches many topics that may be covered in undergraduate statistics courses, including notions about hypothesis test (null and alternative hypothesis, statistics, null distributions, levels and power of test, confidence interval). 

This chapter also includes a short description of population and samples, basic graphics (scatterplot, violinplots, histograms, box-and-whisker plots) and tests for one-sample location problems (signed test, signed rank test and one-sample and paired _t_-tests). Recommended reading are Chapter 1--4 of the OpenIntro book [ “Introductory Statistics with
Randomization and Simulation”](https://www.openintro.org/stat/textbook.php?stat_book=isrs).

## Statistical inference versus predictions

Statistical inference deals with explanatory models --- used for answering scientific questions, i.e. test research hypotheses concerning variable effects. However, these models can also be used to make predictions.


## Reminders and Objectives

This section covers some basics about population and samples, hypothesis tests and duality between confidence intervals and $p$-values. 

## Population and sample

Statistics is about drawing inference from a large group given a representative subset of limited size; we cannot measure for practical and financial reasons a characteristic such as height to get the average length of a human. Often, it is also unnecessary: conclusions drawn from a sample are good enough.

The **population** is the target group under study. A **sample** is a representative subset of the population for which we measure a characteristic. For example, if we are interested in voting intentions for the next federal election, the population would consist of all Canadian citizens aged 18 or more at the day the vote takes place. Pollsters may select a small group of eligible voters and ask them for their voting preferences: you can get pretty good estimates even when 17.5 millions people vote, as the margin of error of a probabilistic sample with 1000 participants is about 3% with 95% confidence level. 

The population is often defined in the context of a study, but one should be careful in not making this definition too narrow. If you want to do a study of breast cancer, you do not want to define the population to be all patients at a given hospital who received a diagnostic of breast cancer; rather, you should aim to have a broad enough definition so that you can generalize your results.
Choosing a sample is hard, and an entire branch of statistics, survey sampling, deals specifically with this. For opinion polls, pollster often have polls and databases of phone numbers to pick from. What makes it more challenging nowadays is that a large share of individuals do not own a landline anymore (and so their phone number is not public), but calling only cellphones leads to bias because it undersample elderly people; selecting exclusively one or the other leads to bad samples. 



## Testing statistical hypotheses

Hypothesis testing is ubiquitous in statistics, yet often poorly understood. The topic is thus presented in an intuitive manner through examples by making an analogy with trial for murder. We review the formulation of null and alternative hypotheses, the choice of test statistic, derivation of its null distribution and finally conclusions in light of $p$-value. As an illustrative example, we test for equal means for two groups using a permutation test.

The best analogy I have for hypothesis testing is a criminal trial: you are a jury member for a murder case and are asked to deliver a verdict, which will be either *guilty* or *not guilty*, based on evidence presented in court. By default, the presumption of innocence apply so your default position is that the accused is not guilty and you analyze evidences in this optic to avoid judicial errors (do not condemn an innocent to death). Only if the evidences presented by the prosecutor are overwhelming will you declare the person *guilty*.

We can make a parallel with hypothesis testing as follows:

- there are only two possible options considered, which correspond to the null hypothesis $H_0$ (the accused is innocent of the murder) and the alternative \(H_1\) (the accused committed the murder).
- the test statistic comprises all evidences. There are many different choices, but not all of them are good (compare DNA samples with circumstantial evidences).
- rejecting the null hypothesis corresponds to the *guilty* verdict, i.e., reject $H_0$.
- sometimes, there are plenty of evidences presented even when the individual accused of murder is innocent. To minimize the risk of judicial error, we only reject if these are overwhelming, meaning that the probability of happening by chance for an innocent are equal to some tolerance level $\alpha$, specified beforehand. The **level of a test** does not change.
- if we fail to reject the null hypothesis, corresponding to a verdict of *not guilty*, this means either of two things: the person is innocent or we have a reasonable doubt. The jury only weights the evidence in light of the apriori assumption that the accused is innocent unless proven otherwise, so we cannot conclude anything about innocence.
- our ability to correctly declare a murderer guilty is called *power*. The choice of test statistic (evidences) determines how well we can confound murdererers. Power is a probability: the higher, the better. A test with no power is useless. If you have to choose a single evidence from the proof, you might select 'blood stains on the accused's clothes which matches the DNA victim' over 'he happened to be in the city as the victim when the murder was committed'. 

The interplay between Type I and Type II errors are best displayed graphically. Nathaniel Stevens (University of Waterloo) made a nice [Shiny app](https://nathaniel-t-stevens.shinyapps.io/ErrorIllustrator/).
In these plots, $\alpha$ is the significance level (usually 5%) corresponding to the type I error we are willing to commit. The effect size measures the difference between the null hypothesis and the true effect size (think about measuring height of people in two populations, with the null hypothesis being that there is no difference). If there was truly a difference, but the latter was small, say 0.01cm, it would be confounded with the measurement variability. On the other hand, if the true difference is 10cm, it would be pretty obvious.
The main factors affecting power are sample size and variability in the population; the variability of the mean estimate goes down like $\sigma/\sqrt{n}$, so decreases when either (a) $\sigma$ decreases or (b) $n$ increases. With large enough samples, all differences will be detected. The type I error is the surface represented in red ($\alpha$), whereas the type II error (false negative) is $\beta$ in blue. The power is the area of the density in dashed blue; the larger, the more the test is capable of detecting when the null hypothesis is false.

## Exploratory data analysis

Looking at the data is arguably the most important of an analysis. This step consists in looking at summary statistics (mean, quantiles, min and max) and to plot the data, one variable at a time and then pairs. This will allow you to detect outliers (for example, missing values were often encoded with $-1$ or $999$ in the old days when only numerical values could be saved by devices). Real data is messy, and often artefacts (e.g., rounding, ties) are most easily viewed from scatterplot, histograms, box-and-whiskers plots, etc.

- a box-and-whiskers plot (or boxplot), consists of a box composed by the 25, 50 and 75 percentiles. The median separates the box, while the whiskers extend beyond 1.5 times the interquartile range (the length of the box). Values that fall outside of this range are often represented by dots and would be extreme points, if the sample was normally distributed.
- a histogram bins observations and can be useful to detect outliers and multimodality. The number of bins is often selected automatically. 
- a kernel density estimator represents local fit of the density function and is a continuous analog of the histogram if the latter is on the density scale (as opposed to frequency). The kernel density is more sensitive to local deviations and may more easily capture discreteness. The bandwidth controls the amount of neighboring data used for the fit. Optimal (and default) choices are typically selected.


