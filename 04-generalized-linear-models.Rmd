# Generalized linear models {#generalized-linear-models}


Linear models are only suitable for data that are (approximately) normally distributed. However, there are many settings where we may wish to analyze a response variable which is not necessarily continuous, including when $Y$ is binary, a count variable or is continuous, but non-negative. We will consider in particular likelihood-based inference for binary/proportion and counts data.

Generalized linear models (GLM) combine a model for the conditional mean with a distribution for the response variable and a link function tying predictors and parameters.

This chapter gives an introduction to generalized linear models and focuses in particular on logistic regression and Poisson regression, but only for the case of independent observations.  Extensions of generalized linear models for correlated and longitudinal data, the so-called *generalized linear mixed models* (GLMM), are covered in MATH80621.


## Basic principles 

The starting point is the same as for linear regression: we have a random sample of independent observations $(\boldsymbol{Y}, \mathbf{X})$, where $Y$ is the response variable and $\mathrm{X}_1, \ldots, \mathrm{X}_p$ are $p$ explanatory variables or covariates which are assumed fixed (non-random). The goal is to model the mean of the response variable as a function of the explanatory variables.

Let $\mu_i=\mathsf{E}(Y_i \mid \mathbf{X}_i)$ denote the conditional expectation of $Y_i$ given covariates and let $\eta_i$ denote the linear combination of the covariates that will be used to model the response variable, 
\begin{align*}
\eta_i=\beta_0 + \beta_1 \mathrm{X}_{i1} + \cdots + \beta_p \mathrm{X}_{ip}.
\end{align*}
The building blocks of the generalized linear model are

- A random component, consisting of the probability distribution for the outcome $Y$ that is a member of the exponential dispersion family (normal, binomial, Poisson, gamma, \ldots). 
- A deterministic component, the **linear predictor** $\boldsymbol{\eta}=\mathbf{X} \boldsymbol{\beta}$, where $\mathbf{X}$ is an $n\times (p+1)$ matrix with columns $\mathbf{1}_n, \mathbf{X}_1, \ldots, \mathbf{X}_p$ and $\boldsymbol{\beta} \in \mathbb{R}^{p+1}$ are coefficients. 
- A monotone function $g$, called **link function**, that maps the mean of $Y_i$ to the predictor variables, $g(\mu_i)=\eta_i$. 



## Theory of generalized linear models

This section borrows from Chapter 4 of 

> Agpasti (2015). *Foundations of Linear and Generalized Linear Models*, Wiley.



### Exponential dispersion family of distributions

In a generalized linear model, the random component arises from an exponential dispersion family. Consider a probability density or mass function for $Y$ with parameters $(\theta, \phi)$,
\begin{align}
f(y; \theta, \phi)&= \exp \left\{  \frac{y \theta -b(\theta)}{a(\phi)} + c(y,
\phi)\right\}, (\#eq:expofam)
\end{align}
where the support, i.e., the set of values taken by $Y$, doesn't depend on the parameters.  Throughout, we will assume the **natural parameter** $\theta$ is unknown, but the **dispersion parameter** $\phi$ may be known (exponential family) or unknown (exponential dispersion family). 

One particularity of exponential dispersion models is that there exists an explicit relationship between mean and variance of $Y$, which the following derivation shows. the first and second derivative of the log likelihood contribution of $Y_i$, say $\ell_i$, with respect to the natural parameter $\theta$ are 
\begin{align*}
\frac{\partial \ell_i(y; \theta, \phi)}{\partial \theta} &= \frac{\partial}{\partial \theta} \left\{\frac{y\theta-b(\theta)}{a(\phi)} + c(y, \phi) \right\} = \frac{y - b'(\theta)}{a(\phi)}\\
\frac{\partial^2 \ell_i(y; \theta, \phi)}{\partial \theta^2} &= - \frac{b''(\theta)}{a(\phi)},
\end{align*}
where $b'(\cdot)$ and $b''(\cdot)$ are the first two derivatives of $b(\cdot)$ with respect to $\theta$.
Under regularity condition, the [Bartlett identities](https://math.stackexchange.com/q/2027660) hold and 
\begin{align*}
\mathsf{E}\left\{\frac{\partial \ell(y; \theta, \phi)}{\partial \theta}\right\}=0, \qquad - \mathsf{E}\left\{\frac{\partial^2 \ell(y; \theta, \phi)}{\partial \theta^2}\right\} = \left[\mathsf{E}\left\{\frac{\partial \ell(y; \theta, \phi)}{\partial \theta}\right\}\right]^2.
\end{align*}
These two equality give
\begin{align*}
\mathsf{E}(Y_i) &= b'(\theta_i) \\
\mathsf{Va}(Y_i) &= b''(\theta_i)a(\phi_i)
\end{align*}
Often, the term $a_i(\phi)=a_i\phi$, where $\phi$ is constant over all observations and $a_i$ is an observation-specific weight.
The mean of $Y_i$, say $\mu_i$, and the natural parameter $\theta_i$ are related through the equation $\mu_i=b'(\theta_i)$. There is also an explicit relationship with the variance through $V(\mu)=b''(\theta)$, unless $V(\mu)=1$. 


```{example, label="poissonglmexpf", name = "Poisson distribution as exponential family member"}
The mass function of the Poisson distribution is 
\begin{align*}
f(y; \lambda) = \frac{\lambda^y \exp(-\lambda)}{y!} =\exp \left\{ y \ln
(\lambda) -\lambda-\ln(y!)\right\}, \qquad y=0,1, \ldots
\end{align*}
The natural parameter is $\theta =\ln(\lambda)$, the dispersion
parameter $\phi=1$, and $b(\theta)=\exp(\theta)$. Replacing these expressions in the mean-variance formulas, we get $\mathsf{E}(Y)=\exp(\theta)=\mu$ and $\mathsf{Va}(Y)=\exp(\theta)=\mu$, meaning $V(\mu)=\mu$.
```

```{example, label ="binomialglmexpf", name = "Binomial distribution as member of the exponential family"}
We consider the mass function of a binomial distribution, $\mathsf{Bin}(m, \pi)$, with the number of trials $m$ known. The parametrization presented in Example \@ref(exm:binomialdist) is not convenient because the mean and the variance both depend on $m$. We consider thus a different parametrization in which $Y$ represents the proportion of successes, so the mass function takes values in $\{0, 1/m, \ldots, 1\}$ and $mY$ denotes the number of successes. The mass function for $Y$ is then
\begin{align*}
f(y, \pi)&=\exp \left\{ my \ln \left( \frac{\pi}{1-\pi} \right) + m \ln
(1-\pi) + \ln \left[
\binom{m}{my}\right]\right\}\\& =\exp \left\{
\frac{y \ln \left( \frac{\pi}{1-\pi} \right) +  \ln
(1-\pi)}{1/m}+ \ln \left[
\binom{m}{my}\right]\right\}
\end{align*}
Set
\begin{align*}
\theta= \ln \left( \frac{\pi}{1-\pi}\right)
\end{align*}
with $b(\theta) = \ln\{1+\exp(\theta)\}$ and $\phi=m^{-1}$. The expectation and variance are easily derived and 
\begin{align*}
\mathsf{E}(Y)&=\pi = \mathrm{expit}(\theta)=\frac{\exp(\theta)}{1+\exp(\theta)}=\mu
\\\mathsf{Va}(Y) &= \frac{\pi(1-\pi)}{m} = \frac{\mu(1-\mu)}{m} = \phi V(\mu)
\end{align*}
where $V(\mu)=\mu(1-\mu)$.
```

```{example, label ="normalglmexpf", name = "Normal distribution as member of the exponential family"}
We consider a sample consisting of independent normal observations, $Y_i \sim \mathsf{No}(\mu_i, \sigma^2)$, with
\begin{align*}
f(y_i, \mu_i, \sigma^2)&= (2\pi\sigma^2)^{-1/2} \exp \left\{ -\frac{(y_i-\mu_i)}{2\sigma^2}\right\}
\\&= \exp \left[ \frac{y\mu - \mu^2/2}{\sigma^2} - \frac{1}{2} \left\{ \frac{y^2}{\sigma^2} + \ln(2\pi\sigma^2)\right\} \right],
\end{align*}
meaning $\theta=\mu$, $\phi=\sigma^2$ and $a(\phi)=\sigma^2$, $b(\theta)=\theta^2/2$.
```


### Link functions

The link between the mean of $Y$ and the **linear predictor** $\eta$  is
\begin{align*}
g\left\{\mathsf{E}(Y \mid \mathrm{X}_1, \ldots, \mathrm{X}_p)\right\}=\eta = \beta_0 + \beta_1 \mathrm{X}_{1} + \cdots + \beta_p \mathrm{X}_{p}.
\end{align*}
The link function thus connects the mean of the random variable $Y$ to the explanatory variables, $g(\mu_i) = \eta_i = \beta_0 + \sum_{j=1}^p\beta_j\mathrm{X}_{ij}$ and likewise $\mu_i  = g^{-1}(\eta_i)$. If the link function is chosen such that $\theta=\eta$, the link function is termed the canonical link function.

The need for a link function often arises from parameter constraints: for example, the mean $\mu=\pi$ of a Bernoulli distribution is the proportion of successes and  must lie in the interval $(0, 1)$. Similarly, the mean $\mu$ of the Poisson distribution must be positive. For the normal distribution (ordinary linear regression model), the mean is unconstrained and the canonical link function is the identity, $\mu_i = \eta_i$.


An appropriate choice of link function $g$ sets $\mu_i$ equal to a transformation of the linear combination $\eta_i$ so as to avoid imposing parameter constraints on $\boldsymbol{\beta}$. Certain choices of link functions facilitate interpretation or make the likelihood function convenient for optimization.  
For the Bernoulli and binomial distributions, the most common link function $g$ is the logit function, 
\begin{align*}
\mathrm{logit}(\mu)&=
\ln \left( \frac{\mu}{1-\mu} \right) =\ln(\mu) - \ln(1-\mu)= \eta,
\\\mathrm{expit}(\eta)&= \frac{\exp(\eta)}{1+\exp(\eta)}=\mu.
\end{align*}
The inverse link function is the distribution function of the logistic distribution, hence the name **logistic regression**. This choice of link function is far from unique: any quantile function of a continuous random variable supported on $\mathbb{R}$ could be considered. For the Poisson distribution, the canonical link function $g$ is the natural logarithm, $\ln$, with associated inverse link function $\exp$.

Canonical link functions are natural choices because of their nice statistical properties: choosing the canonical link ensures that $\mathbf{X}^\top\boldsymbol{y}$ is a minimal sufficient statistic. Other considerations, such as parameter constraints, can be more important in deciding on the choice of $g$.

```{example, label="inverslinklm", name = "Transforming a nonlinear regression model into a linear model through a link function"}

Consider a linear model for which theory dictates that the mean should be $Y \sim \mathsf{No}\{\alpha_0/(1+\alpha_1x), \sigma^2\}$.

This model is nonlinear in $\alpha_0$, $\alpha_1$. If we opt for the reciprocal link function $g(x)=1/x$, we get $\beta_0+\beta_1 x$, where $\beta_0 = 1/\alpha_0$ and $\beta_1 = \alpha_1/\alpha_0$ provided $\alpha_0 \neq 0$. Since the generalized linear model is estimated using maximum likelihood and the latter are invariant to reparametrization, we get easily the estimated coefficients of interest.
```

### Model adjustment

There is generally no closed-form expression for the maximum likelihood estimators $\widehat{\boldsymbol{\beta}}$ in generalized linear models and the score equation is typically nonlinear in $\boldsymbol{\beta}$ and $\widehat{\boldsymbol{\beta}}$ must be obtained through iterative numerical routines. The key reason for restricting attention to exponential family is because the likelihood equation depends on the response $Y_i$ only through $\mu_i$, $\phi$ and $V(\mu)$. The mean-variance thus characterizes the distribution. 

Starting from Equation \@ref(eq:expofam), we differentiate the log likelihood function $\ell = \sum_{i=1}^n \ell_i$ with respect to $\boldsymbol{\beta}$. For simplicity, we consider each likelihood contribution and coefficient in turn. By the chain rule,
\begin{align*}
\frac{\partial \ell_i}{\partial \beta_j} = \frac{\partial \eta_i}{\partial \beta_j} \frac{\partial \mu_i}{\partial \eta_i} \frac{\partial \theta_i}{\partial \mu_i}\frac{\partial \ell_i}{\partial \theta_i}
\end{align*}
and the earlier derivations show $\partial \ell_i/\partial \theta_i = (y_j-\mu_i)/a_i(\phi)$ and $\partial \mu_i / \partial \theta_i = b''(\theta_i) = \mathsf{Va}(Y_i)/a_i(\phi)$. The derivative of the linear predictor, $\partial \eta_i / \partial \beta_j = \mathrm{X}_{ij}$. The only missing term, $\partial \mu_i/\partial \eta_i$, depends on the choice of link function through $\eta_i = g(\mu_i)$, but it is unity for the canonical link function.

Summing all the likelihood contribution, the score vector $\boldsymbol{U}$ has element 
\begin{align*}
\frac{\partial \ell}{\partial \beta_j} = \sum_{i=1}^n \frac{(y_i-\mu_i)\mathrm{X}_{ij}}{g'(\mu_i)V(\mu_i)a_i(\phi)}, \qquad j=0, \ldots, p.
\end{align*}

Let
\begin{align*}
U(\boldsymbol{\beta}) = \frac{\partial \ell}{\partial \boldsymbol{\beta}}, \qquad j(\boldsymbol{\beta}) = - \frac{\partial^2 \ell}{\partial \boldsymbol{\beta} \partial \boldsymbol{\beta}^\top}
\end{align*}
In general, $\widehat{\boldsymbol{\beta}}$ solves the score equation $U(\widehat{\boldsymbol{\beta}})=\boldsymbol{0}_{p+1}$, so we can use a Newton--Raphson algorithm to derive the maximum likelihood estimator. This amounts to a first order Taylor series approximation of the score $U(\widehat{\boldsymbol{\beta}})$ around  $\boldsymbol{\beta}$,
\begin{align*}
\boldsymbol{0}_{p+1} = U(\widehat{\boldsymbol{\beta}}) \stackrel{\cdot}{=} U(\beta) - j(\boldsymbol{\beta}) (\widehat{\boldsymbol{\beta}}-\boldsymbol{\beta})
\end{align*}
If the $(p+1)$ matrix $j(\boldsymbol{\beta}^{(t)})$ is invertible, we can thus device an iterative algorithm:  starting from some initial value $\boldsymbol{\beta}^{(0)}$, we compute at step $t+1$
\begin{align*}
\boldsymbol{\beta}^{(t+1)} = \boldsymbol{\beta}^{(t)} + j^{-1}(\boldsymbol{\beta}^{(t)})U(\boldsymbol{\beta}^{(t)}).
\end{align*}
and iterate this formula until convergence.
Most softwares implement a variant of this algorithm, in which the negative hessian $j(\boldsymbol{\beta})$ is replaced by its absolute value $i(\boldsymbol{\beta})$: the resulting algorithm is known as Fisher scoring. For generalized linear models, these recursions can be done by repeatedly computing a variance of least squares known as iteratively reweighted least squares. 


<!-- We can find $\widehat{\boldsymbol{\beta}}$ through **iteratively reweighted least squares** (IRLS) procedure. -->

<!-- in which $\mathbf{W} = \mathrm{diag}\{(\partial \mu_i/\partial \eta_i)^2/\}$ -->

<!-- \boldsymbol{U} =  \frac{\partial \ell(\boldsymbol{\beta}; \boldsymbol{y}, \mathbf{X})}{\partial \boldsymbol{\beta}} = \frac{\partial \boldsymbol{\eta}^\top}{\partial \boldsymbol{\beta}} \frac{\partial \boldsymbol{\theta}}{\partial \boldsymbol{\eta}^\top} \frac{\partial \ell}{\partial \boldsymbol{\theta}^\top} -->

<!-- This gives $\mathbf{D} = \mathrm{diag}\{\partial \mu_i/\partial \eta_i\}$ and $\mathbf{W} = \mathrm{diag}\{(\partial \mu_i/\partial \eta_i)^2/\}$ -->
<!-- To be continued... -->

### Likelihood inference

Likelihood inference is straightforward, although some care is needed because the asymptotic distribution of test statistics sometimes depend on the model parameters.


Under regularity conditions, the maximum likelihood estimators of $\boldsymbol{\beta}$ and $\phi$ are jointly normally distributed under mild regularity conditions, \begin{align*}\widehat{\boldsymbol{\beta}} \stackrel{\cdot}{\sim}\mathsf{No}_{p+1}\{\boldsymbol{\beta}, (\mathbf{X}^\top\mathbf{W}\mathbf{X})^{-1}\}
\end{align*}
where $\mathbf{W} = \mathrm{diag}\{(\partial \mu_i/\partial \eta_i)^2/\mathsf{Va}(Y_i)\}$ is a diagonal matrix with elements $w_i=\{g'(\mu_i)^2a_i(\phi)V(\mu_i)\}^{-1}$. 
<!-- This matrix also appears in the formula of the leverage and in the iteratively reweighted least square recursions. -->
Since $\mathbf{W}$ depends on the unknown $\boldsymbol{\beta}$ through the expected values $\mu_i$, we can estimate $\widehat{\mathbf{W}}$ by replacing the unknown coefficients by their maximum likelihood estimates at $\widehat{\boldsymbol{\beta}}$.

```{example, label = "weightpois", name = "Elements of the weight matrix for Poisson data"}
If we consider $Y_i \sim \mathsf{Po}(\mu_i)$ with canonical link function $\mu_i = \exp(\eta_i)$, then $\partial \eta_i / \partial \mu_i = \mu_i^{-1}$ and $w_i=\mu_i$.
```

<!-- The maximum likelihood estimator of the dispersion parameter $\phi$ can have poor properties, so a moment estimator is sometimes used instead. For known $\mu$, $\mathsf{Va}(Y_i) = a_i\phi V(\mu)$ and, since $\phi$ is constant over observations, we can estimate it from the scaled empirical variance, -->
<!-- \begin{align*} -->
<!-- \widehat{\phi}=\frac{1}{n-p} \sum_{i=1}^n \frac{(y_i-\widehat{\mu}_i)^2}{a_iV(\widehat{\mu}_i)}. -->
<!-- \end{align*} -->
<!-- This gives $\widehat{\phi}=X^2/(n-p)$, a scaled version of the Pearson $X^2$ statistic presented in the next section. -->

Since we use maximum likelihood estimation, the theory presented in [Chapter 3](likelihood) readily applies. For example, if we want to compare two nested models and test if $q$ of the $\boldsymbol{\beta}$ are jointly zero (corresponding to no effect for the associated covariates), we can fit both null and restricted models and perform a likelihood ratio test.  Under regularity conditions, the likelihood ratio statistic to compare two nested models will follow the usual $\chi^2_q$ distribution. 


### Goodness-of-fit criteria and residuals

Goodness-of-fit diagnostics often rely on test statistics comparing the fitted model with parameters $\widehat{\boldsymbol{\beta}}$ with a **saturated** model in which there are $n$ parameters for the mean, as many as there are observations --- this amounts to maximizing the log-likelihood contribution of each term $\ell_i$ individually, for which the best value of the linear predictor is denoted $\widetilde{\eta}_i$ (oftentimes, this is when $\widetilde{\mu}_i=y_i$). We can then build a likelihood ratio statistic to compare the saturated model with the fitted model, taking
\begin{align*}
\frac{D}{\phi}= \sum_{i=1}^n 2\{\ell(\widetilde{\eta}_i; y_i) - \ell(\widehat{\eta}_i; y_i)\}.
\end{align*}
The **scaled deviance** $D$ will be small when the quality of the adjustment is roughly the same for both models, whereas large values of $D$ are indicative of poor fit.


```{example, name = "Deviance for common generalized linear models", label="devglm"}
Suppose that the model matrix of the model $\mathbf{X}$ includes an intercept and let $\widehat{\mu}_i$ denote the fitted mean. Then, the deviance for the normal generalized linear model with homoscedastic errors is $D = \sum_{i=1}^n (y_i - \widehat{\mu}_i)^2$, which is the sum of squared residuals. For Poisson data, the saturated model has $\widetilde{\mu}_i=y_i$ and $D= 2\sum_{i=1}^n  y_i \ln(y_i/\widehat{\mu}_i)$.
```

Another alternative is found by looking at score test statistic comparing the saturated model with the postulated model with $p+1$ parameters $\boldsymbol{\beta}$. The Pearson $X^2$ statistic is
\begin{align*}
 X^2= \sum_{i=1}^n \frac{(y_i-\widehat{\mu}_i)}{ V(\widehat{\mu}_i)},
\end{align*}
where both expectation and variance are estimated using the model; for Poisson data, $\phi=1$ and the estimated mean and variance are equal, meaning $\widehat{\mu}_i=V(\widehat{\mu}_i)$. Pearson's $X^2$ statistic measures standardized departures between observations and fitted values.

 In large samples, both Pearson $X^2/\phi$ and the deviance $D/\phi$ follow approximately a $\chi^2_{n-p-1}$ distribution if the model is correct and $\phi$ is known (but this result doesn't hold for binary data or binomial data unless $m$ is very large and $\boldsymbol{\beta}$ are small). If $\phi$ is unknown, we would replace it throughout by an estimate and the same distributional results hold approximatively.


We can use the individual contributions to the deviance and Pearson $X^2$ statistic to build residuals. By considering $D = \sum_{i=1}^n d_i^2$, where 
\begin{align*}
d_i &= \mathrm{sign}(\widetilde{\eta}_i - \widehat{\eta}_i) \{2\ell(\widetilde{\eta}_i; y_i) - 2\ell(\widehat{\eta}_i; y_i)\}^{1/2}
\end{align*} and the calculations simplify upon replacing the formula of the log likelihood for the generic exponential family member,
\begin{align*}
d_i^2=2 \left\{y_i (\widetilde{\theta}_i - \widehat{\theta}_i) - b(\widetilde{\theta}_i) + b(\widehat{\theta}_i)\right\}
\end{align*}
The terms $d_i$ are called deviance residuals, whereas Pearson residuals are based on the score contributions $u_i(\widehat{\beta}) w_i(\widehat{\beta})^{-1/2}$,
where the score statistic $u(\boldsymbol{\beta})$ and the weights $w_i$ are
\begin{align*}
u_i &= \frac{\partial \theta_i}{\partial \eta_i} \frac{\partial \ell_i(\theta_i)}{\partial \theta_i} = \frac{y_i - \mu_i}{g'(\mu_i)a_i(\phi)V(\mu_i)}\\
w_i &= \left(\frac{\partial \theta_i}{\partial \eta_i}\right)^2 \frac{\partial^2 \ell_i(\theta_i)}{\partial \theta_i^2} = \frac{1}{g'(\mu_i)^2 a_i(\phi)V(\mu_i)}
\end{align*}

In practice, these residuals are heteroscedastic and it is better to standardize them by considering instead
\begin{align*}
r_{D_i} = \frac{d_i}{(1-h_{ii})^2}, \qquad r_{P_i} = \frac{u_i(\widehat{\beta})}{\{w_i(\widehat{\beta})(1-h_{ii})\}^{1/2}},
\end{align*}
both are scaled by $(1-h_{ii})^{1/2}$, a formula remniscent of the linear model framework. In the above formulas, the leverage $h_{ii}$ is the $i$th diagonal element of the matrix 
\begin{align*}
\mathbf{H}_{\mathbf{X}} = \mathbf{W}^{1/2}\mathbf{X}(\mathbf{X}^\top\mathbf{W}\mathbf{X})^{-1} \mathbf{X}^\top\mathbf{W}^{1/2};
\end{align*}
since the terms of $\mathbf{W}=\mathrm{diag}\{w_1, \ldots, w_n\}$ depend on the unknown coefficient, the latter is estimated by replacing $\boldsymbol{\beta}$ by $\widehat{\boldsymbol{\beta}}$.

The standardized deviance residuals $\{r_{D_i}\}$ and standardized Pearson residuals $\{r_{P_i}\}$ should have an approximate standard normal distribution in large samples, but their distribution can be skewed. Like in the linear regression, we will work with the **jackknifed deviance residuals** for residual plots
\begin{align*}
r_{J_i} &= \mathrm{sign}(y_i - \widehat{\mu}_i) \left\{ (1-{h_ii})r^2_{D_i} + h_{ii}r^2_{P_i}\right\}^{1/2}
\end{align*}

For ordinary linear regression, both $r_{D_i}$ and $r_{P_i}$ reduce to the standardized residuals $t_i=e_i\{S^2(1-h_{ii})\}^{-1/2}$.

<!-- Residuals are employed in regression models with Poisson, Gamma, and binomial response for $m$ large.  -->


There are clear parallels between generalized linear models and linear models: we have so far derived an analog of residuals and leverage. Collinearity is also an issue for generalized linear model; for the latter, we define the Cook statistic as the change in the deviance, 
\begin{align*}
C = \frac{1}{p} 2\{\ell(\widehat{\boldsymbol{\beta}}) - \ell(\widehat{\boldsymbol{\beta}}_{-j})\},
\end{align*}
where $\widehat{\boldsymbol{\beta}}_{-j}$ is the estimate obtained by dropping the $j$th observation from the sample. This requires fitting $n$ different models, which is computationally prohibitive. In the linear regression, we can calculate the Cook distance from the formula $C_j = (p+1)^{-1}t_i^2h_{ii}/(1-h_{ii})$, where $t_i$ are the standardized residuals defined in the previous section. For generalized linear models, no such expression exists, although a good approximation is $C_j \approx (p+1)^{-1}r_{P_i}^2h_{ii}/(1-h_{ii})$.

Diagnostic plots for generalized linear models are harder to interpret because of the lack of orthogonality. It is customary to plot the jackknife deviance residuals against the linear predictor $\widehat{\eta}_i$, produce normal quantile-quantile plots of standardized deviance residuals and the approximate Cook statistics against the $h_{ii}/(1-h_{ii})$. We will show examples of such plots for particular models.

## Binary responses

Binary response data are commonplace, notably due to their role in classification. We begin this section by discussing interpretation of the model in terms of censored observations, than tackle the interpretation of the parameters for the canonical link function on different scales (probability, odds and log-odds). We pursue with issues due to the nature of the data, including non-regular asymptotics and separation of variable. We also present an example of incorrect marginalization that leads to Simpson's paradox. Models for multi-way contingency data and proportions are covered in a dedicated section.

The logistic model specifies
\begin{align*}
\mathsf{P}(Y_i=1 \mid \mathbf{X}_i) = \pi_i = \frac{\exp(\beta_0 + \beta_1 \mathrm{X}_{i1}+ \cdots + \beta_p\mathrm{X}_{ip})}{1 + \exp(\beta_0 + \beta_1 \mathrm{X}_{i1}+ \cdots + \beta_p\mathrm{X}_{ip})},
\end{align*}
whereas, on the linear predictor scale $\eta_i$, 
\begin{align*}
\eta_i =\log \left(\frac{\pi_i}{1-\pi_i}\right) =  \beta_0 + \beta_1 \mathrm{X}_{i1}+ \cdots + \beta_p\mathrm{X}_{ip}.
\end{align*}

One way to view binary variables is as censored observations: suppose $Z_i = \eta_i + \varepsilon_i$ where $\varepsilon_i$ is an error term with distribution function $F$ and $\eta_i$ is a fixed linear predictor. Suppose that the variables $Z_i$ are censored:we only observe the binary indicators $Y_i = 1$ if $Z_i > 0$ and $Y_i=0$ otherwise, not the $Z_i$ terms themselves. It follows that 
\[
\mathsf{P}(Y_i = 1 \mid \eta_i) = 1-F(-\eta_i)
\]
If $F$ is symmetric around zero, then $F(\eta_i) = 1-F(-\eta_i)$. The choice of the logistic distribution for $F$ gives $\pi_i = \mathrm{expit}(\eta_i)$. Another popular alternative is when $F$ is the distribution function of a standard normal distribution, $F(x) = \Phi(x)$, which is the probit regression model. Both link functions only differ in the tail, but the parameters of the the logistic model are more readily interpreted since the quantile function $F^{-1}$ of the logistic distribution, $\mathrm{logit}(x)$, is available in closed-form.

```{r probitvslogit, echo = FALSE, fig.cap = "Comparison of logistic (black) and probit (dashed blue) inverse link functions (left) for binary data: both are monotone increasing functions of the linear predictor. The logistic distribution has thicker tails and thus smaller peak at zero. The right panel shows the derivative of both inverse link functions."}

g1 <- ggplot(data.frame(x=seq(-10, 10, length.out = 1001L)),
       aes(x)) + 
  stat_function(fun=pnorm, col = "blue", lty = 2) + 
  stat_function(fun = function(x){exp(x)/(1+exp(x))}) +
  labs(y = "P(Y=1)", x = expression(eta))
 g2 <- ggplot(data.frame(x=seq(-10, 10, length.out = 1001L)),
        aes(x)) + 
   stat_function(fun=dnorm) + 
   stat_function(fun = function(x){exp(x)/(1+exp(x))^2}, col = "blue", lty = 2) +
   labs(y = "density", x = expression(eta))
#g1
g1 + g2
```

The logistic regression model is nonlinear in $\boldsymbol{\eta}=\mathbf{X}\boldsymbol{\beta}$ and the left panel of Figure \@ref(fig:probitvslogit) shows how changes in the linear predictor impact the probability of success. As $\eta_i$ increases (for example, if $\mathrm{X}_j$ increases by one unit and $\beta_j$ is positive), so does the estimated probability of success because the function $\mathrm{expit}(x)$ is monotone increasing. This characterization is however not helpful.
If we exponentiate the linear predictor $\eta$, we get 
\begin{align*}
\mathrm{odds}(Y\mid \mathbf{X}) = \frac{\pi(\mathbf{X})}{1-\pi(\mathbf{X})}=\exp(\beta_0+ \beta_1 \mathrm{X}_1 + \cdots + \beta_p\mathrm{X}_p),
\end{align*}
where $\pi(\mathbf{X})/\{1-\pi(\mathbf{X})\}$ are the odds of $\mathsf{P}(Y=1 \mid \mathbf{X})$ (success) relative to $\mathsf{P}(Y=0 \mid\mathbf{X})$ (failure). If $\pi=0.8$, the odds are $\pi/(1-\pi) = 0 .8/0.2 = 4$: on average, we expect four successes for one failure.

On the odds scale, the logistic regression model is multiplicative: for an increase of $\mathrm{X}_j$ of one unit, *ceteris paribus*, the odds are multiplied by $\exp(\beta_j)$. 

```{example, label="gpaexample", name = "GPA score and graduate admission"}
We consider a simple logistic model for graduate admission (yes/no) in universities across the USA as a function of their grade point average (GPA).
```




```{r figgpa, echo = FALSE, cache = TRUE, fig.cap = "Graduate admission probability as a function of grade point averages (GPA) on the log odds (left), odds (middle) and probability of success scale (right). The line indicates fitted value with pointwise 95\\% profile-based confidence intervals." }
gpa_dat <- read.csv("https://t.co/4ufswKa2C6?amp=1")
mod <- glm(admit ~ gpa, 
    data=gpa_dat, 
    family=binomial(link="logit"))
gpa_new <- seq(2.3,4, length.out = 171L)
predint <- sapply(gpa_new, function(xst){
  mod1 <- glm(admit ~ I(gpa-xst), 
    data=gpa_dat, 
    family=binomial(link="logit"))
  return(confint(profile(mod1))[1,])
})
plot_df <- data.frame(gpa = gpa_new,
           pred = predict(mod, newdata = data.frame(gpa = gpa_new)),
           lcl = predint[1,],
           ucl = predint[2,])
expit <- function(x){1/(1+exp(-x))}
g1 <- ggplot(data = plot_df, aes(x = gpa)) + 
  geom_ribbon(aes(ymin = lcl, ymax = ucl), alpha = 0.2) + 
  geom_line(aes(y = pred), col = "blue") + 
  geom_rug(data = gpa_dat,  outside = FALSE) + 
  labs(y = "log odds")
g2 <- ggplot(data = plot_df, aes(x = gpa)) + 
  geom_ribbon(aes(ymin = exp(lcl), ymax = exp(ucl)), alpha = 0.2) + 
  geom_line(aes(y = exp(pred)), col = "blue") + 
  geom_rug(data = gpa_dat,  outside = FALSE) + 
  labs(y = "odds")
g3 <- ggplot(data = plot_df, aes(x = gpa)) + 
  geom_ribbon(aes(ymin = expit(lcl), ymax = expit(ucl)), alpha = 0.2) + 
  geom_line(aes(y = expit(pred)), col = "blue") + 
  geom_rug(data = gpa_dat,  outside = FALSE) + 
  labs(y = "P(Y=1)")
g1 + g2 + g3
```

```{r gpatable, cache = TRUE, echo = FALSE}
gpa_dat <- read.csv("https://t.co/4ufswKa2C6?amp=1")
mod <- glm(admit ~ gpa, 
    data=gpa_dat, 
    family=binomial(link="logit"))
names(mod$coefficients)[2] <- c("gpa")
dust(mod) %>%
  sprinkle(col = 2:3, round = 2) %>%
  sprinkle(col = 4, round = 2) %>% 
  sprinkle(col = 5, fn = quote(pvalString(value))) %>% 
  sprinkle_colnames(estimate = "estimate", 
                     std.error = "std. error",
                     statistic = "Wald stat.", 
                     p.value = "p-value") %>% 
  sprinkle_bookdown() %>%
  knitr::kable(align = "lrrrr", booktabs = TRUE, caption = "Logistic regression: table of coefficients for the graduate admission data with gpa as only covariate.")
```

The coefficients of the simple logistic model are reported in Table \@ref(tab:gpatable), showing that the log odds of admission increase by `r round(coef(mod)[2]/10,2)` for each increase in GPA of $0.1$. This translates into $\exp(0.1\widehat{\beta}_{\mathrm{gpa}})=`r round(exp(coef(mod)[2]/10),2)`$ on the odds ratio scale, meaning a `r round(100*(round(exp(coef(mod)[2]/10),2)-1),1)`% increase in odds of admission for every $0.1$ increase in GPA. Of course, this model is simplistic and omits other important factors... Figure \@ref(fig:figgpa) shows the variation on the log odds, odds and probability scale as a function of grade point average. While the probability scale is the most intuitive, the effect of an increase of $0.1$ unit of GPA isn't the same, depending on what is your starting value: according to the model, the absolute increase in the probability of admission for individuals with GPA scores $2.3$ and $2.4$  is
`r round(100*expit(with(plot_df, pred[which(sapply(gpa, function(i){isTRUE(all.equal(i,2.4))}))]))- 100*expit(with(plot_df, pred[gpa == 2.3])),1)`%, whereas that corresponding to an increase in GPA from 3.9 to 4 is `r round(100*(expit(with(plot_df, pred[gpa == 4]))-expit(with(plot_df, pred[gpa == 3.9]))), 1)`% ; this illustrates how the impact of an increase of $0.1$ points is not the same across the board.



```{example label="birthweightex", name="Risk factors related to underweight babies at birth"}
We consider a medical example related to risk factors explaining the birth weight: the birth weight is dichotomized and classified as low if the infant weight is inferior to 2.5 kg (1) and 0 otherwise. The data were collected in a medical center in Massachusetts in 1986.
```

The data contains the following explanatory variables

- `age`: mother's age in years.
- `lwt`: mother's weight in pounds at last menstrual period.
- `race`: mother's race (`white`, `black` or `other`).
- `smoke`: smoking status during pregnancy (`yes`/`no`).
- `ptl`: previous premature labours (`none`/`at least one`).
- `ht`: history of hypertension.
- `ui`: presence of uterine irritability.

```{r birthweighteda, echo = FALSE, cache = TRUE, fig.cap = "Exploratory data analysis for the birthweight data"}
data(birthwt, package = "MASS")
# prepare data
birthwt$low <- as.factor(birthwt$low)
birthwt$race <- as.factor(birthwt$race); levels(birthwt$race) <- c('white','black','other')
birthwt$smoke <- as.factor(birthwt$smoke); levels(birthwt$smoke) <- c('no','yes')
birthwt$ht <- as.factor(birthwt$ht); levels(birthwt$ht) <- c('no','yes')
birthwt$ptl <- as.factor(birthwt$ptl>0); levels(birthwt$ptl) <- c('none','at least one')
birthwt$ui <- as.factor(birthwt$ui); levels(birthwt$ui) <- c('no','yes')

#library(plyr)
# Function to create frequency tables and label positions
freq.fun <- function(cat.var,response)
{
  k <- as.data.frame(table(cat.var, response),nrow=length(levels(cat.var)))
  names(k)[2] <- 'low'
  k <- plyr::ddply(k, plyr::.(cat.var), transform, percent = Freq/sum(Freq) * 100)
  k <- plyr::ddply(k, plyr::.(cat.var), transform, pos = (cumsum(Freq) - 0.5 * Freq))
  k$label <- '' #paste0(sprintf("%.0f", k$percent), "%")
  return(k)
}

# Function to arrange ggplots in a grid with one legend
grid_arrange_shared_legend <- function(...) {
  plots <- list(...)
  g <- ggplotGrob(plots[[1]] + theme(legend.position="bottom"))$grobs
  legend <- g[[which(sapply(g, function(x) x$name) == "guide-box")]]
  lheight <- sum(legend$height)
  gridExtra::grid.arrange(
    do.call(gridExtra::arrangeGrob, lapply(plots, function(x) x + theme(legend.position="none"))),
    legend,
    ncol = 1,
    heights = grid::unit.c(grid::unit(1, "npc") - lheight, lheight))
}

# Creating the plots
p1 <- ggplot(freq.fun(birthwt$race,birthwt$low), aes(x = cat.var, y = Freq, fill = low)) +
  geom_bar(stat = "identity", position = "dodge") + xlab(NULL) + ylab(NULL) + coord_flip() +
  ggtitle('race') + theme(plot.title=element_text(size=10))+
  geom_text(aes(y = pos, label = label), size=2)

p2 <- ggplot(freq.fun(birthwt$ht,birthwt$low), aes(x = cat.var, y = Freq, fill = low)) +
  geom_bar(stat = "identity", position = "dodge") + xlab(NULL) + ylab(NULL) + coord_flip() +
  ggtitle('history of hypertension') + theme(plot.title=element_text(size=10))+
  geom_text(aes(y = pos, label = label), size=2)

p3 <- ggplot(freq.fun(birthwt$smoke,birthwt$low), aes(x = cat.var, y = Freq, fill = low)) + 
  geom_bar(stat = "identity", position = "dodge") + xlab(NULL) + ylab(NULL) + coord_flip() +
  ggtitle('smoke') + theme(plot.title=element_text(size=10))+
  geom_text(aes(y = pos, label = label), size=2)

p4 <- ggplot(freq.fun(birthwt$ptl,birthwt$low), aes(x = cat.var, y = Freq, fill = low))+
  geom_bar(stat = "identity", position = "dodge") + xlab(NULL) + ylab(NULL) + coord_flip() +
  ggtitle('number of previous premature labours') + theme(plot.title=element_text(size=10))+
  geom_text(aes(y = pos, label = label), size=2)

p5 <- ggplot(freq.fun(birthwt$ui,birthwt$low), aes(x = cat.var, y = Freq, fill = low))+
  geom_bar(stat = "identity", position = "dodge") + xlab(NULL) + ylab(NULL) + coord_flip() +
  ggtitle('has uterine irritability') + theme(plot.title=element_text(size=10))+
  geom_text(aes(y = pos, label = label), size=2)

p6 <- ggplot(freq.fun(birthwt$ftv,birthwt$low), aes(x = cat.var, y = Freq, fill = low))+
  geom_bar(stat = "identity", position = "dodge") + xlab(NULL) + ylab(NULL) + coord_flip() +
  ggtitle('number of physician visits the last trimester') + theme(plot.title=element_text(size=10))+
  geom_text(aes(y = pos, label = label), size=2)

p6 <- ggplot(data = birthwt) +
  geom_boxplot(aes(x=lwt, col = low)) +
  ggtitle('weight (lbs)') + 
  theme(plot.title=element_text(size=10),
        axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())+
xlab("") + ylab("")

grid_arrange_shared_legend(p1, p2, p3, p4, p5, p6)
```

The exploratory data analysis suggests that smoking has a large impact, as the relative proportion of women who smoke having underweight babies is significant. Likewise, hypertensions seems an important predictor but there are only a handful in the sample with this condition. Women who previously gave birth to premature babies are more likely to give birth to underweight (and potentially premature) babies, whereas women giving birth to babies under 2.5kg also typically have lower weight. We can use a logistic regression to assess the effect of these explanatories.


```{r logistibirthwgt, echo = FALSE}
logit2 <- glm(formula = low ~ lwt + race + smoke + ptl + ht + ui, family = binomial(link = "logit"), data = birthwt)
#summary(logit2)
names(logit2$coefficients)[2:8] <- c("mother's weight (lbs)","race [black]","race [other]","smoker [yes]", "previous premature labours","hypertension [yes]","uterine irritability")
dust(logit2) %>%
  sprinkle(col = 2:3, round = 2) %>%
  sprinkle(col = 4, round = 2) %>% 
  sprinkle(col = 5, fn = quote(pvalString(value))) %>% 
  sprinkle_colnames(estimate = "estimate", 
                     std.error = "std. error",
                     statistic = "Wald stat.", 
                     p.value = "p-value") %>% 
  sprinkle_bookdown() %>%
  knitr::kable(align = "lrrrr", booktabs = TRUE, caption = "Logistic regression: table of coefficients for the birth weight data.")
```

Table \@ref(tab:logistibirthwgt) gives the estimated coefficients $\widehat{\boldsymbol{\beta}}$ on the logs odds scale along with standard errors, Wald statistics and the $p$-values from the normal approximation.

We can interpret the effect of these variables: given the other variables and *ceteris paribus*:

- the odds of giving birth to an underweight babies decrease by `r round((1-exp(coef(logit2)[2]))*100,1)`% for each additional pound pre-pregnancy; this paradoxical interpretation, at odds with Figure \@ref(fig:birthweighteda) is (notably) due to the correlation between smoking and mother's weight.
- the odds of black women giving birth to an underweight baby are `r round((exp(coef(logit2)[3])-1),2)` times those of white women.
- the odds of women who smoke of having a baby weighting less than 2.5kg at birth are `r round(100*(exp(coef(logit2)[5])-1),1)`% higher than those of women who do not smoke.

The other parameters are interpreted accordingly. We can also check the parameter significance, by comparing the model with all the covariates and withholding one of the explanatories at the time. The results of the likelihood ratio test are presented in Table \@ref(tab:type3birthwgt). At level 5\% and given the other factors, having previous premature labours history and uterine irritability are not statistically significant. There is no significant correlation between the explanatories, since all the variance inflation factors are inferior to $1.5$. 

```{r type3birthwgt, echo = FALSE}
tab <- car::Anova(logit2, type = 3)
rownames(tab) <- c("mother's weight","race","smoker", "previous premature labours","hypertension","uterine irritability")
dust(tab, 
     keep_rownames = TRUE) %>% 
  sprinkle(col = 2, round = 2) %>% 
  sprinkle(col = 3, round = 2) %>% 
  sprinkle(col = 4, fn = quote(pvalString(value))) %>% 
  sprinkle_colnames('.rownames' = "variable",
                    'LR Chisq' = "lik. ratio", 
                     'Df' = "df",
                     "Pr(>Chisq)" = "p-value")  %>% 
knitr::kable(align = "lrrr", booktabs = TRUE, caption = "Analysis of deviance table (Type 3 decomposition) for the birthweight logistic regression model: the table gives the $p$-value for likelihood ratio tests comparing the full model including all covariates with models in which a single explanatory is removed.")
```

### Issues with binary data

One oddity of regression models for binary data is that most of the information in the sample is provided by response whose variance is largest (points that have $\pi$ far from 0 or 1). This can be seen by looking at how the probability $\pi$ changes as a function of changes in the $j$th explanatory $\mathrm{X}_j$: if $\pi = \mathrm{expit}(\beta_0 + \beta_1\mathrm{X}_1 + \cdots + \beta_p\mathrm{X}_p)$, then $\partial \pi/\partial \mathrm{X}_j = \beta_j \pi(1-\pi)$, so the effect of $\beta_j$ is large when $\pi$ is near $0.5$ and near zero if the probabilities are close to the endpoints $0$ or $1$. Thus, if one wants to estimate a coefficient $\beta_j$ of large magnitude, the ratio $|\beta_j|/\mathsf{sd}(\beta_j) \to 0$ as $\beta_j \to \infty$: the power for testing that $\beta_j=0$ using a Wald statistic tends to zero. Wald statistics are not recommended for Bernoulli data: the following simple example illustrates why.

```{example, name = "Incoherent results for Wald tests", label="waldgarbage"}
Suppose we consider a simple binary model and $n=100$ observations and the null hypothesis $\mathscr{H}_0:\pi = 0.5$ and equivalently for the logistic model $Y_i \sim \mathsf{Bin}\{m, \mathrm{expit}(\beta_0)\}$, where this same hypothesis amounts to $\mathscr{H}_0:\beta_0=0$. Figure \@ref(fig:WaldgarbageR) clearly shows that lack of invariance of the Wald statistic to reparametrization. When $\widehat{\beta}_0$ increases and $m$ is large, the Wald statistic for $\beta_0$ decreases as we approach $m$ successes, because the standard error of $\widehat{\beta}_0$ increases faster than the difference $\widehat{\beta}-0$. Both statistics are asymptotically normal if the null hypothesis holds true, so the two-sided test would reject if the value of the Wald statistic is greater than 1.96. Depending on the parametrization, however, they give different results: the statistic increases monotonically for $\pi$, but would converge to zero for $\beta_0$ when $m$ is large (allowing thereby larger values of $\widehat{\beta}_0$).  No such problem arises when considering score and likelihood ratio tests.
```

```{r WaldgarbageR, cache = TRUE, echo = FALSE, fig.cap = "Wald test statistic for $Y_i \\sim \\mathsf{Bin}(m=500, \\pi)$ (left)  for $\\pi=0.5$ and the same hypothesis test on the logistic scale with $Y_i \\sim \\mathsf{Bin}(m=500, \\mathrm{expit}(\\beta_0))$ and $\\mathscr{H}_0:\\beta_0=0$ (right) as a function of the number of success out of 500 trials."}
nm <- 500L
i <- (ceiling(nm/2)+1):(nm-1)
Wald_logit <- sapply(i, function(i){summary(glm(rbind(c(i,nm-i))~1, family=binomial))$coefficients[3]})
Wald_ori <- sqrt((i/nm - 0.5)^2/(i/nm*(1-i/nm)/nm))

Wald_crap <- data.frame(success = i, test1 = Wald_logit, test2 = Wald_ori)

a1 <- ggplot(data = Wald_crap) + 
geom_line(aes(x = success, y = test1)) + xlab("number of success") + ylab("Wald statistic") +ggtitle(expression(beta[0]))
a2 <- ggplot(data = Wald_crap) + 
geom_line(aes(x = success, y = test2)) + xlab("number of success") + ylab("Wald statistic") + ggtitle(expression(pi)) 
a2 + a1
```
Thus, information about the coefficients only accumulates if the true regression coefficients $\boldsymbol{\beta}$ are small.  The deviance and Pearson provide little to no information about the quality of the model adjustment and their null distribution depends on $\widehat{\boldsymbol{\beta}}$: with large coefficients, the asymptotic $\chi^2$ distribution is not a good approximation and relying on these to determine goodness-of-fit is not recommended.

### Quasi-separation of variables

One common problem with binary data is the problem of complete separation of variables: in this scenario, a linear combination of the explanatories allows to perfectly categorize $Y_i$ into $0/1$ depending on the value of the covariates and the predicted probabilities will be exactly zero or one. While this is not a problem for prediction, the coefficients that yield such a sharp transition are near infinite, as displayed in Figure \@ref(fig:sepvarplot); software may or not pick up such error, but in any case Wald tests for individual coefficients are rubbish. This is problematic for inference because we cannot meaningfully interpret the resulting parameters.


```{r quasisepvar, echo = FALSE}
set.seed(1234)
x <- sort(rexp(20, rate = 0.2))
y <- as.integer(x>9)
m1 <- glm(y~ x, family=binomial)
m2 <- logistf::logistf(y~x, family=binomial)
tab1 <- dust(data.frame("term" = c("(Intercept)", "x"), estimate = as.numeric(m1$coefficients), "std.error" = as.numeric(sqrt(diag(vcov(m1)))))) %>%
  sprinkle(col = 2, round = 1) %>%
  sprinkle(col = 3, round = 0) %>% 
   sprinkle_colnames(estimate = "estimate", 
                     std.error = "std. error") %>% 
  sprinkle_bookdown() 
tab2 <- dust(data.frame(estimate = as.numeric(m2$coef), "std.error" = as.numeric(sqrt(diag(m2$var))))) %>%
  sprinkle(col = 1:2, round = 1) %>%
   sprinkle_colnames(estimate = "estimate", 
                     std.error = "std. error") %>% 
  sprinkle_bookdown() 
knitr::kable(x = list(tab1, tab2), align = "lrrrr", booktabs = TRUE, caption = "Logistic regression: simulated dataset exhibiting separation of variables with standard logistic model (left) and Firth's regression (right). The large coefficients and standard errors, combined with near zero residuals, are all indicative of separation of variable.")
```

Inspection of the coefficients and their standard errors is a good way to detect such problems: parameters that are large (with standardized explanatories with mean zero variance one inputs) with $|\widehat{\beta}_j| > 36$ are particularly suspicious; the number of iterations for the optimization routine is also often abnormally large. While the Wald statistic for $\beta_1=0$ is nearly zero and the $p$-value 1, the likelihood ratio test yields a statistic of `r as.numeric(car::Anova(m1, type=3)[1])` with a negligible associated $p$-value.


One way to restore finiteness of the $\boldsymbol{\beta}$ is to impose a penalty term that prevents near infinite parameter values. Firth's penalty is the most popular solution (option `firth` in the **SAS** procedure `logistic` and `logistf::logistf` function in **R**.

```{r sepvarplot, echo = FALSE,fig.cap= "Illustration of complete separation of variable on simulated observations. Parameters are near infinite to yield a sharp transition at $x=9$. Quasi-complete separation of variable occurs if both outcomes are observed at the cutoff point."}
fittedsepmod <- function(x){1/(1+exp(-coef(m1)[1] - x*coef(m1)[2]))}
ggplot(data.frame(x, y), aes(x=x,y=y)) +
  geom_point() +
  stat_function(fun = fittedsepmod, xlim = c(min(x),max(x)), n = 1001L)
```


### Diagnostic plots for binary data ($\star$)

There are also other issues arising from the discreteness of the observations. Since observations are 0/1, residuals are often separated.  Figure \@ref(fig:diagplotsbirthwt1)
shows residual plots for Example \@ref(exm:birthweightex):  both plots are of limited use to assess goodness-of-fit and model assumptions.

```{r diagplotsbirthwt1, echo = FALSE, fig.cap = "Diagnostic plots for the binary regression for the birth weight data: jackknife deviance residuals against linear predictor (left) and quantile-quantile plot of ordered deviance residuals (right)."}
# glm.diag.plots.ggplot2 <- function(glmfit, which = 1:4){
  #if(logit2$family[[1]]=="binomial")
  glmdiag <- boot::glm.diag(logit2)
  x1 <- predict(logit2)
  g1 <- ggplot(data.frame(x = x1, y = glmdiag$res)) + geom_point(aes(x = x, y = y)) +
    xlab("linear predictor") + 
    ylab("jackknife deviance residuals")
    y2 <- glmdiag$rd
    x2 <- qnorm(ppoints(length(y2)))[rank(y2)]
 g2 <- ggplot(data.frame(x=x2, y= y2)) +
      geom_abline(slope=1, intercept=0) + 
      geom_point(aes(x = x, y = y)) + 
      ylab("ordered deviance residuals") + 
      xlab("theoretical normal quantiles")
g1 + g2
```

<!-- One possible mitigation strategy for the quantile-quantile plot is to look at higher-order approximations, like -->
<!-- \begin{align*} -->
<!-- r^{*}_i = r_{D_i} + \frac{1}{r_{D_i}} \log \left( \frac{r_{P_i}}{r_{D_i}}\right); -->
<!-- \end{align*} -->
<!-- $r^*_i$ is built to have a standard normal distribution to high accuracy.  -->

Another strategy is use of quantile residuals  [@Brillinger/Preisler:1983]. If $Y \sim F$, the quantile transform gives $U=F(Y) \sim \mathsf{U}(0,1)$, meaning uniformly distributed on $(0,1)$. Replacing the unknown distribution $F$ by $\widehat{F}$ should yield approximately uniform observations. With $\widehat{\pi}_i$ denoting the probability of success, we take
\[
U_i = U_{i1} Y_i + U_{i2}(1-Y_i), \qquad U_{i1} \sim \mathsf{U}(0, \widehat{\pi}_i), \quad U_{i2} \sim \mathsf{U}(\widehat{\pi}_i, 1)
\]
and the resulting **uniform residuals** will be, as their name hints, approximately uniform on $(0,1)$. The drawback of this approach lies in the randomness of the residuals.
Figure \@ref(fig:diagplotbirthwgt) shows the diagnostic plots based on the uniform residuals (top two rows): there is no overall indication of poor fit, except for seemingly too few low/high residuals. The two observations with high leverage correspond to non-smoker mothers with no premature labour record: one with hypertension and a weight of 95lbs (unusual combination), the other weighting 200 lbs, with no hypertension and presence of uterine irritability.

```{r diagplotbirthwgt, cache = TRUE, echo = FALSE, fig.cap = "Diagnostic plots based on uniform residuals for the birth weight data. From left to right, top to bottom: residuals against explanatory and linear predictors, histogram of uniform residuals and Tukey's quantile-quantile plot, Cook statistic against weighted leverage and case number. "}
# uniform residuals for logit2
set.seed(1234)
pi <- predict(logit2,type='response')
ui <- (as.integer(birthwt$low)-1L)*runif(189,0,pi)+(1-as.integer(birthwt$low)+1L)*runif(189,pi,1)

# CI for the uniform QQplot
library(boot)
nboot <- 10000
M <- matrix(runif(length(ui)*nboot),ncol=length(ui),nrow=nboot)
M <- t(apply(M,1,sort))
e <- envelope(mat=M)

g1 <- ggplot(data.frame(x=birthwt$lwt,y=ui)) + 
  geom_point(aes(x=x, y=y)) + 
  labs(x="mother's weight",y='uniform residuals') 
g2 <- ggplot(data.frame(x=predict(logit2,type='link'),y= ui)) + geom_point(aes(x=x,y=y)) + labs(x='linear predictor',y='uniform residuals')
g3 <- ggplot(data.frame(x=ui),aes(x=x)) +
  geom_histogram(bins=10L) + 
  labs(x='uniform residuals',y= "frequency")
ppointsU <- ppoints(n = 189)
g4 <- ggplot(data.frame(x = ppointsU, y =sort(ui)-ppointsU, ymin = e$overall[1,]-ppointsU, ymax = e$overall[2,]-ppointsU)) +
  geom_ribbon(aes(x=x,ymin =ymin, ymax = ymax), alpha = 0.2) + 
  geom_abline(intercept = 0, slope = 0) + 
  geom_point(aes(x=x, y=y)) + 
  labs(x='theoretical uniform quantiles',
      y='detrended uniform residuals')
 
g5 <- ggplot(data.frame(x = glmdiag$h/(1 - glmdiag$h), y = glmdiag$cook)) + 
  geom_point(aes(x=x,y=y)) +
  labs(x = "h/(1-h)", y= "Cook statistic")
ry <- range(glmdiag$cook)
rank.fit <- logit2$rank
nobs <- rank.fit + logit2$df.residual
cooky <- 8/(nobs - 2 * rank.fit) 
g6 <- ggplot(data.frame(x=1:189, y = glmdiag$cook)) + 
  geom_point(aes(x=x,y=y)) + 
  geom_hline(yintercept = cooky) + 
  labs(x="case",y="Cook statistic")

 # rstar <- glmdiag$rd + log(glmdiag$rp/glmdiag$rd)/glmdiag$rd
# g6 <- ggplot(data.frame(x = qnorm(ppointsU), y =sort(rstar))) +
#   geom_abline(intercept = 0, slope = 1) + 
#   geom_point(aes(x=x, y=y)) + 
#   labs(x='theoretical normal quantiles',
#       y=expression(paste("", "r", phantom()^{
#     paste(symbol("*"))
# }, "", " residuals")))
 (g1 + g2) / (g3 + g4) / (g5 + g6)
```



## Count data

If a response is integer value, the linear model is seldom appropriate because the variable is skewed, which even the log-linear model may fail to capture. It may thus be appropriate to look at generalized linear models for discrete data. 

The most widespread model for count data is the Poisson regression. The Poisson distribution arises as the limit of binomial data with a small probability of success as the number of attempts becomes large: examples include rare medical conditions, number of accidents, or goals in a soccer (aka football) match. The mean and variance of the Poisson distribution are both $\lambda$, so the model imposes stringent constraints on the observations. Extensions that deal with zero-inflation, underdispersed and overdispersed data are widespread. Most count data are found in contingency tables, in which the cells give the counts, while the dimensions of the table represent categorical variables. Such data format are convenient for storage and display, but must be transformed to long format for model fitting.

The canonical link of the Poisson model is the natural logarithm $\ln(x)$ and the interpretation will be similar to that of the [log-linear model](transformation-response). Specifically, suppose the mean model is 
\begin{align*}
\mu = \exp(\beta_0 + \beta_1 \mathrm{X}_{1} + \cdots + \beta_p \mathrm{X}_{p}),
\end{align*}
so the mean is multiplied by $\exp(\beta_j)$ for an increase of one unit of $\mathrm{X}_{j}$, *ceteris paribus*. If $\beta_j < 0$, $\exp(\beta_j) < 1$ and so we have a decrease of $100\cdot(1-\exp(\beta_j))$% of the mean response. Likewise, if $\beta_j>0$, the mean number increases by $100\cdot(\exp(\beta_j)-1)$%. The interpretation of interactions between categorical and continuous/categorical variables is similar to that of multiplicative (log-linear) models.






## Modelling proportions





### Simpson's paradox


```{r, eval = FALSE, echo = FALSE}
dat <- data.frame(dead=c(2,3,14,27,51,29,13,1,5,7,12,40,101,64),total=c(55,124,109,130,115,36,13,62,157,121,78,121,129,64),age=c('18-24','25-34','35-44','45-54','55-64','65-74','75+','18-24','25-34','35-44','45-54','55-64','65-74','75+'),smoker=c(rep('yes',7),rep('no',7)))
dat$alive <- dat$total-dat$dead
logit1 <- glm(cbind(dead,alive)~smoker,family=binomial(link ='logit'),data=dat)
summary(logit1)
xtable::xtable(logit1)

logit2 <- glm(cbind(dead,alive)~age+smoker-1,family=binomial(link ='logit'),data=dat)
summary(logit2)
xtable(logit2)

# illustration pb marginalization
set.seed(1)
x1 <- rnorm(20,-1,1)
y1 <- 5+0.2*x1+rnorm(20,0,0.5)
x2 <- rnorm(20,1,1)
y2 <- 1+0.5*x2+rnorm(20,0,0.5)
x <- c(x1,x2)
y <- c(y1,y2)

par(mar=c(3.1,3.1,1.6,1.5),mgp=c(1.7,0.6,0),font.main=1,cex.main=0.8, bty = "l")
par(mfrow=c(1,2))
plot(x,y,pch=c(rep(1,20),rep(19,20)))
abline(5,0.2,col=2)
abline(1,0.5,col=4)
plot(x,y)
abline(lm(y~x),col='black')
dev.off()
```



