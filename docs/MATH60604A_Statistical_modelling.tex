% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  11pt,
  letterpaper,
]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Statistical Modelling},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs}
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% \usepackage{amsmath,amssymb,mathtools}
\usepackage{enumerate}
\usepackage{geometry}
\geometry{hmargin=1.2in}
\usepackage[mathscr]{eucal}
\DeclareMathAlphabet{\mathcrl}{U}{rsfs}{m}{n}
\usepackage{utopia}
\DeclareMathAlphabet{\mathcal}{OMS}{cmsy}{m}{n}
\usepackage{booktabs}
\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother

\usepackage{framed,color}
\definecolor{shadecolor}{RGB}{248,248,248}

\renewcommand{\textfraction}{0.05}
\renewcommand{\topfraction}{0.8}
\renewcommand{\bottomfraction}{0.8}
\renewcommand{\floatpagefraction}{0.75}

\let\oldhref\href
\renewcommand{\href}[2]{#2\footnote{\url{#1}}}

\ifxetex
  \usepackage{letltxmacro}
  \setlength{\XeTeXLinkMargin}{1pt}
  \LetLtxMacro\SavedIncludeGraphics\includegraphics
  \def\includegraphics#1#{% #1 catches optional stuff (star/opt. arg.)
    \IncludeGraphicsAux{#1}%
  }%
  \newcommand*{\IncludeGraphicsAux}[2]{%
    \XeTeXLinkBox{%
      \SavedIncludeGraphics#1{#2}%
    }%
  }%
\fi

\makeatletter
\newenvironment{kframe}{%
\medskip{}
\setlength{\fboxsep}{.8em}
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\makeatletter
\@ifundefined{Shaded}{
}{\renewenvironment{Shaded}{\begin{kframe}}{\end{kframe}}}
\makeatother

\newenvironment{rmdblock}[1]
  {
  \begin{itemize}
  \renewcommand{\labelitemi}{
    \raisebox{-.7\height}[0pt][0pt]{
      {\setkeys{Gin}{width=3em,keepaspectratio}\includegraphics{images/#1}}
    }
  }
  \setlength{\fboxsep}{1em}
  \begin{kframe}
  \item
  }
  {
  \end{kframe}
  \end{itemize}
  }
\newenvironment{rmdnote}
  {\begin{rmdblock}{note}}
  {\end{rmdblock}}
\newenvironment{rmdcaution}
  {\begin{rmdblock}{caution}}
  {\end{rmdblock}}
\newenvironment{rmdimportant}
  {\begin{rmdblock}{important}}
  {\end{rmdblock}}
\newenvironment{rmdtip}
  {\begin{rmdblock}{tip}}
  {\end{rmdblock}}
\newenvironment{rmdwarning}
  {\begin{rmdblock}{warning}}
  {\end{rmdblock}}
\usepackage{amsmath,amssymb,mathtools,enumerate,booktabs}
\ifluatex
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{apalike2}

\title{Statistical Modelling}
\author{}
\date{\vspace{-2.5em}}

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}{Lemma}[chapter]
\newtheorem{corollary}{Corollary}[chapter]
\newtheorem{proposition}{Proposition}[chapter]
\newtheorem{conjecture}{Conjecture}[chapter]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\theoremstyle{definition}
\newtheorem{example}{Example}[chapter]
\theoremstyle{definition}
\newtheorem{exercise}{Exercise}[chapter]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\begin{document}
\maketitle

\let\href\oldhref

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{preliminary-remarks}{%
\chapter*{Preliminary remarks}\label{preliminary-remarks}}
\addcontentsline{toc}{chapter}{Preliminary remarks}

These notes by Léo Belzile (HEC Montréal) are licensed under a \href{http://creativecommons.org/licenses/by-nc-sa/4.0/}{Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License} and were last compiled on 2020-09-04.

While we show how to implement statistical tests and models in \textbf{SAS} in class, these note will illustrate the concepts using \textbf{R}: visit \href{https://cran.r-project.org/}{the R-project website} to download the program. The most popular graphical cross-platform front-end is \href{https://www.rstudio.com/products/rstudio/download/}{RStudio Desktop}.

Bien que les diapositives illustrent l'implémentation des techniques statistiques et des modèles à l'aide de \textbf{SAS}, ces notes présentent le pendant \textbf{R}: visitez \href{https://cran.r-project.org/}{le site web du projet \textbf{R}} pour télécharger le logiciel. L'interface graphique la plus populaire (et celle que je vous recommande) est \href{https://www.rstudio.com/products/rstudio/download/}{RStudio Desktop}.

The most famous quote about statistical models is probably due to George Box, who claimed that ``all models are wrong, but some are useful''. This standpoint is reductive: Peter McCullagh and John Nelder wrote in the preamble of their book (emphasis mine)

\begin{quote}
Modelling in science remains, partly at least, an art. Some principles do exist, however, to guide the modeller. The first is that all models are wrong; \textbf{some, though, are better} than others and we can \textbf{search for the better ones}. At the same time we must recognize that eternal truth is not within our grasp.
\end{quote}

And this quote by David R. Cox adds to the point:

\begin{quote}
\ldots it does not seem helpful just to say that all models are wrong. The very word model implies simplification and idealization. The idea that com-
plex physical, biological or sociological systems can be exactly described
by a few formulae is patently absurd. The construction of idealized representations that \textbf{capture important stable aspects of such systems}
is, however, a vital part of general scientific analysis and statistical models, especially substantive ones, do not seem essentially different from
other kinds of model.
\end{quote}

\newcommand{\bs}[1]{\boldsymbol{#1}}
\newcommand{\Hmat}{\mathbf{H}}
\newcommand{\Mmat}{\mathbf{M}}
\newcommand{\mX}{\mathbf{X}}
\newcommand{\bX}{{\mathbf{X}}}
\newcommand{\bx}{{\mathbf{x}}}
\newcommand{\by}{{\boldsymbol{y}}}
\newcommand{\bY}{{\boldsymbol{Y}}}
\newcommand{\eps}{\varepsilon}
\newcommand{\beps}{\boldsymbol{\varepsilon}}
\newcommand{\bbeta}{\boldsymbol{\beta}}
\newcommand{\hbb}{\hat{\boldsymbol{\beta}}}
\newcommand{\limni}{\lim_{n \ra \infty}}
\newcommand{\Sp}{\mathscr{S}}
\newcommand{\Hy}{\mathscr{H}}
\newcommand{\E}[2][]{{\mathsf E}_{#1}\left(#2\right)}
\newcommand{\Va}[2][]{{\mathsf{Var}_{#1}}\left(#2\right)}
\newcommand{\I}[1]{{\mathbf 1}_{#1}}

\hypertarget{intro}{%
\chapter{Introduction to statistical inference}\label{intro}}

Statistical modelling requires a good graps of statistical inference: as such, we begin with a review of hypothesis testing and graphical exploratory data analysis.

The purpose of statistical inference is to draw conclusions based on data. Scientific research relies on hypothesis testing: once an hypothesis is formulated, the researcher collects data, performs a test and concludes as to whether there is evidence for the proposed theory.

There are two main data type: \textbf{experimental} data are typically collected in a control environment following a research protocol with a particular experimental design: they serve to answer questions specified ahead of time. This approach is highly desirable to avoid the garden of forking paths \href{http://www.stat.columbia.edu/~gelman/research/unpublished/p_hacking.pdf}{(researchers unfortunately tend to refine or change their hypothesis in light of data, which invalidates their findings} --- preregistration alleviates this somewhat). While experimental data are highly desirable, it is not always possible to collect experimental data: for example, an economist cannot modify interest rates to see how it impacts consumer savings. When data have been collected beforehand without intervention (for other purposes), these are called \textbf{observational}. These will be the ones most frequently encountered.

A stochastic model will comprise two ingredients: a distribution for the random data and a formula linking the parameters or the conditional expectation of a response variable \(Y\) to a set of explanatories \(\mathbf{X}\). A model can serve to either predict new outcomes (predictive modelling) or else to test research hypothesis about the effect of the explanatory variables on the response (explanatory model). These two objectives are of course not mutually exclusive even if we distinguish in practice inference and prediction.

A predictive model gives predictions of \(Y\) for different combinations of explanatory variables or future data. For example, one could try to forecast the enery consumption of a house as a function of weather, the number of inhabitants and its size. Black boxes used in machine learning are often used solely for prediction: these models are not easily interpreted and they often ignore the data structure.

By constrast, explicative models are often simple and interpretable: regression models are often used for inference purpose and we will focus on these.

\begin{itemize}
\tightlist
\item
  Are consumer ready to spend more when they pay by credit card rather than by cash?
\item
  Is there wage discrimination towards women in a US college?
\item
  University degree: \href{https://www.theglobeandmail.com/report-on-business/is-the-university-experience-worth-the-cost/article31703109/}{``is the university experience worth the cost'\,'?}
\item
  What are the criteria impacting health insurance premiums?
\item
  Is the price of gasoline more expansive in the Gaspé peninsula than in the rest of Quebec? \href{http://www.regie-energie.qc.ca/energie/rapports/Rapport_priceGasp\%C3\%A9sie_20191219.pdf}{A report of the \emph{Régie de l'énergie} examines the question}
\item
  Are driving tests in the UK easier if you live in a rural area? \href{https://www.theguardian.com/world/2019/aug/23/an-easy-ride-scottish-village-fuels-debate-driving-test-pass-rates}{An analysis of \emph{The Guardian}} hints that it is the case.
\item
  Does the risk of transmission of Covid19 increase with distancing? \href{https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(20)31142-9/fulltext}{A (bad) meta-analysis says two meters is better than one} (or how to draw erroneous conclusions from a bad model).
\end{itemize}

\hypertarget{tests}{%
\section{Hypothesis testing}\label{tests}}

An hypothesis test is a binary decision rule used to evaluate the statistical evidence provided by a sample to make a decision regarding the underlying population. The main steps involved are:

\begin{itemize}
\tightlist
\item
  define the model parameters
\item
  formulate the alternative and null hypothesis
\item
  choose and calculate the test statistic
\item
  obtain the null distribution describing the behaviour of the test statistic under \(\mathscr{H}_0\)
\item
  calculate the \emph{p}-value
\item
  conclude (reject or fail to reject \(\mathscr{H}_0\)) in the context of the problem.
\end{itemize}

A good analogy for hypothesis tests is a trial for murder on which you are appointed juror.

\begin{itemize}
\tightlist
\item
  The judge lets you choose between two mutually exclusive outcome, guilty or not guilty, based on the evidence presented in court.
\item
  The presumption of innocence applies and evidences are judged under this optic: are evidence remotely plausible if the person was innocent? The burden of the proof lies with the prosecution to avoid as much as possible judicial errors. The null hypothesis \(\mathscr{H}_0\) is \emph{not guilty}, whereas the alternative \(\mathscr{H}_a\) is \emph{guilty}. If there is a reasonable doubt, the verdict of the trial will be not guilty.
\item
  The test statistic (and the choice of test) represents the summary of the proof. The more overwhelming the evidence, the higher the chance the accused will be declared guilty. The prosecutor chooses the proof so as to best outline this: the choice of evidence (statistic) ultimately will maximise the evidence, which parallels the power of the test.
\item
  The final step is the verdict. This is a binary decision, guilty or not guilty. For an hypothesis test performed at level \(\alpha\), one would reject (guilty) if the \emph{p}-value is less than \(\alpha\).
\end{itemize}

The above description provides some heuristic, but lack crucial details developed in the next section written by Juliana Schulz.

\hypertarget{hypothesis}{%
\subsection{Hypothesis}\label{hypothesis}}

In statistical tests we have two hypotheses: the null hypothesis (\(H_0\)) and the alternative hypothesis (\(H_1\)). Usually, the null hypothesis is the `status quo' and the alternative is what we're really interested in testing. A statistical hypothesis test allows us to decide whether or not our data provides enough evidence to reject \(H_0\) in favour of \(H_1\), subject to some pre-specified risk of error. Usually, hypothesis tests involve a parameter, say \(\theta\), which characterizes the underlying distribution at the population level ans whose value is unknown. A two-sided hypothesis test regarding a parameter \(\theta\) has the form
\begin{align*}
\mathscr{H}_0: \theta=\theta_0 \qquad \text{versus} \qquad \mathscr{H}_a:\theta \neq \theta_0.
\end{align*}
We are testing whether or not \(\theta\) is precisely equal to the value \(\theta_0\). The hypotheses are a statistical representation of our research question.

For example, for a two-sided test for the regression coefficient \(\beta_j\) associated to an explanatory variable \(\mathrm{X}_j\), the null and alternative hypothesis are
explicative d'intérêt \(\mathrm{X}_j\), les hypothèses sont
\begin{align*}
\mathscr{H}_0: \beta_j=\beta_j^0 \qquad \text{versus} \qquad \mathscr{H}_a:\beta_j \neq \beta_j^0, 
\end{align*}
where \(\beta_j^0\) is some value that reflects the research question of interest. For example, if \(\beta_j^0=0\), the underlying question is: is covariate \(\mathrm{X}_j\) impacting the response \(Y\) once other variables have been taken into account?

Note that we can impose direction in the hypotheses and consider alternatives of the form \(\mathscr{H}_a: \theta > \theta_0\) or \(\mathscr{H}_a: \theta < \theta_0\).

\hypertarget{test-statistic}{%
\subsection{Test statistic}\label{test-statistic}}

A test statistic \(T\) is a functional of the data that summarise the information contained in the sample for \(\theta\). The form of the test statistic is chosen such that we know its underlying distribution under \(H_0\), that is, the potential values taken by \(T\) and their relative probability if \(H_0\) is true. Indeed, \(Y\) is a random variable and its value change from one sample to the next.
This allows us to determine what values of \(T\) are likely if \(H_0\) is true. Many statistics we will consider are \textbf{Wald statistic}, of the form
\begin{align*}
T = \frac{\widehat{\theta} - \theta_0}{\mathrm{se}(\widehat{\theta})} 
\end{align*}
where \(\widehat{\theta}\) is an estimator of \(\theta\), \(\theta_0\) is the postulated value of the parameter and \(\mathrm{se}(\widehat{\theta})\) is an estimator of the standard deviation of the test statistic \(\widehat{\theta}\).

For example, to test whether the mean of a population is zero, we set
\begin{align*}
\mathscr{H}_0: \mu=0, \qquad  \mathscr{H}_a:\mu \neq 0, 
\end{align*}
and the Wald statistic is
\begin{align*}
T &= \frac{\overline{X}-0}{S_n/\sqrt{n}}
\end{align*}
where \(\overline{X}\) is the sample mean of \(X_1, \ldots, X_n\),
\begin{align*}
\overline{X} &= \frac{1}{n} \sum_{i=1}^n X_i = \frac{X_1+ \cdots + X_n}{n}
\end{align*}
and the standard error (of the mean) \(\overline{X}\) is \(S_n/\sqrt{n}\); the sample variance \(S_n\) is an estimator of the standard deviation \(\sigma\),
\begin{align*}
S^2_n &= \frac{1}{n-1} \sum_{i=1}^n (X_i-\overline{X})^2.
\end{align*}

Its important to distinguish between procedures/formulas and their numerical values. An \textbf{estimator} is a rule or formula used to calculate an estimate of some parameter or quantity of interest based on observed data. For example, the sample mean \(\bar{X}\) is an estimator of the population mean \(\mu\). Once we have observed data we can actually compute the sample mean, that is, we have an estimate --- an actual value. In other words,

\begin{itemize}
\tightlist
\item
  an estimator is the procedure or formula telling us how to use sample data to compute an estimate. Its a random variable since it depends on the sample.
\item
  an estimate is the numerical value obtained once we apply the formula to observed data
\end{itemize}

\hypertarget{null-distribution-and-p-value}{%
\subsection{\texorpdfstring{Null distribution and \emph{p}-value}{Null distribution and p-value}}\label{null-distribution-and-p-value}}

The \emph{p}-value allows us to decide whether the observed value of the test statistic \(T\) is plausible under \(H_0\). Specifically, the \emph{p}-value is the probability that the test statistic is equal or more extreme to the estimate computed from the data, assuming \(H_0\) is true. Suppose that based on a random sample \(X_1, \ldots, X_n\) we obtain a statistic whose value \(T=t\). For a two-sided test \(\mathscr{H}_0:\theta=\theta_0\) vs.~\(\mathscr{H}_a:\theta \neq \theta_0\), the \emph{p}-value is \(\mathsf{Pr}_0(|T| \geq |t|)\). If the distribution of \(T\) is symmetric around zero, the \emph{p}-value is
\begin{align*}
p = 2 \times \mathsf{Pr}_0(T \geq |t|).
\end{align*}

Consider the example of a two-sided test involving the population mean \(H_0:\mu=0\) against the alternative \(H_1:\mu \neq 0\). Assuming the random sample comes from a normal (population) \(\mathsf{No}(\mu, \sigma^2)\), it can be shown that if \(H_0\) is true (that is, if \(\mu=0\)), the test statistic
\begin{align*}
T = \frac{\overline{X}}{S/\sqrt{n}}
\end{align*}
follows a Student-\emph{t} distribution with \(n-1\) degrees of freedom, denoted \(\mathsf{St}_{n-1}\). This allows us to calculate the \emph{p}-value (either from a table, or using some statistical software). The Student-\emph{t} distribution is symmetric about zero, so the \emph{p}-value is \(P = 2\times\mathsf{Pr}(T_{n-1} > |t|)\), where \(T \sim \mathsf{St}_{n-1}\).

\hypertarget{conclusion}{%
\subsection{Conclusion}\label{conclusion}}

The \emph{p}-value allows us to make a decision about the null hypothesis. If \(\mathscr{H}_0\) is true, the \emph{p}-value follows a uniform distribution. \href{https://xkcd.com/1478/}{Thus, if the \emph{p}-value is small}, this means observing an outcome more extreme than \(T=t\) is unlikely, and so we're inclined to think that \(H_0\) is not true. There's always some underlying risk that we're making a mistake when we make a decision. In statistic, there are \href{https://xkcd.com/2303/}{two type of errors}:

\begin{itemize}
\tightlist
\item
  type I error: we reject \(H_0\) when \(H_0\) is true,
\item
  type II error: we fail to reject \(H_0\) when \(H_0\) is false.
\end{itemize}

These hypothesis are not judged equally: we seek to avoid error of type I (judicial errors, corresponding to condamning an innocent). To prevent this, we fix a the level of the test, \(\alpha\), which captures our tolerance to the risk of commiting a type I error: the higher the level of the test \(\alpha\), the more often we will reject the null hypothesis when the latter is true. The value of \(\alpha \in (0, 1)\) is the probability of rejecting \(\mathscr{H}_0\) when \(\mathscr{H}_0\) is in fact true,
\begin{align*}
\alpha = \mathsf{Pr}_0\left(\text{ reject } \mathscr{H}_0\right).
\end{align*}
The level \(\alpha\) is fixed beforehand, typically \(1\)\%, \(5\)\% or \(10\)\%. Keep in mind that the probability of type I error is \(\alpha\) only if the null model for \(\mathscr{H}_0\) is correct (sic) and correspond to the data generating mechanism.

The focus on type I error is best understood by thinking about medical trial: you need to prove a new cure is better than existing alternatives drugs or placebo, to avoid extra costs or harming patients (think of Didier Raoult and his unsubstantiated claims that hydrochloroquine, an antipaludean drug, should be recommended treatment against Covid19).

\begin{longtable}[]{@{}lcc@{}}
\toprule
\textbf{Decision} \textbackslash{} \textbf{true model} & \(\mathscr{H}_0\) & \(\mathscr{H}_a\)\tabularnewline
\midrule
\endhead
fail to reject \(\mathscr{H}_0\) & \(\checkmark\) & type II error\tabularnewline
reject \(\mathscr{H}_0\) & type I error & \(\checkmark\)\tabularnewline
\bottomrule
\end{longtable}

To make a decision, we compare our \emph{p}-value \(P\) with the level of the test \(\alpha\):

\begin{itemize}
\tightlist
\item
  if \(P < \alpha\), we reject \(\mathscr{H}_0\);
\item
  if \(P \geq \alpha\), we fail to reject \(\mathscr{H}_0\).
\end{itemize}

Do not mix up level of the test (probability fixed beforehand by the researcher) and the \emph{p}-value. If you do a test at level 5\%, the probability of type I error is by definition \(\alpha\) and does not depend on the \emph{p}-value. The latter is conditional probability of observing a more extreme likelihood given the null distribution \(\mathscr{H}_0\) is true.

\hypertarget{power}{%
\subsection{Power}\label{power}}

There are two sides to an hypothesis test: either we want to show it is not unreasonable to assume the null hypothesis, or else we want to show beyond reasonable doubt that a difference or effect is significative: for example, one could wish to demonstrate that a new website design (alternative hypothesis) leads to a significant increase in sales relative to the status quo. Our ability to detect these improvements and make discoveries depends on the power of the test: the larger the power, the greater our ability to reject \(\mathscr{H}_0\) when the latter is false.

Failing to reject \(\mathscr{H}_0\) when \(\mathscr{H}_a\) is true corresponds to the definition of type II error, the probability of which is \(1-\gamma\), say. The \textbf{power of a test} is the probability of rejecting \(\mathscr{H}_0\) when \(\mathscr{H}_0\) is false, i.e.,
\begin{align*}
\gamma = \mathsf{Pr}_a(\text{reject} \mathscr{H}_0)
\end{align*}
Depending on the alternative models, it is more or less easy to detect that the null hypothesis is false and reject in favor of an alternative.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{MATH60604A_Statistical_modelling_files/figure-latex/power1-1} 

}

\caption{Comparison between null distribution (full curve) and a specific alternative for a *t*-test (dashed line). The power corresponds to the area under the curve of the density of the alternative distribution which is in the rejection area (in white).}\label{fig:power1}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{MATH60604A_Statistical_modelling_files/figure-latex/power2-1} 

}

\caption{Increase in power due to an increase in the mean difference between the null and alternative hypothesis. Power is the area in the rejection region (in white) under the alternative distribution (dashed): the latter is more shifted to the right relative to the null distribution (full line).}\label{fig:power2}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{MATH60604A_Statistical_modelling_files/figure-latex/power3-1} 

}

\caption{Increase of power due to an increase in the sample size or a decrease of standard deviation of the population: the null distribution (full line) is more concentrated. Power is given by the area (white) under the curve of the alternative distribution (dashed). In general, the null distribution changes with the sample size.}\label{fig:power3}
\end{figure}

We want a test to have high power, i.e., that \(\gamma\) be as close to 1 as possible. Minimally, the power of the test should be \(\alpha\) because we reject the null hypothesis \(\alpha\) fraction of the time even when \(\mathscr{H}_0\) is true. Power depends on many criteria, notably

\begin{itemize}
\tightlist
\item
  the effect size: the bigger the difference between the postulated value for \(\theta_0\) under \(\mathscr{H}_0\) and the observed behavior, the easier it is to detect it.
  (Figure \ref{fig:power3});
\item
  variability: the less noisy your data, the easier it is to detect differences between the curves (big differences are easier to spot, as Figure \ref{fig:power2} shows);
\item
  the sample size: the more observation, the higher our ability to detect significative differences because the standard error decreases with sample size \(n\) at a rate (typically) of \(n^{-1/2}\). The null distribution also becomes more concentrated as the sample size increase.
\item
  the choice of test statistic: for example, rank-based statistics discard information about the actual values and care only about relative ranking. Resulting tests are less powerful, but are typically more robust to model misspecification and outliers. The statistics we will choose are standard and amongst the most powerful: as such, we won't dwell on this factor.
\end{itemize}

To calculate the power of a test, we need to single out a specific alternative hypothesis. In very special case, analytic derivations are possible: for example, the one-sample \emph{t}-test statistic \(T=\sqrt{n}(\overline{X}_n-\mu_0)/S_n \sim \mathcal{T}_{n-1}\) for a normal sample follows a noncentral Student-\(t\) distribution with noncentrality parameter \(\Delta\) if the expectation of the population is \(\Delta + \mu_0\). In general, such closed-form expressions are not easily obtained and we compute instead the power of a test through Monte Carlo methods. For a given alternative, we simulate repeatedly samples from the model, compute the test statistic on these new samples and the associated \emph{p}-values based on the postulated null hypothesis. We can then calculate the proportion of tests that lead to a rejection of the null hypothesis at level \(\alpha\), namely the percentage of \emph{p}-values smaller than \(\alpha\).

\hypertarget{confidence-interval}{%
\subsection{Confidence interval}\label{confidence-interval}}

A \textbf{confidence interval} is an alternative way to present the conclusions of an hypothesis test performed at significance level \(\alpha\). It is often combined with a point estimator \(\hat{\theta}\) to give an indication of the variability of the estimation procedure. Wald-based \((1-\alpha)\) confidence intervals for a parameter \(\theta\) are of the form
\begin{align*}
\widehat{\theta} \pm \mathfrak{q}_{\alpha/2} \; \mathrm{se}(\widehat{\theta})
\end{align*}
where \(\mathfrak{q}_{\alpha/2}\) is the \(1-\alpha/2\) quantile of the null distribution of the Wald statistic
\begin{align*}
T =\frac{\widehat{\theta}-\theta}{\mathrm{se}(\widehat{\theta})},
\end{align*}
and where \(\theta\) represents the postulated value for the fixed, but unknown value of the parameter. The bounds of the confidence intervals are random variables, since both \(\widehat{\theta}\) and \(\mathrm{se}(\widehat{\theta})\) are random variables: their values depend on the sample, and will vary from one sample to another.

For example, for a random sample \(X_1, \ldots, X_n\) from a normal distribution \(\mathsf{No}(\mu, \sigma)\), the (\(1-\alpha\)) confidence interval for the population mean \(\mu\) is
\begin{align*}
\overline{X} \pm t_{n-1, \alpha/2} \frac{S}{\sqrt{n}}
\end{align*}
where \(t_{n-1,\alpha/2}\) is the \(1-\alpha/2\) quantile of a Student-\(t\) distribution with \(n-1\) degrees of freedom.

Before the interval is calculated, there is a \(1-\alpha\) probability that \(\theta\) is contained in the \textbf{random} interval \((\widehat{\theta} - \mathfrak{q}_{\alpha/2} \; \mathrm{se}(\widehat{\theta}), \widehat{\theta} + \mathfrak{q}_{\alpha/2} \; \mathrm{se}(\widehat{\theta}))\), where \(\widehat{\theta}\) denotes the estimator. Once we obtain a sample and calculate the confidence interval, there is no more notion of probability: the true value of the parameter \(\theta\) is either in the confidence interval or not. We can interpret confidence interval's as follows: if we were to repeat the experiment multiple times, and calculate a \(1-\alpha\) confidence interval each time, then roughly \(1-\alpha\) of the calculated confidence intervals would contain the true value of \(\theta\) in repeated samples (in the same way, if you flip a coin, there is roughly a 50-50 chance of getting heads or tails, but any outcome will be either). Our confidence is in the \emph{procedure} we use to calculate confidence intervals and not in the actual values we obtain from a sample.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{MATH60604A_Statistical_modelling_files/figure-latex/intconf-1} 

}

\caption{95\% confidence intervals for the mean of a standard normal population $\mathsf{No}(0,1)$, with 100 random samples. On average, 5\% of these intervals fail to include the true mean value of zero (in red).}\label{fig:intconf}
\end{figure}

If we are only interested in the binary decision rule reject/fail to reject \(\mathscr{H}_0\), the confidence interval is equivalent to a \emph{p}-value since it leads to the same conclusion. Whereas the \(1-\alpha\) confidence interval gives the set of all values for which the test statistic doesn't provide enough evidence to reject \(\mathscr{H}_0\) at level \(\alpha\), the \emph{p}-value gives the probability under the null of obtaning a result more extreme than the postulated value and so is more precise for this particular value. If the \emph{p}-value is smaller than \(\alpha\), our null value \(\theta\) will be outside of the confidence interval and vice-versa.

\begin{example}[Online purchases of millenials]
\protect\hypertarget{exm:achats-milleniaux}{}{\label{exm:achats-milleniaux} \iffalse (Online purchases of millenials) \fi{} }
Suppose a researcher studies the evolution of online sales in Canada. She postulates that generation Y members make more online purchase than older generations. A survey is sent to a simple random sample of \(n=500\) individuals from the population with 160 members of generation Y and 340 older people. The response ariable is the total amount of online goods purchased in the previous month (in dollars).
\end{example}

In this example, we consider the difference between the average amount spent by Y members and those of previous generations: the mean difference in the samples is -16.49 dollars and thus millenials spend more. However, this in itself is not enough to conclude that the different is significative, nor can we say it is meaningful. The amount spent online varies from one individual to the next (and plausibly from month to month), and so different random samples would yield different mean differences.

The first step of our analysis is defining the parameters corresponding to quantities of interest and formulating the null and alternative hypothesis as a function of these parameters. We will consider a test for the difference in mean of the two populations, say \(\mu_1\) for the expected amount spent by generation Y and \(\mu_2\) for older generations, with respective standard errors \(\sigma_1\) and \(\sigma_2\). We next write down our hypothesis: the researcher is interested in whether millenials spend more, so this is the alternative hypothesis, \(\mathscr{H}_a: \mu_1 > \mu_2\). The null consists of all other values \(\mathscr{H}_0: \mu_1 \leq \mu_2\), but only \(\mu_1=\mu_2\) matters for the purpose of testing (why?)

The second step is the choice of test statistic. We consider the \citet{Welch:1947} statistic for a difference in mean between two samples,
\begin{align*}
T = \frac{\overline{X}_1 - \overline{X}_2}{\left(\frac{S_1^2}{n_1}+\frac{S_2^2}{n_2} \right)^{1/2}}, \end{align*}
where \(\overline{X}_i\) is the sample mean, \(S_i^2\) is the unbiased variance estimator and \(n_i\) is the sample size for group \(i\) (\(i=1, 2\)). If the mean difference between the two samples is zero, then \(\overline{X}_1-\overline{X}_2\) has mean zero and the difference has variance \(\sigma^2_1/n_1+\sigma^2_2/n_2\). For our sample, the value of statistic is \(T=-2.76\) Since the value changes from one sample to the next, we need to determine if this value is compatible with the null hypothesis by comparing it to the null distribution of \(T\) (when \(\mathscr{H}_0\) is true and \(\mu_1-\mu_2=0\)). We perform the test at level \(\alpha=0.05\).

The third step consists in obtaining a benchmark to determine if our result is extreme or unusual. To make comparisons easier, we standardize the statistic so its has mean zero and variance one under the null hypothesis \(\mu_1=\mu_2\), so as to obtain a dimensionless measure whose behaviour we know for large sample. The (mathematical) derivation of the null distribution is beyond the scope of this course, and will be given in all cases. Asymptotically, \(T\) follows a standard normal distribtion \(\mathsf{No}(0, 1)\), but there exists a better finite-sample approximation when \(n_1\) or \(n_2\) is small; we use \citet{Satterthwaite:1946} and a Student-\(t\) distribution as null distribution.

It only remains to compute the \emph{p}-value. If the null distribution is well-specified and \(\mathscr{H}_0\) is true, then the random variable \(P\) is uniform on \([0, 1]\); we thus expect to obtain under the null something larger than 0.95 only 5\% of the time for our one-sided alternative since we consider under \(\mathscr{H}_0\) the event \(\mathsf{Pr}(T > t)\). The \(p\)-value is \(1\) and, at level 5\%, we reject the null hypothesis to conclude that millenials spend significantly than previous generation for monthly online purchases, with an estimated average difference of -16.49.

\begin{example}[Price of Spanish high speed train tickets]
\protect\hypertarget{exm:price-trains-tests}{}{\label{exm:price-trains-tests} \iffalse (Price of Spanish high speed train tickets) \fi{} }The Spanish national railway company, \href{https://www.renfe.com/}{Renfe}, manages regional and high speed train tickets all over Spain and The Gurus \href{https://www.kaggle.com/thegurusteam/spanish-high-speed-rail-system-ticket-pricing}{harvested} the price of tickets sold by Renfe. We are interested in trips between Madrid and Barcelona and, for now, ask the question: are tickets more expensive one way or another? To answer this, we consider a sample of 10000 tickets, but restrict attention to AVE tickets sold at Promo rate. Our test statistic will again be the mean difference between the price (in euros) for a train ticket for Madrid--Barcelona (\(\mu_1\)) and the price for Barcelona--Madrid (\(\mu_2\)), i.e., \(\mu_1-\mu_2\). The null hypothesis is that there are no difference in price, so \(\mathscr{H}_0: \mu_1-\mu_2=0\). We again use Welch test statistic for two samples.
\end{example}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Library for manipulating data, including the pipe operator (\%\textgreater{}\%)}
\KeywordTok{library}\NormalTok{(poorman)}
\CommentTok{\# Load data}
\KeywordTok{data}\NormalTok{(renfe, }\DataTypeTok{package =} \StringTok{"hecstatmod"}\NormalTok{)}
\KeywordTok{head}\NormalTok{(renfe, }\DataTypeTok{n =} \DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 5 x 7
##   price type    class      fare     dest             duration wday 
##   <dbl> <fct>   <fct>      <fct>    <fct>               <dbl> <fct>
## 1 143.  AVE     Preferente Promo    Barcelona-Madrid      190 6    
## 2 182.  AVE     Preferente Flexible Barcelona-Madrid      190 2    
## 3  86.8 AVE     Preferente Promo    Barcelona-Madrid      165 7    
## 4  86.8 AVE     Preferente Promo    Barcelona-Madrid      190 7    
## 5  69.0 AVE-TGV Preferente Promo    Barcelona-Madrid      175 4
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Sub{-}sample with only Promo tickets}
\NormalTok{renfe\_promo \textless{}{-}}\StringTok{ }\NormalTok{renfe }\OperatorTok{\%\textgreater{}\%}\StringTok{ }\KeywordTok{subset}\NormalTok{(fare }\OperatorTok{==}\StringTok{ "Promo"}\NormalTok{)}
\CommentTok{\# two{-}sample t{-}test and mean difference}
\NormalTok{ttest \textless{}{-}}\StringTok{ }\KeywordTok{t.test}\NormalTok{(price}\OperatorTok{\textasciitilde{}}\NormalTok{dest, }\DataTypeTok{data =}\NormalTok{ renfe\_promo)}
\NormalTok{ttest }\CommentTok{\#print result}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## 	Welch Two Sample t-test
## 
## data:  price by dest
## t = -1, df = 8040, p-value = 0.2
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -1.100  0.209
## sample estimates:
## mean in group Barcelona-Madrid mean in group Madrid-Barcelona 
##                           82.1                           82.6
\end{verbatim}

Rather than use the asymptotic distribution, whose validity stems from the central limit theorem, we could consider another approximation under the less restrictive assumption that the data are exchangeable: under the null hypothesis, there is no difference between the two destinations and so the label for destination (a binary indicator) is arbitrary. The reasoning underlying \href{https://www.jwilber.me/permutationtest/}{permutation tests} is as follows: to create a benchmark, we will consider observations with the same number in each group, but permuting the labels. We then compute the test statistic on each of these datasets. If there are only a handful in each group (fewer than 10), we could list all possible permutations of the data, but otherwise we can repeat this procedure many times, say 9999, to get a good approximation. This gives an approximate distribution from which we can extract the \emph{p}-value by computing the rank of our statistic relative to the others.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# p{-}value (permutation test)}
\NormalTok{n \textless{}{-}}\StringTok{ }\KeywordTok{nrow}\NormalTok{(renfe\_promo)}
\NormalTok{B \textless{}{-}}\StringTok{ }\FloatTok{1e4}
\NormalTok{ttest\_stats \textless{}{-}}\StringTok{ }\KeywordTok{numeric}\NormalTok{(B) }
\NormalTok{ttest\_stats[}\DecValTok{1}\NormalTok{] \textless{}{-}}\StringTok{ }\NormalTok{ttest}\OperatorTok{$}\NormalTok{statistic}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{20200608}\NormalTok{) }\CommentTok{\# set seed of pseudo{-}random number generator}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{2}\OperatorTok{:}\NormalTok{B)\{}
  \CommentTok{\# Recalculate the test statistic, permuting the labels}
\NormalTok{  ttest\_stats[i] \textless{}{-}}\StringTok{ }\KeywordTok{t.test}\NormalTok{(price }\OperatorTok{\textasciitilde{}}\StringTok{ }\NormalTok{dest[}\KeywordTok{sample.int}\NormalTok{(}\DataTypeTok{n =}\NormalTok{ n)], }
                           \DataTypeTok{data =}\NormalTok{ renfe\_promo)}\OperatorTok{$}\NormalTok{statistic}
\NormalTok{\}}
\CommentTok{\# Graphics library}
\KeywordTok{library}\NormalTok{(ggplot2)}
\CommentTok{\# Plot the empirical permutation distribution}
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{statistic =}\NormalTok{ ttest\_stats), }
       \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{statistic)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom\_histogram}\NormalTok{(}\DataTypeTok{bins =} \DecValTok{30}\NormalTok{, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{y=}\NormalTok{..density..), }\DataTypeTok{alpha =} \FloatTok{0.2}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom\_density}\NormalTok{() }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom\_vline}\NormalTok{(}\DataTypeTok{xintercept =}\NormalTok{ ttest\_stats[}\DecValTok{1}\NormalTok{]) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{ylab}\NormalTok{(}\StringTok{"density"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{stat\_function}\NormalTok{(}\DataTypeTok{fun =}\NormalTok{ dnorm, }\DataTypeTok{col =} \StringTok{"blue"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{MATH60604A_Statistical_modelling_files/figure-latex/renfepermut-1} 

}

\caption{Permutation-based approximation to the null distribution of Welch two-sample t-test statistic (histogram and black curve) with standard normal approximation (blue curve) for the price of AVE tickets at promotional rate between Madrid and Barcelona. The value of the test statistic calculated using the original sample is represented by a vertical line.}\label{fig:renfepermut}
\end{figure}

The so-called bootstrap approximation to the \emph{p}-value of the permutation test, \(0.186\), is the proportion of statistics that are more extreme than the one based on the original sample. It is nearly identical to that obtained from the Satterthwaite approximation, \(0.182\) (the Student-\(t\) distribution is numerically equivalent to a standard normal with that many degrees of freedom), as shown in Figure \ref{fig:renfepermut}. Even if our sample is very large (\(n=8059\) observations), the difference is not statistically significative. With a bigger sample (the database has more than 2 million tickets), we could estimate more precisely the average difference, up to 1/100 of an euro: the price difference would eventually become statistically significative, but this says nothing about practical difference: \(0.28\) euros relative to an Promo ticket priced on average \(82.56\) euros is a negligible amount.

\hypertarget{eda}{%
\section{Exploratory Data Analysis}\label{eda}}

Before fitting a model, it is advisable to understand the structure of the data to avoid interpretation errors. Basic knowledge of graphs is required and we will spend some time \href{https://rstudio.cloud/learn/primers/1.1}{addressing this}. Further references include

\begin{itemize}
\tightlist
\item
  \href{https://r4ds.had.co.nz/exploratory-data-analysis.html}{Chapter 3, \emph{\textbf{R} for Data Science} by Garrett Grolemund and Hadley Wickham}
\item
  \href{https://www.openintro.org/book/isrs/}{Section 1.6 of OpenIntro \emph{Introductory Statistics with Randomization and Simulation}}
\item
  \href{https://clauswilke.com/dataviz/}{\emph{Fundamentals of Data Visualization} by Claus O. Wilke}
\item
  \href{https://socviz.co/lookatdata.html\#lookatdata}{Chapter 1 of \emph{Data Visualization: A practical introduction} by Kieran Healy}
\end{itemize}

If exploratory data analysis is often neglected in statistics (perhaps because it has little to no mathematical foundations), it is crucial. More than a rigorous approach, it is an art: Grolemund and Wickham talk of ``state of mind''. The purpose of graphical exploratory data analysis is the extraction of useful information, often through a series of preliminary questions that are refined as the analysis progresses. Of particular interest and the relations and interactions between difference variables and the distribution of the variables themselves. The major steps for undertaking an exploratory analysis are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Formulate questions about the data
\item
  Look for answers using frequency table, descriptive statistics and graphics.
\item
  Refine the questions in light of the finding
\end{enumerate}

In a report, you should highlight the most import features in a summary so that the reader can grasp your understanding and so that you guide him or her in the interpretation of the data.

\hypertarget{polish-your-work}{%
\subsection{Polish your work}\label{polish-your-work}}

Pay as much attention to figures and tables than to the main text. These should always include a legend that describes and summarize the findings in the graph (so that the latter is standalone), name of variables (including units) on the axes, but also proper formatting so that the labels and numbers are readable (good printing quality, not too small). One picture is worth 1000 words, but make sure the graph tells a coherent story and that it is mentioned in the main text. Also ensure that only the necessary information is displayed: superfluous information (spurious digits, useless summary statistics) should not be presented.

\hypertarget{variable-type}{%
\subsection{Variable type}\label{variable-type}}

The data we will handled are stored in tables or frames. If the data frame is stocked in wide format, each line corresponds to an observation and each column to a variable: the entries of the data base contain the (numeric) values.

\begin{itemize}
\tightlist
\item
  a \textbf{variable} represents a characteristic of the population, for example the sex of an individual, the price of an item, etc.
\item
  an \textbf{observation} is a set of measures (variables) collected under identical conditions for an individual or at a given time.
\end{itemize}

The choice of statistical model and test depends on the underlying type of the data collected. There are many choices: quantitative (discrete or continuous) if the variables are numeric, or qualitative (binary, nominal, ordinal) if they can be described using an adjective; I prefer the term categorical, which is more evocative.

Most of the models we will deal with are so-called regression models, in which the mean of a quantitative variable is a function of other variables, termed explanatories. There are two types of numerical variables

\begin{itemize}
\tightlist
\item
  a discrete variable takes a countable number of values, prime examples being binary variables or count variables.
\item
  a continuous variable can take (in theory) an infinite possible number of values, even when measurements are rounded or measured with a limited precision (time, width, mass). In many case, we could also consider discrete variables as continuous if they take enough values (e.g., money).
\end{itemize}

Categorical variables take only a finite of values. They are regrouped in two groups, nominal if there is no ordering between levels (sex, color, country of origin) or ordinal if they are ordered (Likert scale, salary scale) and this ordering should be reflected in graphs or tables. We will bundle every categorical variable using arbitrary encoding for the levels: for modelling, these variables taking \(K\) possible values (or levels) must be transformed into a set of \(K-1\) binary 0/1 variables, the omitted level corresponding to a baseline. Failing to declare categorical variables in your favorite software is a common mistake, especially when these are saved in the database using integers rather than strings.

\hypertarget{graphs}{%
\subsection{Graphs}\label{graphs}}

The main type of graph for representing categorical variables is bar plot (and modifications thereof). In a bar plot, the frequency of each category is represented in the \(y\)-axis as a function of the (ordered) levels on the \(x\)-axis. This representation is superior to the \href{http://www.perceptualedge.com/articles/08-21-07.pdf}{ignominious pie chart}, a nuisance that ought to be banned (humans are very bad at comparing areas and a simple rotation changes the perception of the graph)!

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{MATH60604A_Statistical_modelling_files/figure-latex/barplotrenfe-1} 

}

\caption{Bar plot of ticket class for Renfe tickets data}\label{fig:barplotrenfe}
\end{figure}

Continuous variables can take as many distinct values as there are observations, so we cannot simply count the number of occurences by unique values. Instead, we bin them into distinct intervals so as to obtain an histogram. The number of class depends on the number of observations: as a rule of thumb, the number of bins should not exceed \(\sqrt{n}\), where \(n\) is the sample size. We can then obtain the frequency in each class, or else normalize the histogram so that the area under the bands equals one: this yields a discrete approximation of the underlying density function. Varying the number of bins can help us detect patterns (rounding, asymmetry, multimodality).

Since we bin observations together, it is sometimes difficult to see where they fall. Adding rugs below or above the histogram will add observation about the range and values taken, where the heights of the bars in the histogram carry information about the (relative) frequency of the intervals.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{MATH60604A_Statistical_modelling_files/figure-latex/histrenfe-1} 

}

\caption{Histogram of Promo tickets for Renfe ticket data}\label{fig:histrenfe}
\end{figure}

If we have a lot of data, it sometimes help to focus only on selected summary statistics. A box-and-whiskers plot (or boxplot) represents five numbers

\begin{itemize}
\tightlist
\item
  The box gives the quartiles \(q_1, q_2, q_3\) of the distribution. The middle bar \(q_2\) is thus the median, so 50\% of the observations are smaller or larger than this number.
\item
  The length of the whiskers is up to \(1.5\) times the interquartiles range \(q_3-q_1\) (the whiskers extend until the latest point in the interval, so the largest observation that is smaller than \(q_3+1.5(q_3-q_1)\), etc.)
\item
  Observations beyond the whiskers are represented by dots or circles, sometimes termed outliers. However, beware of this terminology: the larger the sample size, the more values will fall outside the whiskers. This is a drawback of boxplots, which was conceived at a time where the size of data sets was much smaller than what is current standards.
\end{itemize}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/01-intro-boxplot} 

}

\caption{Box-and-whiskers plot}\label{fig:boxplot}
\end{figure}

We can represent the distribution of a response variable as a function of a categorical variable by drawing a boxplot for each category and laying them side by side. A third variable, categorical, can be added via a color palette, as shown in Figure \ref{fig:histboxplot}.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{MATH60604A_Statistical_modelling_files/figure-latex/histboxplot-1} 

}

\caption{Box-and-whiskers plots for Promo fare tickets as a function of class and type for the Renfe tickets data.}\label{fig:histboxplot}
\end{figure}

Scatterplots are used to represent graphically the co-variation between two continuous variables: each tuple gives the coordinate of the point. If only a handful of large values are visible on the graph, a transformation may be useful: oftentimes, you will encounter graphs where the \(x\)- or \(y\)-axis is on the log-scale when the underlying variable is positive. If the number of data points is too large, it is hard to distinguish points because they are overlaid: adding transparency, or binning using a two-dimensional histogram with the frequency represented using color are potential solutions. The left panel of Figure \ref{fig:scatterplot} shows the 100 simulated observations, whereas the right-panel shows a larger sample of 10 000 points using hexagonal binning, an analog of the bivariate density.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{MATH60604A_Statistical_modelling_files/figure-latex/scatterplot-1} 

}

\caption{Scatterplot (left) and hexagonal heatmap of bidimensional bin counts (right) of simulated data.}\label{fig:scatterplot}
\end{figure}

Sometimes, continuous data have a particular structure, mostly when observations are collected over space or time. Time series are ordered and the response should be plotted on the \(y\)-axis as a function of time (on the \(x\)-axis). It is customary to draw segments between observations, but this display is sometimes misleading.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{MATH60604A_Statistical_modelling_files/figure-latex/seriechrono-1} 

}

\caption{Graphical representation of a time series.}\label{fig:seriechrono}
\end{figure}

\hypertarget{exploratory-data-analysis}{%
\subsection{Exploratory data analysis}\label{exploratory-data-analysis}}

Rather than describe in details the exploratory analysis procedure, we proceed with an example that illustrates the process on the Renfe ticket dataset that was introduced previously.

\begin{example}[Exploratory data analysis of Renfe tickets]
\protect\hypertarget{exm:renfe-aed}{}{\label{exm:renfe-aed} \iffalse (Exploratory data analysis of Renfe tickets) \fi{} }First, read the documentation accompanying the dataset! The data base \texttt{renfe} contains the following variables:
\end{example}

\begin{itemize}
\tightlist
\item
  \texttt{price} price of the ticket (in euros);
\item
  \texttt{dest} binary variable indicating the journey, either Barcelona to Madrid (\texttt{0}) or Madrid to Barcelona (\texttt{1});
\item
  \texttt{fare} categorical variable indicating the ticket fare, one of \texttt{AdultoIda}, \texttt{Promo} or \texttt{Flexible};
\item
  \texttt{class} categorical variable giving the ticket class, either \texttt{Preferente}, \texttt{Turista}, \texttt{TuristaPlus} or \texttt{TuristaSolo};
\item
  \texttt{type} categorical variable indicating the type of train, either Alta Velocidad Española (\texttt{AVE}), Alta Velocidad Española jointly with TGV (parternship between SNCF and Renfe for trains to/from Toulouse) \texttt{AVE-TGV} or regional train \texttt{REXPRESS}; only trains labelled \texttt{AVE} or \texttt{AVE-TGV} are high-speed trains.
\item
  \texttt{duration} length of train journey (in minutes);
\item
  \texttt{wday} categorical variable (integer) denoting the week day, ranging from Sunday (\texttt{1}) to Saturday (\texttt{7}).
\end{itemize}

There are no missing values and a quick view of the first row of the data frame (\texttt{head(renfe)}) shows that the data are stored in long format, meaning each line corresponds to a different ticket. We will begin our exploratory analysis with vague questions, for example

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What are the factors determining the price and travel time?
\item
  Does travel time depend on the type of train?
\item
  What are the distinctive features of train types?
\item
  What are the main differences between fares?
\end{enumerate}

Except for \texttt{price} and \texttt{duration}, all the other (explanatory) variables are categoricals. These need to be cast into factors (\texttt{factor}), especially integer-valued levels such as \texttt{wday}.

The database is clean and this preliminary preprocessing step has been done already. We can check the type of encoding using the command \texttt{str}, which also shows the data; the function \texttt{summary} is used to obtain descriptive statistics (min, max, mean, quartiles for continuous variables or else frequency for categorical variables); the function also returns the number of missing values (\texttt{NA}) of each column.

Data manipulation is often messy and \textbf{R} base syntax is particularly inelegant: data frames are list whose elements are accessed using \texttt{\$}: for example \texttt{renfe\$price}. A more legible and modular alternative is the pipe operator (\texttt{\%\textgreater{}\%}), with which one creates a logical chain of command (this function is not part of \textbf{R} base packages, but the libraries \texttt{tidyverse} and the minimal alternative \texttt{poorman} include it).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{renfe }\OperatorTok{\%\textgreater{}\%}\StringTok{ }\KeywordTok{count}\NormalTok{(class)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         class    n
## 1  Preferente  809
## 2     Turista 7197
## 3 TuristaPlus 1916
## 4 TuristaSolo   78
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# \textasciigrave{}count\textasciigrave{} is a shortcut for the following syntax}
\NormalTok{renfe }\OperatorTok{\%\textgreater{}\%}\StringTok{ }\KeywordTok{group\_by}\NormalTok{(type) }\OperatorTok{\%\textgreater{}\%}\StringTok{ }\KeywordTok{tally}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       type    n
## 1      AVE 9174
## 2  AVE-TGV  429
## 3 REXPRESS  397
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{renfe }\OperatorTok{\%\textgreater{}\%}\StringTok{ }\KeywordTok{group\_by}\NormalTok{(fare) }\OperatorTok{\%\textgreater{}\%}\StringTok{ }\KeywordTok{tally}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        fare    n
## 1 AdultoIda  397
## 2  Flexible 1544
## 3     Promo 8059
\end{verbatim}

By counting the number of train tickets in each category, we notice there are as many \texttt{REXPRESS} tickets as there are tickets sold at \texttt{AdultoIda} fare. Using a contingency table to get the number in respective sub-categories of each of those variables confirms that all tickets in the database for RegioExpress trains are sold with the \texttt{AdultoIda} fare and that there is only a single class, \texttt{Turista}. There are few such tickets, only 397 out of 10 000. This raises a new question: why are such trains so unpopular?

\begin{verbatim}
##        fare     type    n
## 1 AdultoIda REXPRESS  397
## 2  Flexible      AVE 1446
## 3  Flexible  AVE-TGV   98
## 4     Promo      AVE 7728
## 5     Promo  AVE-TGV  331
\end{verbatim}

We have only scratched the surface, but one could also notice that there are only 17 duration values on tickets (\texttt{renfe\ \%\textgreater{}\%\ distinct(duration)} or \texttt{unique(renfe\$duration)}). This leads us to think the duration on the ticket (in minutes) is the expected travel time. The majority of those travel time (15 out of 17) are smaller than 3h15, but the other two exceed 9h! Looking at Google Maps, Madrid and Barcelona are 615km apart by car, 500km as the crow flies. this means some trains travel at about 200km/h, while others are closer to 70km/h. What are these slower trains? the variable \texttt{type} is the one most likely to encode this feature, and a quick look shows that the RegioExpress trains fall in the slow category (mystery solved!)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{renfe }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{subset}\NormalTok{(duration }\OperatorTok{\textgreater{}}\StringTok{ }\DecValTok{200}\NormalTok{) }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{group\_by}\NormalTok{(type, dest) }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\StringTok{"average duration"}\NormalTok{ =}\StringTok{ }\KeywordTok{mean}\NormalTok{(duration), }
            \StringTok{"std. dev"}\NormalTok{ =}\StringTok{ }\KeywordTok{sd}\NormalTok{(duration),}
            \StringTok{"average price"}\NormalTok{ =}\StringTok{ }\KeywordTok{mean}\NormalTok{(price), }
            \StringTok{"std. dev"}\NormalTok{ =}\StringTok{ }\KeywordTok{sd}\NormalTok{(price)) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       type             dest average duration std. dev average price std. dev
## 1 REXPRESS Barcelona-Madrid              544        0          43.2        0
## 2 REXPRESS Madrid-Barcelona              562        0          43.2        0
\end{verbatim}

The regular trains running between two cities take more than 9h, but one way (Madrid to Barcelona) is 18 minutes slower than in the other direction. More striking, we see that the price of the RegioExpress tickets is fixed: 43.25 euros regardless of direction. This is the most important finding so far, because these are not a sample for price: there is no variability! Graphics could have lead to the discovery (the boxplot of price as a function of train type would collapse to a single value).

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{MATH60604A_Statistical_modelling_files/figure-latex/renfe-aed4-1} 

}

\caption{Boxplot of ticket price as a function of destination and train type.}\label{fig:renfe-aed4}
\end{figure}

We could have suspected that trains labeled \texttt{AVE} are faster: after all, it is the acronym of \emph{Alta Velocidad Española}, literally Spanish high speed. What is the distinction between the two high speed train types. According to the \href{https://www.renfe-sncf.com/rw-en/services/a-unique-experience/Pages/services.aspx}{SNCF website}, AVE-TGV trains are partnership between Renfe and SNCF that operate between France and Spain.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{renfe }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{subset}\NormalTok{(type }\OperatorTok{\%in\%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"AVE"}\NormalTok{,}\StringTok{"AVE{-}TGV"}\NormalTok{)) }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{group\_by}\NormalTok{(type, dest) }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\StringTok{"average duration"}\NormalTok{ =}\StringTok{ }\KeywordTok{mean}\NormalTok{(duration), }
            \StringTok{"std. dev"}\NormalTok{ =}\StringTok{ }\KeywordTok{sd}\NormalTok{(duration),}
            \StringTok{"average price"}\NormalTok{ =}\StringTok{ }\KeywordTok{mean}\NormalTok{(price),}
            \StringTok{"std. dev"}\NormalTok{ =}\StringTok{ }\KeywordTok{sd}\NormalTok{(price))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      type             dest average duration std. dev average price std. dev
## 1     AVE Barcelona-Madrid              171     15.9          87.4     19.8
## 2     AVE Madrid-Barcelona              170     16.6          88.2     20.8
## 3 AVE-TGV Barcelona-Madrid              175      0.0          87.0     16.8
## 4 AVE-TGV Madrid-Barcelona              179      0.0          90.6     20.2
\end{verbatim}

The price of high speed trains are on average more than twice as expensive as regular trains. There is strong evidence of heterogeneity (standard deviation of 20 euros), which should raise scrutiny and suggests that high speed train tickets are dynamically priced. There is a single duration time for AVE-TGV tickets. We do not see meaningful differences in price depending on the type or the direction, but fares of ticket class availability may differ depending on whether the train is run in partnership with SNCF.

We have not looked at ticket fare and class, except for RegioExpress trains. Figure \ref{fig:renfe-aed7} shows large disparity in the variance of price according to fare: Promo fare takes many more distinct values than AdultoIda (duh) and Flexible fares. First class tickets (\texttt{Preferente}) are more expensive, but there are fewer observations falling in this group. Turista class is the least expensive for high-speed trains and the most popular. \href{http://web.archive.org/web/20161111134241/http://www.renfe.com/viajeros/tarifas/billete_promo.html}{\texttt{TuristaPlus}} offer an alternative to the latter with more comfort, whereas \texttt{TuristaSolo} gives access to individual seats.

Fare-wise \href{http://web.archive.org/web/20161111134241/http://www.renfe.com/viajeros/tarifas/billete_promo.html}{Promo} and \href{http://web.archive.org/web/20161110220249/http://www.renfe.com/viajeros/tarifas/billete_promoplus.html}{PromoPlus} give access to rebates that can go up to 70\% and 65\%, respectively. Promo tickets cannot be cancelled or exchanged, while both are possible with PromoPlus by paying a penalty amounting to 30-20\% of the ticket price. \href{http://web.archive.org/web/20161108192609/http://www.renfe.com/viajeros/tarifas/billete_flexible.html}{Flexible} fare ticket a sold at the same price as regular high-speed train tickets, but offer additional benefits (and no rebates!)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{renfe }\OperatorTok{\%\textgreater{}\%}\StringTok{ }\KeywordTok{subset}\NormalTok{(fare  }\OperatorTok{!=}\StringTok{ "AdultoIda"}\NormalTok{) }\OperatorTok{\%\textgreater{}\%}
\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ price, }\DataTypeTok{x =}\NormalTok{ class, }\DataTypeTok{col =}\NormalTok{ fare)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom\_boxplot}\NormalTok{() }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{y =} \StringTok{"price (in euros)"}\NormalTok{,}
       \DataTypeTok{x =} \StringTok{"class"}\NormalTok{,}
       \DataTypeTok{color =} \StringTok{"fare"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{legend.position =} \StringTok{"bottom"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{MATH60604A_Statistical_modelling_files/figure-latex/renfe-aed6-1} 

}

\caption{Boxplot of ticket price as a function of fare and class for high-speed Renfe trains.}\label{fig:renfe-aed6}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ renfe, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ price, }\DataTypeTok{y=}\NormalTok{..density.., }\DataTypeTok{fill =}\NormalTok{ fare)) }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom\_histogram}\NormalTok{(}\DataTypeTok{binwidth =} \DecValTok{5}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"price (in euros)"}\NormalTok{, }\DataTypeTok{y =} \StringTok{"density"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{legend.position =} \StringTok{"bottom"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{MATH60604A_Statistical_modelling_files/figure-latex/renfe-aed7-1} 

}

\caption{Histograms of ticket price as a function of fare for Renfe trains.}\label{fig:renfe-aed7}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Check the spread of Flexible tickets}
\NormalTok{renfe }\OperatorTok{\%\textgreater{}\%}\StringTok{ }\KeywordTok{subset}\NormalTok{(fare  }\OperatorTok{==}\StringTok{ "Flexible"}\NormalTok{) }\OperatorTok{\%\textgreater{}\%}\StringTok{ }\KeywordTok{count}\NormalTok{(price, class)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   price       class    n
## 1   108     Turista 1050
## 2   108 TuristaSolo   67
## 3   127     Turista  285
## 4   127 TuristaSolo    9
## 5   129 TuristaPlus   31
## 6   140  Preferente    2
## 7   152 TuristaPlus   10
## 8   182  Preferente   78
## 9   214  Preferente   12
\end{verbatim}

Note how Flexible tickets prices are spread: the boxplot is crushed and the interquartile range seems zero, even if some of the values are larger: this is indicative either constant price or of (too few) tickets in the category. We can find out which of these two possibities is most likely by counting the number of Flexible fare tickets for the different types.

Neither duration, nor type or destination explains why some Flexible tickets are more or less expensive than the average. Promo tickets, on the other hand, are cheaper on average and Preferente more expensive.

We can summarize our findings:

\begin{itemize}
\tightlist
\item
  more than 91\% of trains are high-speed trains.
\item
  travel time depends on the type of train: high-speed train take at most 3h20.
\item
  duration records expected travel time: there are only 17 unique values, 13 of which are for AVE trains.
\item
  the price of RegioExpress train ticket is fixed (43.25€); all such tickets are sold with AdultoIda fare and there only one class (Turista). 57\% of these trains go from Barcelona to Madrid and travel time is 9h22 from Barcelona to Madrid, 9h04 in the other direction.
\item
  \texttt{Turista} is the cheapest and most popular class. \texttt{Preferente} class tickets are more expensive and are less often sold. \texttt{TuristaPlus} offers more comfort and \texttt{TuristaSolo} let you get individual seats.
\item
  according to the \href{https://www.renfe.com/es/es/viajar/tarifas/billetes.html}{Renfe website}, \texttt{Flexible} fare tickets ``come with additional offers and passengers can exchange or cancel their tickets if they miss their train''; as a counterpart, these tickets are more expensive and most tickets have a fixed fare (a handful are cheaper or more expensive, but this price difference is unexplained).
\item
  the distribution of \texttt{Promo} fare high-speed trains ticket prices are more or less symmetric, but \texttt{Flexible} tickets seem left-truncated (the minimum price for these tickets in the sample is 107.7€).
\item
  it appears that tickets sold by Renfe (\texttt{Promo} fare) are dynamically priced: the latter can be up to 70\% cheaper than regular high-speed train tickets when purchased through the official agency or Renfe's website. These tickets cannot be refunded or exchanged.
\item
  there is no indication that prices depend on the direction of travel.
\end{itemize}

\hypertarget{linear-regression}{%
\chapter{Linear regression}\label{linear-regression}}

A linear regression is a model for the conditional mean of a response variable \(Y\) as a function of \(p\) explanatory variables (also termed regressors or covariates),
\begin{align}
\mathsf{E}(Y \mid \mathbf{X})=\beta_0 + \beta_1\mathrm{X}_{1} + \cdots + \beta_p \mathrm{X}_{p}. \label{eq:linearreg}
\end{align}
The mean of \(Y\) is conditional on the values of the observed covariate \(\mathbf{X}\); this amounts to treating them as non-random, known in advance.

In practice, any model is an approximation of reality. An error term is included to take into account the fact that no exact linear relationship links \(\mathbf{X}\) and \(Y\) (otherwise this wouldn't be a statistical problem), or that measurements of \(Y\) are subject to error. The random error term \(\varepsilon\) will be the source of information for our inference, as it will quantify the goodness of fit of the model.

We can rewrite the linear model in terms of the error for a random sample of size \(n\): denote by \(Y_i\) the value of the response for observation \(i\), and \(\mathrm{X}_{ij}\) the value of of the \(j\)th explanatory variable of observation \(i\). The model is
\begin{align}
Y_i = \beta_0 + \beta_1 \mathrm{X}_{i1} + \ldots + \beta_p \mathrm{X}_{ip} +\varepsilon_{i}, \qquad i =1, \ldots, n, \label{eq:olsmean}
\end{align}
where \(\varepsilon_i\) is the additive error term specific to observation \(i\). While we may avoid making distributional assumption about \(\varepsilon_i\), we nevertheless fix its expectation to zero to encode the fact we do not believe the model is systematically off,, so \(\mathsf{E}(\varepsilon_i \mid \boldsymbol{X}_i)=0\) \((i=1, \ldots, n)\).

One important remark is that the model is linear in the coefficients \(\boldsymbol{\beta}\in \mathbb{R}_{p+1}\), not in the explanatory variables! the latter are arbitrary and could be (nonlinear) functions of other explanatory variables, for example \(\mathrm{X}=\log(\texttt{annees})\), \(\mathrm{X}=\texttt{horsepower}^2\) or \(\mathrm{X}= \mathsf{I}_{\texttt{man}}\cdot\mathsf{I}_{\texttt{full}}\). The mean of the response is specified as a \textbf{linear combination of explanatory variables}. This is at the core of the flexibility of the linear regression, which is used mainly for the following purposes:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Evaluate the effects of covariates \(\mathbf{X}\) on the mean response
  of \(Y\).
\item
  Quantify the influence of the explanatories \(\mathbf{X}\) on the
  response and test for their significance.
\item
  Predict the response for new sets of explanatories \(\mathbf{X}\).
\end{enumerate}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Linear regression is the most famous and the most widely used statistical model around. The name may appear reductive, but many tests statistics (\emph{t}-tests, ANOVA, Wilcoxon, Kruskal--Wallis) \href{https://lindeloev.github.io/tests-as-linear/linear_tests_cheat_sheet.pdf}{can be formulated using a linear regression}, while \href{https://threadreaderapp.com/thread/1286420597505892352.html}{models as diverse as trees, principal components and deep neural networks are just linear regression model in disguise}. What changes under the hood between one fancy model to the next are the optimization method (e.g., ordinary least squares, constrained optimization or stochastic gradient descent) and the choice of variables entering the model (spline basis for nonparametric regression, indicator variable selected via a greedy search for trees, activation functions for neural networks).

This chapter explores the basics of linear regression, parameter interpretation and testing for coefficients and sub-models. Analysis of variance will be presented as special case of linear regression.

To make concepts and theoretical notions more concrete, we will use data from a study performed in a college in the United States. The goal of the administration who collected these information was to investigate potential gender inequality in the salary of faculty members. The data contains the following variables:

\begin{itemize}
\tightlist
\item
  \texttt{salary}: nine-month salary of professors during the 2008--2009 academic year (in thousands USD).
\item
  \texttt{rank}: academic rank of the professor (\texttt{assistant}, \texttt{associate} or \texttt{full}).
\item
  \texttt{field}: categorical variable for the field of expertise of the professor, one of \texttt{applied} or \texttt{theoretical}.
\item
  \texttt{sex}: binary indicator for sex, either \texttt{man} or \texttt{woman}.
\item
  \texttt{service}: number of years of service in the college.
\item
  \texttt{annees}: number of years since PhD.
\end{itemize}

Before drafting a model, a quick look at the data is in due order. If salary increases with year, there is more heterogeneity in the salary of higher ranked professors: logically, assistant professors are either promoted or kicked out after at most 6 years according to the data. The limited number of years prevents large variability for their salaries.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{MATH60604A_Statistical_modelling_files/figure-latex/edacollege-1} 

}

\caption{Exploratory data analysis of $\texttt{college}$ data: salaries of professors as a function of the number of years of service and the academic ranking}\label{fig:edacollege}
\end{figure}

Salary increases over years of service, but its variability also increases with rank. Note the much smaller number of women in the sample: this will impact our power to detect differences between sex. A contingency table of sex and academic rank can be useful to see if the proportion of women is the same in each rank: women represent 16\% of assistant professors and 16\% of associate profs, but only 7\% of full professors and these are better paid on average.

\begin{table}

\caption{\label{tab:tableaucontingence}Contingency table of the number of prof in the college by sex and academic rank.}
\centering
\begin{tabular}[t]{lrrr}
\toprule
  & assistant & associate & full\\
\midrule
man & 56 & 54 & 248\\
woman & 11 & 10 & 18\\
\bottomrule
\end{tabular}
\end{table}

The simple linear regression model only includes a single explanatory variable and defines a straight line linking two variables \(\mathrm{X}\) and \(Y\) by means of an equation of the form \(y=a+bx\); Figure \ref{fig:droitenuage} shows the line passing through the scatterplot for years of service.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{MATH60604A_Statistical_modelling_files/figure-latex/droitenuage-1} 

}

\caption{Simple linear regression model for the salary of professors as a function of the number of years of service; the line is the solution of the least squares problem.}\label{fig:droitenuage}
\end{figure}

\hypertarget{ordinary-least-squares}{%
\section{Ordinary least squares}\label{ordinary-least-squares}}

The ordinary least square estimators \(\widehat{\boldsymbol{\beta}}=(\widehat{\beta}_0, \ldots, \widehat{\beta}_p)\) are the values that simultaneously minimize the Euclidean distance between the random observations \(Y_i\) and the \textbf{fitted values}
\begin{align*}
 \widehat{Y}_i &= \widehat{\beta}_0 + \widehat{\beta}_1 \mathrm{X}_{i1} + \cdots + \widehat{\beta}_p \mathrm{X}_{ip}, \qquad i =1, \ldots, n.
\end{align*}
In other words, the least square estimators are the solution of the convex optimization problem
\begin{align*}
\widehat{\boldsymbol{\beta}} &=\min_{\boldsymbol{\beta} \in \mathbb{R}^{p+1}}\sum_{i=1}^n (Y_i-\widehat{Y}_i)^2= \min_{\boldsymbol{\beta}} \|\boldsymbol{Y}-\mathbf{X}\boldsymbol{\beta}\|^2
\end{align*}
This system of equations has an explicit solution which is better expressed using matrix notation: this amounts to expressing equation \eqref{eq:olsmean} with one observation per line.

Consider the matrices
\begin{align*}
\boldsymbol{Y} =
 \begin{pmatrix}
  Y_1 \\
  Y_2 \\
  \vdots \\
  Y_n 
 \end{pmatrix} ,
 \;
 \boldsymbol{\varepsilon} =
 \begin{pmatrix}
  \varepsilon_1 \\
  \varepsilon_2 \\
  \vdots \\
  \varepsilon_n 
 \end{pmatrix} ,
 \;
\mathbf{X} = \begin{pmatrix}
\mathrm{X}_{11} & \mathrm{X}_{12} & \cdots & \mathrm{X}_{1p} \\
\mathrm{X}_{21} & \mathrm{X}_{22} & \cdots & \mathrm{X}_{2p} \\
\vdots & \vdots & \ddots & \vdots \\
\mathrm{X}_{n1} & \mathrm{X}_{n2} & \cdots & \mathrm{X}_{np} 
\end{pmatrix} , \;
\boldsymbol{\beta} =
 \begin{pmatrix}
  \beta_1 \\
  \beta_2 \\
  \vdots \\
  \beta_p 
 \end{pmatrix}
\end{align*}
The model in compact form is
\begin{align*}
\boldsymbol{Y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\varepsilon}.
\end{align*}
LThe ordinary least squares estimator solves the unconstrained optimization problem
\begin{align*}
\widehat{\boldsymbol{\beta}}=\min_{\boldsymbol{\beta} \in \mathbb{R}^{p+1}}(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})^\top(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta}).
\end{align*}
We can compute the derivative of the right hand side with respect to \(\boldsymbol{\beta}\), set it to zero and solve for \(\widehat{\boldsymbol{\beta}}\),\\
\begin{align*}
\mathbf{0}_n&=\frac{\partial}{\partial\boldsymbol{\beta}}(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})^\top(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})\\
\\&=\frac{\partial (\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})}{\partial \boldsymbol{\beta}}\frac{\partial (\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})^\top(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})}{\partial (\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})}\\
 \\&=\mathbf{X}^\top (\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})
\end{align*}
using the chain rule. Distributing the terms leads to the so-called \emph{normal equation}
\begin{align*}
 \mathbf{X}^\top \mathbf{X}\boldsymbol{\beta}&=\mathbf{X}^\top \boldsymbol{y}.
\end{align*}
If the \(n \times p\) matrix \(\mathbf{X}\) is full-rank, the quadratic form \(\mathbf{X}^\top \mathbf{X}\) is invertible and we obtain a unique solution to the optimization problem,
\begin{align*}
\widehat{\boldsymbol{\beta}} = (\mathbf{X}^{\top} \mathbf{X})^{-1} \mathbf{X}^{\top} \boldsymbol{Y}.
\end{align*}

What do these least squares represent in two dimensions? The estimator is the one minimising the sum of squared residuals: the \(i\)th \textbf{ordinary residual} \(e_i = y_i -\widehat{y}_i\) is the \emph{vertical} distance between a point \(y_i\) and the fitted value \(\widehat{y}_i\) on the line; the blue segments on Figure \ref{fig:distancevert} represent the individual vectors of residuals.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{MATH60604A_Statistical_modelling_files/figure-latex/distancevert-1} 

}

\caption{Illustration of ordinary residuals added to the regression line (blue vectors).}\label{fig:distancevert}
\end{figure}

\begin{remark}[Geometry of least squares]
\iffalse{} {Remark (Geometry of least squares). } \fi{}If we consider the \(n\) observations as a (column) vector, the term \(\mathbf{X} \widehat{\boldsymbol{\beta}}\) is the projection of the response vector \(\boldsymbol{y}\) on the linear span generated by the columns of \(\mathbf{X}\), \(\mathscr{S}_{\mathbf{X}}\). The ordinary residuals are thus orthogonal to \(\mathscr{S}_{\mathbf{X}}\) and to the fitted values, meaning \(\boldsymbol{e}^\top\widehat{\boldsymbol{y}}=0\).
A direct consequence of this fact is that the linear correlation between \(\boldsymbol{e}\) and \(\widehat{\boldsymbol{y}}\) is zero; we will use this property to build graphical diagnostics.
\end{remark}

\begin{remark}[Complexity of ordinary least squares]
\iffalse{} {Remark (Complexity of ordinary least squares). } \fi{}This is an aside: in machine learning, you will often encounter linear models fitted using a (stochastic) gradient descent algorithm. Unless your sample size \(n\) or the number of covariates \(p\) is significant (think at the Google scale), an approximate should not be prefered to the exact solution! From a numerical perspective, obtaining the least square estimates requires inverting a \(p \times p\) matrix \(\mathbf{X}^\top\mathbf{X}\), which is the most costly operation. In general, direct inversion should be avoided because it is not the most numerically stable way of obtaining the solution. \textbf{R} uses the QR decomposition, which has a complexity of \(\mathrm{O}(np^2)\). Another more stable alternative, which has the same complexity but is a bit more costly is use of a singular value decomposition.
\end{remark}

Any good software will calculate ordinary least square estimates for you. Keep in mind that there is an explicit and unique solution provided your design matrix \(\mathbf{X}\) doesn't contain collinear columns. If you have more than one explanatory variable, the fitted values lie on a hyperplan (which is hard to represent graphically). Mastering the language and technical term (fitted values, ordinary residuals, etc.) is necessary for the continuation.

\hypertarget{interpretation-of-the-model-parameters}{%
\section{Interpretation of the model parameters}\label{interpretation-of-the-model-parameters}}

What do the \(\boldsymbol{\beta}\) parameters of the linear model represent? In the simple case presented in Figure \ref{fig:droitenuage}, the equation of the line is \(\widehat{Y} = \widehat{\beta}_0 + \widehat{\beta}_1\mathrm{X}_1\), \(\beta_0\) is the intercept (the mean value of \(Y\) when \(\mathrm{X}_1=0\)) and \(\beta_1\) is the slope, i.e., the average increase of \(Y\) when \(\mathrm{X}_1\) increases by one unit.

In some cases, the intercept is \textbf{meaningless} because the value \(\mathrm{X}_1=0\) is impossible (e.g., \(\mathrm{X}_1\) represents the height of a human). In the same vein, there may be no observation in a neighborhood of \(\mathrm{X}_1=0\), even if this value is plausible, in which case the intercept is an \textbf{extrapolation}.

If the columns of \(\mathbf{X}\) are arbitrary, it is customary to include an intercept: this amounts to including \(\mathbf{1}_n\) as column of the design matrix \(\mathbf{X}\). Because the residuals are orthogonal to the columns of \(\mathbf{X}\), their mean is zero, since \(n^{-1}\mathbf{1}_n^\top\boldsymbol{e}=\bar{\boldsymbol{e}}=0\). In general, we could also obtain mean zero residuals by including a set of vectors in \(\mathbf{X}\) that span \(\mathbf{1}_n\).

In our example, the equation of the fitted line of Figure \ref{fig:droitenuage} is \[\widehat{\texttt{salary}} = 99.975 + 0.78\texttt{service}.\]
The average salary of a new professor would then be 99974.653 dollars, whereas the average annual increase for each additional year of service is 779.569 dollars.

If the response variable \(Y\) should be continuous (for the least square criterion to be meaningful), we place no such restriction on the explanatories. The case of dummies in particular is common: these variables are encoded using binary indicators (\(0/1\)). Consider for example the sex of the professors in the study:
\[\texttt{sex} = \begin{cases} 0 , & \text{for men},\\
1, & \text{for women.}
\end{cases}
\]
The equation of the simple linear model that includes the binary variable \(\texttt{sex}\) is \(\texttt{salary} = \beta_0 + \beta_1 \texttt{sex} + \varepsilon\). Let \(\mu_0\) denote the average salary of mean and \(\mu_1\) that of women. The intercept \(\beta_0\) can be interpreted as usual: it is the average salary when \(\texttt{sex}=0\), meaning that \(\beta_0=\mu_0\). We can write the equation for the conditional expectation for each sex,
\begin{align*}
\mathsf{E}(\texttt{salary} \mid \texttt{sex})= \begin{cases}
\beta_0, & \texttt{sex}=0 \text{ (men)}, \\
\beta_0 + \beta_1 & \texttt{sex}=1 \text{ (women)}.
\end{cases}
\end{align*}
A linear model that only contains a binary variable \(\mathrm{X}\) as regressor amounts to specifying a different mean for each of two groups: the average of women is \(\mathsf{E}(\texttt{salary} \mid \texttt{sex}=1) = \beta_0 + \beta_1 = \mu_1\) and \(\beta_1=\mu_1-\mu_0\) represents the difference between the average salary of men and women. The least square estimator \(\widehat{\beta}_0\) is the sample mean of men and \(\widehat{\beta}_1\) is the difference of the sample mean of women and men. The parametrization of the linear model with \(\beta_0\) and \(\beta_1\) is in terms of \textbf{contrasts} and is particularly useful if we want to test for mean difference between the groups, as this amounts to testing \(\mathscr{H}_0: \beta_1=0\). If we wanted our model to directly output the sample means, we would need to replace the design matrix \(\mathbf{X}=[\mathbf{1}_n, \texttt{sex}]\) by \([\mathbf{1}_n- \texttt{sex}, \texttt{sex}]\). The fitted model would be the same because they span the same 2D subspace, but this is not recommended because software treat cases without intercept differently and it can lead to unexpected behavior (more on this latter).

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{MATH60604A_Statistical_modelling_files/figure-latex/graphcollegesex-1} 

}

\caption{Simple linear model for the $\texttt{college}$ data using the binary variable sex as regressor: even if the equation defines a line, only its values in $0/1$ are realistic.}\label{fig:graphcollegesex}
\end{figure}

If we fit the model with sex only to the \texttt{college} data, we find that the average salary of men is \(\widehat{\beta}_0=\ensuremath{1.151\times 10^{5}}\) USD and the mean difference estimate of the salary between women and men is \(\widehat{\beta}_1=14088.009\) dollars. Since the estimate is negative, this means women are paid less. Bear in mind that the model is not adequate for determining if there are gender inequalities in the salary distribution: \ref{fig:droitenuage} shows that the number of years of service and the academic rank strongly impact wages, yet the distribution of men and women is not the same within each rank.

Even if the linear model defines a line, the latter is only meaningful when evaluated at \(0\) or \(1\); Figure \ref{fig:graphcollegesex} shows it in addition to sample observations (jittered) and a density estimate for each sex. The colored dot represents the mean, showing that the line does indeed pass through the mean of each group.

A binary indicator is a categorical variable with two levels, so we could extend our reasoning and fit a model with a categorical explanatory variable with \(k\) levels. To do this, we add \(k-1\) indicator variables plus the intercept: if we want to model a different mean for each of the \(k\) groups, it is logical to only include \(k\) parameters in the mean model. We will choose, as we did with sex, a \textbf{reference category} or \textbf{baseline} whose average will be encoded by the intercept \(\beta_0\). The other parameters \(\beta_1, \ldots, \beta_{k-1}\) are contrasts relative to the baseline. The college data includes the ordinal variable \texttt{rank}, which has three levels (assistant, associate and full). We thus need two binary variables, \(\mathrm{X}_1 = \mathsf{I}(\texttt{rank}=\texttt{associate})\) and \(\mathrm{X}_2 = \mathsf{I}(\texttt{rank}=\texttt{full})\); the \(i\)th element of the vector \(\mathrm{X}_1\) is one for an associate professor and zero otherwise. The linear model is
\begin{align*}
\texttt{salary} =\beta_0 + \beta_1 \mathrm{X}_1+\beta_2\mathrm{X}_2 + \varepsilon,
\end{align*}
and the conditional expectation of salary
\begin{align*}
\mathsf{E}(\texttt{salary} \mid \texttt{rank})= \begin{cases}
\beta_0, & \texttt{rank}=\texttt{assistant},\\
\beta_0 + \beta_1 & \texttt{rank}=\texttt{associate},\\
\beta_0 + \beta_2 & \texttt{rank}=\texttt{full},
\end{cases}
\end{align*}
Thus \(\beta_1\) (respectively \(\beta_2\)) are the difference between the average salary of associate (respectively full) professors and assistant professors. The choice of the baseline category is arbitrary and all choices yield the same model: only the interpretation changes from one parametrization to the next. For an ordinal variable, it is recommended to choose the smallest or the largest category to ease comparisons.

The models we have fitted so far are not adequate because they ignore variables that are necessarily to correctly explain variations in salaries: Figure \ref{fig:edacollege} show for example that rank is critical for explaining the salary variations in the college. We should thus fit a model that include those simultaneously to investigate the gender gap (which consists of differences that are unexplained by other factors). Before doing this, we come back to the interpretation of the parameters in the multiple linear regression setting.

Consider the model \(Y= \beta_0 + \beta_1 \mathrm{X}_1 + \cdots + \beta_p\mathrm{X}_p + \varepsilon\). The intercept \(\beta_0\) represents the mean value of \(Y\) when \emph{all} of the covariates are set to zero,
\begin{align*}
\beta_0 &= \mathsf{E}(Y \mid \mathrm{X}_1=0,\mathrm{X}_2=0,\ldots,\mathrm{X}_p=0).
\end{align*}
For categorical variables, this yields the baseline, whereas we fix the continous variables to zero: again, this may be nonsensical depending on the study. The coefficient \(\beta_j\) \((j \geq 1)\) can be interpreted as the mean increase of the response \(Y\) when \(\mathrm{X}_j\) increases by one unit, all other things being equal (\emph{ceteris paribus}); e.g.,
\begin{align*}
\beta_1 &= \mathsf{E}(Y \mid \mathrm{X}_1=x_1+1,\mathrm{X}_2=x_2,\ldots,\mathrm{X}_p=x_p) \\
& \qquad \qquad - \mathsf{E}(Y \mid \mathrm{X}_1=x_1,\mathrm{X}_2=x_2,\ldots,\mathrm{X}_p=x_p) \\
&= \left\{\beta_0 + \beta_1 (x_1+1) + \beta_2 x_2 + \cdots +\beta_p \mathrm{X}_p \right\} \\
& \qquad \qquad -\left\{\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots +\beta_p \mathrm{X}_p \right\} 
\end{align*}
It is not always possible to fix the value of an explanatory if multiple columns of \(\mathbf{X}\) contains functions/transformations of it. For example, if we included a polynomial of order \(k\) for some variable \(\mathrm{X}\),
\begin{align*}
Y=\beta_0+ \beta_1 \mathrm{X}+ \beta_2 \mathrm{X}^2 + \ldots +\beta_k \mathrm{X}^k + \varepsilon.
\end{align*}
If we include a term of order \(k\), \(\mathrm{X}^k\), we must \emph{always} include the lower order terms \(1, \mathrm{X}, \ldots, \mathrm{X}^{k-1}\) to make sure the resulting model is interpretable (otherwise, it amounts to a particular class of polynomials with some zero coefficients). Interpreting nonlinear effects (even polynomials, for which \(k\leq 3\) in practice), is complicated because the effect of an increase of one unit of \(\mathrm{X}\) \emph{depends of the value of the latter}.

\begin{example}[Auto data]
\protect\hypertarget{exm:auto}{}{\label{exm:auto} \iffalse (Auto data) \fi{} }We consider a linear regression model for the fuel autonomy of cars as a function of the power of their motor (measured in horsepower) from the \texttt{auto} dataset. The postulated model,
\[
\texttt{mpg}_i = \beta_0 + \beta_1 \texttt{horsepower}_i + \beta_2 \texttt{horsepower}_i^2 + \varepsilon_i,
\]
includes a quadratic term. Figure \ref{fig:autoquad2d} shows the scatterplot with the fitted regression line, above which the line for the simple linear regression for horsepower is added.
\end{example}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{MATH60604A_Statistical_modelling_files/figure-latex/autoquad2d-1} 

}

\caption{Linear regression models for the fuel autonomy of cars as a function of motor power}\label{fig:autoquad2d}
\end{figure}

It appears graphically that the quadratic model fits better than the simple linear alternative: we will assess this hypothesis formally later. For the degree two polynomial, Figure \ref{fig:autoquad2d} show that fuel autonomy decreases rapidly when power increases between 50 to 100, then more slow until 189.35 hp. After that, the model postulates that autonomy increases again as evidenced by the scatterplot, but beware of extrapolating (weird things can happen beyond the range of the data, as exemplified by \href{https://livefreeordichotomize.com/2020/05/05/model-detective/}{Hassett's cubic model for the number of daily cases of Covid19 in the USA}).

The representation in Figure \ref{fig:autoquad2d} may seem counter-intuitive given that we fit a linear model, but it is a 2D projection of 3D coordinates for the equation \(\beta_0 + \beta_1x-y +\beta_2z =0\), where \(x=\texttt{horsepower}\), \(z=\texttt{horsepower}^2\) and \(y=\texttt{mpg}\). Physics and common sense force \(z = x^2\), and so the fitted values lie on a curve in a 2D subspace of the fitted plan, as shown in grey in the 3D Figure \ref{fig:hyperplan}.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/hyperplane_auto} 

}

\caption{3D graphical representation of the linear regression model for the $	exttt{auto}$ data.}\label{fig:hyperplan}
\end{figure}

\begin{remark}[There are better alternatives to polynomials for modelling nonlinear effects]
\iffalse{} {Remark (There are better alternatives to polynomials for modelling nonlinear effects). } \fi{}Generally speaking, one uses flexible basis vectors (splines) rather than polynomials for smoothing when the relation between the response \(Y\) and an explanatory variable \(\mathrm{X}\) is nonlinear; these models involve many covariates and it is customary to add a penalty term to control for overfitting and wiggliness. A better (physical) understanding of the system, or a theoretical model can also guide the choice of functions to use.
\end{remark}

The coefficient \(\beta_j\) in Eq. \eqref{eq:linearreg} represents the \emph{marginal} contribution of \(\mathrm{X}_j\) when all the other covariates are included in the model and which is not explained by them. This can be represented graphically by projecting \(Y\) and \(\mathrm{X}_j\) in the orthogonal complement of \(\mathbf{X}_{-j}\) (the matrix containing all but the \(j\)th column \(\mathrm{X}_j\)). The \textbf{added-variable plot} is a graphical tool showing this projection: the residuals from the linear regression of \(Y\) onto \(\mathscr{S}(\mathbf{X}_{-j})\) are mapped to the \(y\)-axis, whereas the residuals from the linear regression of \(\mathrm{X}_j\) as a function of \(\mathbf{X}_{-j}\) are shown on the \(x\)-axis. The regression line passes through (\(0,0\)) and its slope is \(\hat{\beta}_j\). This graphical diagnostic is useful for detecting collinearity and the impact of outliers.

\begin{example}[Wage inequality in an American college]
\protect\hypertarget{exm:inequite-salariale}{}{\label{exm:inequite-salariale} \iffalse (Wage inequality in an American college) \fi{} }We consider a multiple regression model for the \texttt{college} data that includes sex, academic rank, field of study and the number of years of service as regressors.
\end{example}

If we multiply \(\texttt{salary}\) by a thousand to get the resulting estimates in US dollars, the postulated model is
\begin{align*}
\texttt{salary}\times 1000 &= \beta_0 + \beta_1 \texttt{sex}_{\texttt{woman}} +\beta_2 \texttt{field}_{\texttt{theoretical}} \\&\quad +\beta_3 \texttt{rank}_{\texttt{associate}}
+\beta_4 \texttt{rank}_{\texttt{full}}  +\beta_5 \texttt{service} + \varepsilon.
\end{align*}

\begin{table}

\caption{\label{tab:collegecoefs}Estimated coefficients of the linear model for the $\texttt{college}$ (in USD, rounded to the nearest dollar).}
\centering
\begin{tabular}[t]{rrrrrr}
\toprule
$\widehat{\beta}_0$ & $\widehat{\beta}_1$ & $\widehat{\beta}_2$ & $\widehat{\beta}_3$ & $\widehat{\beta}_4$ & $\widehat{\beta}_5$\\
\midrule
86596 & -4771 & -13473 & 14560 & 49160 & -89\\
\bottomrule
\end{tabular}
\end{table}

The interpretation of the coefficients is as follows:

\begin{itemize}
\tightlist
\item
  The estimated intercept is \(\widehat{\beta}_0=86596\) dollars; it corresponds to the mean salary of men assistant professors who just started the job and works in an applied domain.
\item
  everything else being equal (same field, academic rank, and number of years of service), the estimated salary difference between a woman and is estimated at \(\widehat{\beta}_1=-4771\) dollars.
\item
  \emph{ceteris paribus}, the salary difference between a professor working in a theoretical field and one working in an applied field is \(\beta_2\) dollars: our estimate of this difference is \(-13473\) dollars, meaning applied pays more than theoretical.
\item
  \emph{ceteris paribus}, the estimated mean salary difference between associate and assistant professors is \(\widehat{\beta}_3=14560\) dollars.
\item
  \emph{ceteris paribus}, the estimated mean salary difference between full and assistant professors is \(\widehat{\beta}_4=49160\) dollars.
\item
  au sein d'un même échelon, chaque année supplémentaire de service mène à une augmentation de salary annuelle moyenne de \(\widehat{\beta}_5=-89\) dollars.
\end{itemize}

All other factors taken into account, women get paid less than men. It remains to see if this difference is statistically significant. Perhaps more surprising, the model estimates that salary decreases with every additional year of service: this seems counterintuitive when looking at Figure \ref{fig:droitenuage}, which showed a steady increas of salary over the years. However, this graphical representation is misleading because Figure \ref{fig:edacollege} showed that academic ranking mattered the most. Once all the other factors are accounted for, years of service serves to explain the salary of full professors who have been working unusual amounts of time and who gain less than the average full professor, as shown by the added-variable plot of Figure \ref{fig:avplotcollege}.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{MATH60604A_Statistical_modelling_files/figure-latex/avplotcollege-1} 

}

\caption{Added-variable plot for years of service in the linear regression model of the  $\texttt{college}$ data.}\label{fig:avplotcollege}
\end{figure}

\hypertarget{r-interlude-the-lm-function}{%
\section{\texorpdfstring{\textbf{R} interlude: the \texttt{lm} function}{R interlude: the lm function}}\label{r-interlude-the-lm-function}}

The function \texttt{lm} is the workshorse for fitting linear models in \textbf{R}. It takes as input a formula: suppose you have a data frame containing columns \texttt{x} (a regressor) and \texttt{y} (the regressand); you can then call \texttt{lm(y\ \textasciitilde{}\ x)} to fit the linear model \(y = \beta_0 + \beta_1 x + \varepsilon\). The explanatory variable \texttt{y} is on the left hand side,
while the right hand side should contain the predictors, separated by a \texttt{+} sign if there are more than one.
If you provide the data frame name using \texttt{data}, then the shorthand \texttt{y\ \textasciitilde{}\ .} fits all the columns of the data frame (but \texttt{y}) as regressors.

If you include categorical variables, make sure they are transformed to factors. Normally, strings are cast to factor (unless you specify \texttt{stringsAsFactors\ =\ FALSE}) upon import of the data, but the danger here lies with variables that are encoded using integers (sex, revenue class, level of education, marital status, etc.) It is okay if we keep binary variables as is if they are encoded using \(0/1\), but it is often better to cast them to factor to get more meaningful labels given the lack of obvious ordering.

By default, the baseline level for a factor is based on the alphabetical order, while \textbf{SAS} uses the first value it encounters. Once the variables are cast to factor, \texttt{summary} will print the counts for each respective categories; these could be likewise be obtained using \texttt{table}.

To fit higher order polynomials or transformations, use the \texttt{I} function to tell \textbf{R} to interpret the input ``as is''.
Thus, \texttt{lm(y\textasciitilde{}x+I(x\^{}2))}, would fit a linear model with design matrix \((\boldsymbol{1}_n, \mathbf{x}^\top, \mathbf{x}^2)^\top\). A constant is automatically included in the regression, but can be removed by writing \texttt{-1} or \texttt{+0} on the right hand side of the formula (but don't do that!). The \texttt{lm} function output will display ordinary least squares estimates along with standard errors, \(t\) values for the Wald test of the hypothesis \(\mathrm{H}_0: \beta_i=0\) and the associated \(P\)-values. Other statistics and information about the sample size, the degrees of freedom, etc., are given at the bottom of the table.

Many methods allow you to extract specific objects from \texttt{lm} objects. For example, the functions \texttt{coef}, \texttt{resid}, \texttt{fitted}, \texttt{model.matrix} will return the coefficients \(\widehat{\boldsymbol{\beta}}\), the ordinary residuals \(\boldsymbol{e}\), the fitted values \(\widehat{\boldsymbol{y}}\) and the design matrix \(\mathbf{X}\), respectively.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{data}\NormalTok{(college, }\DataTypeTok{package =} \StringTok{"hecstatmod"}\NormalTok{) }\CommentTok{\#load data}
\KeywordTok{class}\NormalTok{(college}\OperatorTok{$}\NormalTok{rank) }\CommentTok{\#check that rank is a factor}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "factor"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#if not, use the following "\textless{}{-}" to assign, "$" to access the column of a data.frame/list}
\CommentTok{\# college$rank \textless{}{-} factor(college$rank, labels = c("assistant","associate","full"))}
\NormalTok{linmod \textless{}{-}}\StringTok{ }\KeywordTok{lm}\NormalTok{(salary }\OperatorTok{\textasciitilde{}}\StringTok{ }\NormalTok{sex }\OperatorTok{+}\StringTok{ }\NormalTok{rank }\OperatorTok{+}\StringTok{ }\NormalTok{service }\OperatorTok{+}\StringTok{ }\NormalTok{field, }\DataTypeTok{data =}\NormalTok{ college)}
\KeywordTok{coef}\NormalTok{(linmod) }\CommentTok{\#coefficients}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      (Intercept)         sexwoman    rankassociate         rankfull 
##          86.5963          -4.7712          14.5604          49.1596 
##          service fieldtheoretical 
##          -0.0888         -13.4734
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(linmod) }\CommentTok{\#summary table}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = salary ~ sex + rank + service + field, data = college)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -64.20 -14.26  -1.53  10.57  99.16 
## 
## Coefficients:
##                  Estimate Std. Error t value Pr(>|t|)    
## (Intercept)       86.5963     2.9603   29.25  < 2e-16 ***
## sexwoman          -4.7712     3.8780   -1.23  0.21931    
## rankassociate     14.5604     4.0983    3.55  0.00043 ***
## rankfull          49.1596     3.8345   12.82  < 2e-16 ***
## service           -0.0888     0.1116   -0.80  0.42696    
## fieldtheoretical -13.4734     2.3155   -5.82  1.2e-08 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 22.7 on 391 degrees of freedom
## Multiple R-squared:  0.448,	Adjusted R-squared:  0.441 
## F-statistic: 63.4 on 5 and 391 DF,  p-value: <2e-16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{confint}\NormalTok{(linmod) }\CommentTok{\#confidence intervals for model parameters}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                    2.5 % 97.5 %
## (Intercept)       80.776 92.416
## sexwoman         -12.396  2.853
## rankassociate      6.503 22.618
## rankfull          41.621 56.698
## service           -0.308  0.131
## fieldtheoretical -18.026 -8.921
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{yhat \textless{}{-}}\StringTok{ }\KeywordTok{fitted}\NormalTok{(linmod) }\CommentTok{\#fitted values}
\NormalTok{e \textless{}{-}}\StringTok{ }\KeywordTok{resid}\NormalTok{(linmod) }\CommentTok{\#ordinary residuals}
\end{Highlighting}
\end{Shaded}

\hypertarget{likelihood}{%
\chapter{Likelihood-based inference}\label{likelihood}}

\hypertarget{generalized-linear-models}{%
\chapter{Generalized linear models}\label{generalized-linear-models}}

\hypertarget{correlated-longitudinal-data}{%
\chapter{Correlated and longitudinal data}\label{correlated-longitudinal-data}}

\hypertarget{linear-mixed-models}{%
\chapter{Linear mixed models}\label{linear-mixed-models}}

\hypertarget{survival}{%
\chapter{Survival analysis}\label{survival}}

\hypertarget{appendix-appendix}{%
\appendix}


\hypertarget{r}{%
\chapter*{\texorpdfstring{\textbf{R}}{R}}\label{r}}
\addcontentsline{toc}{chapter}{\textbf{R}}

\textbf{R} is an object-oriented interpreted language. It differs from usual programming languages in that it is designed for interactive analyses.

You can find several introductions to \textbf{R} online. Have a look at the \href{https://cran.r-project.org/manuals.html}{\textbf{R} manuals} or better at \href{https://cran.r-project.org/other-docs.html}{contributed manuals}. A nice official reference is \href{http://colinfay.me/intro-to-r/index.html}{An introduction to \textbf{R}}.
You may wish to look up the following chapters of the \textbf{R} language definition (\href{http://colinfay.me/r-language-definition/evaluation-of-expressions.html}{Evaluation of expressions} and part of the \href{http://colinfay.me/r-language-definition/objects.html}{\emph{Objects} chapter}). Another good (small reference) is the cheatsheet \href{https://eddelbuettel.github.io/gsir-te/Getting-Started-in-R.pdf}{Getting started in \textbf{R}}.

\hypertarget{basics-of-r}{%
\section{\texorpdfstring{Basics of \textbf{R}}{Basics of R}}\label{basics-of-r}}

\hypertarget{help}{%
\subsection{Help}\label{help}}

Help can be accessed via \texttt{help} or simply \texttt{?}, e.g., \texttt{help("Normal")}. \href{https://www.r-project.org/help.html}{See \textbf{R} page about help files}.

\hypertarget{basic-commands}{%
\subsection{Basic commands}\label{basic-commands}}

Basic \textbf{R} commands are fairly intuitive, especially if you want to use \textbf{R} as a calculator.
Elementary functions such as \texttt{sum}, \texttt{min}, \texttt{max}, \texttt{sqrt}, \texttt{log}, \texttt{exp}, etc., are self-explanatory.

Some (unconventional) features of the language:

\begin{itemize}
\tightlist
\item
  \textbf{R} is case sensitive.
\item
  Use \texttt{\textless{}-} for assignments to a variable, and \texttt{=} for matching arguments inside functions
\item
  Indexing in \textbf{R} starts at 1, \textbf{not} 0.
\item
  Most functions in \textbf{R} are vectorized and loops are typically inefficient.
\item
  Integers are obtained by appending \texttt{L} to the number, so \texttt{2L} is an integer and \texttt{2} a double (\texttt{numerical}).
\end{itemize}

Besides integers and doubles, the common types are

\begin{itemize}
\tightlist
\item
  logical (\texttt{TRUE} and \texttt{FALSE});
\item
  null pointers (\texttt{NULL}), which can be assigned to arguments;
\item
  missing values, namely \texttt{NA} or \texttt{NaN}. These can also be obtained a result of invalid mathematical operations such as \texttt{log(-2)}.
\end{itemize}

Beware! In \textbf{R}, invalid calls will often returns \emph{something} rather than an error. It is therefore good practice to check that the output is sensical.

\hypertarget{linear-algebra-in-r}{%
\subsection{\texorpdfstring{Linear algebra in \textbf{R}}{Linear algebra in R}}\label{linear-algebra-in-r}}

\textbf{R} is an object oriented language, and the basic elements in \textbf{R} are (column) vector. Below is a glossary with some useful commands for performing basic manipulation of vectors and matrix operations:

\begin{itemize}
\tightlist
\item
  \texttt{c} concatenates elements to form a vector
\item
  \texttt{cbind} (\texttt{rbind}) binds column (row) vectors
\item
  \texttt{matrix} and \texttt{vector} are constructors
\item
  \texttt{diag} creates a diagonal matrix (by default with ones)
\item
  \texttt{t} is the function for transpose
\item
  \texttt{rep} creates a vector of duplicates, \texttt{seq} a sequence. For integers \(i\), \(j\) with \(i<j\), \texttt{i:j} generates the sequence \(i, i+1, \ldots, j-1, j\).
\end{itemize}

Subsetting is fairly intuitive and general; you can use vectors, logical statements. For example, if \texttt{x} is a vector,
then

\begin{itemize}
\tightlist
\item
  \texttt{x{[}2{]}} returns the second element
\item
  \texttt{x{[}-2{]}} returns all but the second element
\item
  \texttt{x{[}1:5{]}} returns the first five elements
\item
  \texttt{x{[}(length(x)\ -\ 5):length(x){]}} returns the last five elements
\item
  \texttt{x{[}c(1,\ 2,\ 4){]}} returns the first, second and fourth element
\item
  \texttt{x{[}x\ \textgreater{}\ 3{]}} return any element greater than 3. Possibly an empty vector of length zero!
\item
  \texttt{x{[}\ x\ \textless{}\ -2\ \textbar{}\ x\ \textgreater{}\ 2{]}} multiple logical conditions.
\item
  \texttt{which(x\ ==\ max(x))} index of elements satisfying a logical condition.
\end{itemize}

For a matrix \texttt{x}, subsetting now involves dimensions: \texttt{{[}1,2{]}} returns the element in the first row, second column. \texttt{x{[},2{]}} will return all of the rows, but only the second column. For lists, you can use \texttt{{[}{[}} for subsetting by index or the \texttt{\$} sign by names.

\hypertarget{packages}{%
\subsection{Packages}\label{packages}}

The great strength of \textbf{R} comes from its contributed libraries (called packages), which contain functions and datasets provided by third parties. Some of these (\texttt{base}, \texttt{stats}, \texttt{graphics}, etc.) are loaded by default whenever you open a session.

To install a package from CRAN, use \texttt{install.packages("package")}, replacing \texttt{package} by the package name. Once installed, packages can be loaded using \texttt{library(package)}; all the functions in \texttt{package} will be available in the environment.

\begin{rmdcaution}
There are drawbacks to loading packages: if an object with the same name from another package is already present in your environment, it will be hidden. Use the double-colon operator \texttt{::} to access a single object from an installed package (\texttt{package::object}).
\end{rmdcaution}

\hypertarget{datasets}{%
\subsection{Datasets}\label{datasets}}

\begin{itemize}
\tightlist
\item
  datasets are typically stored inside a \texttt{data.frame}, a matrix-like object whose columns contain the variables and the rows the observation vectors.
\item
  The columns can be of different types (\texttt{integer}, \texttt{double}, \texttt{logical}, \texttt{character}), but all the column vectors must be of the same length.
\item
  Variable names can be displayed by using \texttt{names(faithful)}.
\item
  Individual columns can be accessed using the column name using the \texttt{\$} operator. For example, \texttt{faithful\$eruptions} will return the first column of the \texttt{faithful} dataset.
\item
  To load a dataset from an (installed) \textbf{R} package, use the command \texttt{data} with the name of the \texttt{package} as an argument (must be a string). The package \texttt{datasets} is loaded by default whenever you open \textbf{R}, so these are always in the search path.
\end{itemize}

The following functions can be useful to get a quick glimpse of the data:

\begin{itemize}
\tightlist
\item
  \texttt{summary} provides descriptive statistics for the variable.
\item
  \texttt{str} provides the first few elements with each variable, along with the dimension
\item
  \texttt{head} (\texttt{tail}) prints the first (last) \(n\) lines of the object to the console (default is \(n=6\)).
\end{itemize}

We start by loading a dataset of the Old Faithful Geyser of Yellowstone National park and looking at its entries.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load Old faithful dataset}
\KeywordTok{data}\NormalTok{(faithful, }\DataTypeTok{package =} \StringTok{"datasets"}\NormalTok{)}
\CommentTok{\# Query the database for documentation}
\NormalTok{?faithful}
\CommentTok{\# look at first entries}
\KeywordTok{head}\NormalTok{(faithful)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   eruptions waiting
## 1      3.60      79
## 2      1.80      54
## 3      3.33      74
## 4      2.28      62
## 5      4.53      85
## 6      2.88      55
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(faithful)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 'data.frame':	272 obs. of  2 variables:
##  $ eruptions: num  3.6 1.8 3.33 2.28 4.53 ...
##  $ waiting  : num  79 54 74 62 85 55 88 85 51 85 ...
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# What kind of object is faithful? }
\KeywordTok{class}\NormalTok{(faithful)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "data.frame"
\end{verbatim}

Other common classes of objects:

\begin{itemize}
\tightlist
\item
  \texttt{matrix}: an object with attributes \texttt{dim}, \texttt{ncol} and \texttt{nrow} in addition to \texttt{length}, which gives the total number of elements.
\item
  \texttt{array}: a higher dimensional extension of \texttt{matrix} with arguments \texttt{dim} and \texttt{dimnames}.
\item
  \texttt{list}: an unstructured class whose elements are accessed using double indexing \texttt{{[}{[}\ {]}{]}} and elements are typically accessed using \texttt{\$} symbol with names. To delete an element from a list, assign \texttt{NULL} to it.
\item
  \texttt{data.frame} is a special type of list where all the elements are vectors of potentially different type, but of the same length.
\end{itemize}

\hypertarget{graphics}{%
\subsection{Graphics}\label{graphics}}

The \texttt{faithful} dataset consists of two variables: the regressand \texttt{waiting} and the regressor \texttt{eruptions}. One could postulate that the waiting time between eruptions will be smaller if the eruption time is small, since pressure needs to build up for the eruption to happen. We can look at the data to see if there is a linear relationship between the variables.

An image is worth a thousand words and in statistics, visualization is crucial. Scatterplots are produced using the function \texttt{plot}. You can control the graphic console options using \texttt{par} --- see \texttt{?plot} and \texttt{?par} for a description of the basic and advanced options available.

Once \texttt{plot} has been called, you can add additional observations as points (lines) to the graph using \texttt{point} (\texttt{lines}) in place of \texttt{plot}. If you want to add a line (horizontal, vertical, or with known intercept and slope), use the function \texttt{abline}.

Other functions worth mentioning for simple graphics:

\begin{itemize}
\tightlist
\item
  \texttt{boxplot} creates a box-and-whiskers plot
\item
  \texttt{hist} creates an histogram, either on frequency or probability scale (option \texttt{freq\ =\ FALSE}). \texttt{breaks} control the number of bins. \texttt{rug} adds lines below the graph indicating the value of the observations.
\item
  \texttt{pairs} creates a matrix of scatterplots, akin to \texttt{plot} for data frame objects.
\end{itemize}

\begin{rmdnote}
There are two options for basic graphics: the base graphics package and the package \texttt{ggplot2}. The latter is a more recent proposal that builds on a modular approach and is more easily customizable --- I suggest you stick to either and \texttt{ggplot2} is a good option if you don't know \textbf{R} already, as the learning curve will be about the same. Even if the display from \texttt{ggplot2} is nicer, this is no excuse for not making proper graphics. Always label the axis and include measurement units!
\end{rmdnote}

\hypertarget{complement}{%
\chapter{Mathematical concepts}\label{complement}}

\hypertarget{population-sample}{%
\section{Population and samples}\label{population-sample}}

Statistics is the science of uncertainty quantification: of paramount importance is the notion of randomness. Generally, we will seek to estimate characteristics of a population using only a sample (a sub-group of the population of smaller size).

The \textbf{population of interest} is a collection of individuals which the study targets. For example, the Labour Force Survey (LFS) is a monthly study conducted by Statistics Canada, who define the target population as ``all members of the selected household who are 15 years old and older, whether they work or not.'' Asking every Canadian meeting this definition would be costly and the process would be long: the characteristic of interest (employment) is also a snapshot in time and can vary when the person leaves a job, enters the job market or become unemployed.

In general, we therefore consider only \textbf{samples} to gather the information we seek to obtain. The purpose of \textbf{statistical inference} is to draw conclusions about the population, but using only a share of the latter and accounting for sources of variability. George Gallup made this great analogy between sample and population:

\begin{quote}
One spoonful can reflect the taste of the whole pot, if the soup is well-stirred
\end{quote}

A \textbf{sample} is a random sub-group of individuals drawn from the population. Creation of sampling plans is a complex subject and semester-long sampling courses would be required to evens scratch the surface of the topic. Even if we won't be collecting data, keep in mind the following information: for a sample to be good, it must be representative of the population under study. Selection bias must be avoided, notably samples of friends or of people sharing opinions.

Because the individuals are selected at \textbf{random} to be part of the sample, the measurement of the characteristic of interest will also be random and change from one sample to the next. However, larger samples of the same quality carry more information and our estimator will be more precise. Sample size is not guarantee of quality, as the following example demonstrates.

\begin{example}
\protect\hypertarget{exm:Galluppoll}{}{\label{exm:Galluppoll} }
\emph{The Literary Digest} surveyed 10 millions people by mail to know voting preferences for the 1936 USA Presidential Election. A sizeable share, 2.4 millions answered, giving Alf Landon (57\%) over incumbent President Franklin D. Roosevelt (43\%). The latter nevertheless won in a landslide election with 62\% of votes cast, a 19\% forecast error. \href{https://www.jstor.org/stable/2749114}{Biased sampling and differential non-response are mostly responsible for the error:}) the sampling frame was built using ``phone number directories, drivers' registrations, club memberships, etc.'\,', all of which skewed the sample towards rich upper class white people more susceptible to vote for the GOP.

In contrast, Gallup correctly predicted the outcome by polling (only) 50K inhabitants. \href{https://medium.com/@ozanozbey/how-not-to-sample-11579793dac}{Read the full story here.}
\end{example}

\hypertarget{random-variable}{%
\section{Random variable}\label{random-variable}}

Suppose we wish to describe the behaviour of a stochastic phenomenon. To this effect, one should enumerate the set of possible values taken by the variable of interest and their probability: this is what is encoded in the distribution. We will distinguish between two cases: discrete and continuous variables. Random variables are denoted using capital letters: for example \(Y \sim \mathsf{No}(\mu, \sigma^2)\) indicates that \(Y\) follows a normal distribution with parameters \(\mu\) and \(\sigma^2\), which represent respectively the expectation and variance of \(Y\).

The (cumulative) distribution function \(F(y)\) gives the cumulative probability that an event doesn't exceed a given numerical value \(y\), \(F(y) = \mathsf{Pr}(Y \leq y)\).

If \(Y\) is discrete, then it has atoms of non-zero probability and the mass function \(f(y)=\mathsf{Pr}(Y=y)\) gives the probability of each outcome \(y\). In the continuous case, no numerical value has non-zero probability and so we consider intervals instead: the density function gives the probability of \(Y\) falling in a set \(B\), via \(\mathsf{Pr}(Y \in B) = \int_B f(y) \mathrm{d} y\). It follows that the distribution function of a continuous random variable is simply \(F(y) = \int_{-\infty}^y f(x) \mathrm{d} x\).

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/02-ttest-DF_illustration} 

}

\caption{(Cumulative) distribution functions (top) and density/mass functions (bottom) of continuous (left) and discrete (right) random variables.}\label{fig:distributions}
\end{figure}

\hypertarget{moments}{%
\section{Moments}\label{moments}}

One of the first topics covered in introductory statistics is descriptive statistics such as the mean and standard deviation. These are estimators of (centered) moments, which characterise a random varaible. In the case of the standard normal distribution, the expectation and variance fully characterize the distribution.

Let \(Y\) be a random variable with density (or mass) function \(f(x)\). This function is non-negative and satisfies \(\int_{\mathbb{R}} f(x) \mathrm{d}x=1\): the integral over a set \(B\) gives the probability of \(Y\) falling inside \(B \in \mathbb{R}\).

The expectation of a continuous random variable \(Y\) is \[\mathsf{E}(Y)=\int_{\mathbb{R}} x f(x) \mathrm{d} x.\]
Expectation is the ``theoretical mean'' in the discrete case, we set rather \(\mu = \mathsf{E}(Y)=\sum_{x \in \mathcal{X}} x \mathsf{Pr}(X=x)\), where \(\mathcal{X}\) stands for the support of \(Y\), which is the set of numerical values at which the probability of \(Y\) is non-zero. More generally, we can look at the expectation of a function \(g(x)\) for \(Y\), which is nothing but the integral (or sum in the discrete case) of \(g(x)\) weighted by the density or mass function of \(f(x)\). In the same fashion, provided the integral is finite, the variance is
\[\mathsf{Va}(Y)=\mathsf{E}\{Y-\mathsf{E}(Y)\}^2 \equiv \int_{\mathbb{R}} (x-\mu)^2 f(x) \mathrm{d} x.\]

An estimator \(\hat{\theta}\) for a parameter \(\theta\) is unbiased if its bias \(\mathsf{bias}(\hat{\theta})=\mathsf{E}(\hat{\theta})- \theta\) is zero.
The unbiased estimator of the mean of \(Y\) is \(\overline{Y}_n = n^{-1} \sum_{i=1}^n Y_i\) and that of the variance is \(S_n = (n-1)^{-1} \sum_{i=1}^n (Y_i-\overline{Y})^2\). While unbiasedness of an estimator is a desirable property, but may not be optimal. There may even be cases where no unbiased estimator exists for a parameter!

Often, rather, we seek to balance bias and variance: recall that an estimator is a function of random variables and thus it is itself random: even if it is unbiased, the numerical value obtained will vary from one sample to the next. We often seek an estimator that minimises the mean squared error, \[\mathsf{MSE}(\hat{\theta}) = \mathsf{E}\{(\hat{\theta}-\theta)^2\}=\mathsf{Va}(\hat{\theta}) + \{\mathsf{E}(\hat{\theta})\}^2.\]
The mean squared error is an objective function consisting of the sum of the squared bias and the variance.

An alternative to this criterion is to optimize a function such as the likelihood of the sample: the resulting estimator is termed maximum likelihood estimator. These estimator are asymptotically efficient, in the sense that they have the lowest mean squared error of all estimators for large samples. Other properties of maximum likelihood estimators make them attractive default choice for estimation.

The likelihood of a sample is the joint density of the \(n\) observations, which requires a distribution to be considered. Many such distributions describe simple physical phenomena and can be described using a few parameters: we only cover the most frequently encountered.

\begin{example}[Bernoulli distribution]
\protect\hypertarget{exm:bernoullidist}{}{\label{exm:bernoullidist} \iffalse (Bernoulli distribution) \fi{} }We consider a binary event such as coin toss (heads/tails). In general, the two events are associated with success/failure. By convention, failures are denoted by zeros and successes by ones, the probability of success being \(\pi\) so \(\mathsf{Pr}(Y=1)=\pi\) and \(\mathsf{Pr}(Y=0)=1-\pi\) (complementary event). The mass function of the \href{https://en.wikipedia.org/wiki/Bernoulli_distribution}{Bernoulli distribution} is thus
\begin{align*}
\mathsf{Pr}(Y=y) = \pi^y (1-\pi)^{1-y}, \quad y=0, 1.
\end{align*}
A rapid calculation shows that \(\mathsf{E}(Y)=\pi\) and \(\mathsf{Va}(Y)=\pi(1-\pi)\).
Many research questions have binary responses, for example:

\begin{itemize}
\tightlist
\item
  did a potential client respond favourably to a promotional offer?
\item
  is the client satisfied with service provided post-purchase?
\item
  will a company go bankrupt in the next three years?
\item
  did a study participant successfully complete a task?
\end{itemize}
\end{example}

\begin{example}[Binomial distribution]
\protect\hypertarget{exm:binomialdist}{}{\label{exm:binomialdist} \iffalse (Binomial distribution) \fi{} }If the data five the sum of independent Bernoulli events, the number of sucessess \(Y\) out of \(m\) trials is \href{https://en.wikipedia.org/wiki/Binomial_distribution}{binomial}, denoted \(\mathsf{Bin}(m, \pi)\); the mass function of the binomial distribution is
\begin{align*}
\mathsf{Pr}(Y=y) = \binom{m}{y}\pi^y (1-\pi)^{1-y}, \quad y=0, 1.
\end{align*}
The likelihood of a sample from a binomial distribution is (up to a normalizing constant that doesn't depend on \(\pi\)) the same as that of \(m\)independent Bernoulli trials. The expectation of the binomial random variable is \(\mathsf{E}(Y)=m\pi\) and its variance \(\mathsf{Va}(Y)=m\pi(1-\pi)\).

As examples, we could consider the number of successful candidates out of \(m\) who passed their driving license test or the number of customers out of \(m\) total which spent more than 10\$ in a store.
\end{example}

More generally, we can also consider count variables whose realizations are integer-valued, for examples the number of

\begin{itemize}
\tightlist
\item
  insurance claims made by a policyholder over a year,
\item
  purchases made by a client over a month on a website,
\item
  tasks completed by a study participant in a given time frame.
\end{itemize}

\begin{example}[Geometric distribution]
\protect\hypertarget{exm:geomdist}{}{\label{exm:geomdist} \iffalse (Geometric distribution) \fi{} }The \href{https://en.wikipedia.org/wiki/Geometric_distribution}{geometric distribution} is a model describing the number of Bernoulli trials with probability of success \(\pi\) required to obtain a first success. The mass function of \(Y \sim \mathsf{Geo}(\pi)\) is
\begin{align*}
\mathsf{Pr}(Y=y) = \pi (1-\pi)^{y-1}, \quad y=1,2, \ldots
\end{align*}

For example, we could model the numbers of visits for a house on sale before the first offer is made using a geometric distribution.
\end{example}

\begin{example}[Poisson distribution]
\protect\hypertarget{exm:poissondist}{}{\label{exm:poissondist} \iffalse (Poisson distribution) \fi{} }If the probability of success \(\pi\) of a Bernoulli event is small in the sense that \(m\pi \to \lambda\) wen the number of trials \(m\) increases, then the number of success followss approximately a Poisson distribution with mass function
\begin{align*}
\mathsf{Pr}(Y=y) = \frac{\exp(-\lambda)\lambda^y}{\Gamma(y+1)}, \quad y=0, 1, 2, \ldots
\end{align*}
where \(\Gamma(\cdot)\) denotes the gamma function. The parameter \(\lambda\) of the Poisson distribution is both the expectation and the variance of the distribution, meaning \(\mathsf{E}(Y)=\mathsf{Va}(Y)=\lambda\).
\end{example}

\begin{example}[Negative binomial distribution]
\protect\hypertarget{exm:negbindist}{}{\label{exm:negbindist} \iffalse (Negative binomial distribution) \fi{} }The negative binomial distribution arises as a natural generalization of the geometric distribution if we consider the number of Bernoulli trials with probability of success \(\pi\) until we obtain \(m\) success. Let \(Y\) denote the number of failures: the order of success and failure doesn't matter, but for the latest trial which is a success. The mass function is thus
\begin{align*}
\mathsf{Pr}(Y=y)= \binom{m-1+y}{y} \pi^m (1-\pi)^{y}.
\end{align*}

The negative binomial distribution also appears as the unconditional distribution of a two-stage hierarchical gamma-Poisson model, in which the mean of the Poisson distribution is random and follows a gamma distribution. In notation, this is \(Y \mid \Lambda=\lambda \sim \mathsf{Po}(\lambda)\) and \(\Lambda\) follows a gamma distribution with shape \(r\) and scale \(\theta\), whose density is \[f(x) = \theta^{-r}x^{r-1}\exp(-x/\theta)/\Gamma(r).\] The unconditional number of success is then negative binomial.

In the context of generalized linear models, we will employ yet another parametrisation of the distribution, with the mass function
\begin{align*}
\mathsf{Pr}(Y=y)=\frac{\Gamma(y+r)}{\Gamma(y+1)\Gamma(r)} \left(\frac{r}{r + \mu} \right)^{r} \left(\frac{\mu}{r+\mu}\right)^y, y=0, 1, \ldots, \mu,r >0,
\end{align*}
where \(\Gamma\) is the gamma function and the parameter \(r>0\) is not anymore integer valued. The expectation and variance of \(Y\) are
\(\mathsf{E}(Y)=\mu\) et \(\mathsf{Va}(Y)=\mu+k\mu^2\), where \(k=1/r\). The variance of the negative binomial distribution is thus higher than its expectation, which justifies the use of the negative binomial distribution for modelling overdispersion.
\end{example}

\hypertarget{diagramme-qq}{%
\subsection{Quantiles-quantiles plots}\label{diagramme-qq}}

Models are (at best) an approximation of the true data generating mechanism and we will want to ensure that our assumptions are reasonable and the quality of the fit decent. Quantile-quantile plots are graphical goodness-of-fit diagnostics that are based on the following principle: if \(Y\) is a continuous random variable with distribution function \(F\), then the mapping \(F(Y) \sim \mathsf{U}(0,1)\) yields uniform variables. Similarly, the quantile transform applied to a uniform variable provides a mean to simulating samples from \(F\), viz.~\(F^{-1}(U)\). Consider then a random sample of size \(n\) from the uniform distribution ordered from smallest to largest, with \(U_{(1)} \leq \cdots \leq U_{(n)}\). One can show these ranks have marginally a Beta distribution, \(U_{(k)} \sim \mathsf{Beta}(k, n+1-k)\) with expectation \(k/(n+1)\).

In practice, we don't know \(F\) and, even if we did, one would need to estimate the parameters. We consider some estimator \(\widehat{F}\) for the model and apply the inverse transform to an approximate uniform sample \(\{i/(n+1)\}_{i=1}^n\). The quantile-quantile plot shos the data as a function of the (first moment) of the transformed order statistics:

\begin{itemize}
\tightlist
\item
  on the \(x\)-axis, the theoretical quantiles \(\widehat{F}^{-1}\{\mathrm{rank}(y_i)/(n+1)\}\)
\item
  on the \(y\)-axis, the empirical quantiles \(y_i\)
\end{itemize}

If the model is adequate, the ordered values should follow a straight line with unit slope passing through the origin. Whether points fall on a 45 degree line is difficult to judge by eye and so it is advisable to ease the interpretation to subtract the slope: the detrended plot is easier to interpret and was proposed by Tukey (but beware of the scale of the \(y\)-axis!). Figure \ref{fig:diagrammeqq2} shows two representations of the same data using simulated samples from a standard normal distribution.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{MATH60604A_Statistical_modelling_files/figure-latex/diagrammeqq2-1} 

}

\caption{Normal quantile-quantile plot (left) and detrended version (Tukey's) of the same plot (right).}\label{fig:diagrammeqq2}
\end{figure}

Even if we knew the true distribution of the data, the sample variability makes it very difficult to spot if deviations from the model are abnormal or compatible with the model. A simple point estimate with no uncertainty measure can lead to wrong conclusions. As such, we add approximate pointwise or simultaneous confidence intervals. The simplest way to do this is by simulation (using a parametric bootstrap), by repeating the following steps \(B\) times:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  simulate a (bootstrap) sample \(\{Y^{(b)}_{i}\} (i=1,\ldots, n)\) from \(\widehat{F}\)
\item
  re-estimate the parameters of \(F\) to obtain \(\widehat{F}_{(b)}\)
\item
  calculate and save the plotting positions \(\widehat{F}^{-1}_{(b)}\{i/(n+1)\}\).
\end{enumerate}

The result of this operation is an \(n \times B\) matrix of simulated data. We obtain a symmetric (\(1-\alpha\)) confidence interval by keeping the empirical quantile of order \(\alpha/2\) and \(1-\alpha/2\) from each row. The number \(B\) should be larger than 999, say, and be chosen so that \(B/alpha\) is an integer.

For the pointwise interval, each order statistic from the sample is a statistic and so the probability of any single one falling outside the confidence interval is approximately \(\alpha\). However, order statistics are not independent (they are ordered), so its common to see neighboring points falling outside of their respective intervals. {[}It is also possible to use the bootstrap samples to derive an (approximate) simultaneous confidence intervals, in which we expected values to fall \(100(1-\alpha)\)\% of the time inside the bands in repeated samples; \href{https://lbelzile.github.io/lineaRmodels/qqplot.html}{see Section 4.4.3 of these course notes}. The intervals shown in Figure \ref{fig:diagrammeqq2} are pointwise and derived (magically) using a simple function. The uniform order statistics have larger variability as we move away from 0.5, but the uncertainty in the quantile-quantile plot largely depends on \(F\).

Interpretation of quantile-quantile plots requires some experience: \href{https://stats.stackexchange.com/questions/101274/how-to-interpret-a-qq-plot/101290\#101290}{this post by \emph{Glen\_b} on StackOverflow} nicely summarizes what can be detected (or not) from them.

\hypertarget{law-large-numbers}{%
\section{Laws of large numbers}\label{law-large-numbers}}

An estimator for a parameter \(\theta\) is \textbf{consistent} if the value obtained as the sample size increases (to infinity) converges to the true value of \(\theta\). Mathematically speaking, this translates into convergence in probability, meaning \(\hat{\theta} \stackrel{\mathsf{Pr}}{\to} \theta\). In common language, we say that the probability that \(\hat{\theta}\) and \(\theta\) differ becomes negligible as \(n\) gets large.

Consistency is the \emph{a minima} requirement for an estimator: when we collect more information, we should approach the truth. The law of large number states that the sample mean of \(n\) (independent) observations with common mean \(\mu\), say \(\overline{Y}_n\), converges to \(\mu\), denoted \(\overline{Y}_n \rightarrow \mu\). Roughly speaking, our approximation becomes less variable and asymptotically unbiased as the sample size (and thus the quantity of information available for the parameter) increases. The law of large number is featured in Monte Carlo experiments: we can approximate the expectation of some (complicated) function \(g(x)\) by simulating repeatedly independent draws from \(Y\) and calculating the sample mean \(n^{-1} \sum_{i=1}^n g(Y_i)\).

If the law of large number tells us what happens in the limit (we get a single numerical value), the result doesn't contain information about the rate of convergence and the uncertainty at finite levels.

\hypertarget{CLT}{%
\section{Central Limit Theorem}\label{CLT}}

The central limit theorem is perhaps the flagship result of probability theory: for a random sample of size \(n\) with (independent) random variables whose expectation is \(\mu\) and variance \(\sigma^2\), then the sample mean converges to \(\mu\), but

\begin{itemize}
\tightlist
\item
  the estimator \(\overline{Y}\) is centered around \(\mu\),
\item
  the standard error is \(\sigma/\sqrt{n}\); the rate of convergence is thus \(\sqrt{n}\). For a sample of size 100, the standard error of the sample mean will be 10 times smaller than that of the underlying random variable.
\item
  the sample mean, once properly scaled, follows approximately a normal distribution
\end{itemize}

Mathematically, the central limit theorem states \(\sqrt{n}(\overline{Y}-\mu) \stackrel{\mathrm{d}}{\rightarrow} \mathsf{No}(0, \sigma^2)\). If \(n\) is large (a rule of thumb is \(n>30\), but this depends on the underlying distribution of \(Y\)), then \(\overline{Y} \stackrel{\cdot}{\sim} \mathsf{No}(\mu, \sigma^2/n)\).

How do we make sense of this result? Let us consider the mean travel time of high speed Spanish trains (AVE) between Madrid and Barcelona that are operated by Renfe.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{MATH60604A_Statistical_modelling_files/figure-latex/renfeclt-1} 

}

\caption{Empirical distribution of travel times of high speed trains.}\label{fig:renfeclt}
\end{figure}

Our exploratory data analysis showed previously that the duration is the one advertised on the ticket: there are only 15 unique travel time. Based on 9603 observations, we estimate the mean travel time to be 170 minutes and 41 seconds. Figure \ref{fig:renfeclt} shows the empirical distribution of the data.

Consider now samples of size \(n=10\), drawn repeatedly from the population: in the first sample, the sample mean is 170.9 minutes, whereas we get an estimate of 164.5 minutes in our second , 172.3 minutes in the third, etc.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{MATH60604A_Statistical_modelling_files/figure-latex/renfemeanCLT-1} 

}

\caption{Graphical representation of the central limit theorem. The upper left panel shows a sample of 20 observations with its sample mean (vertical red). The three other panels show the histograms of the sample mean from repeated samples of size 5 (top right), 20 (bottom left) and 20, 50 and 100 overlaid, with the density approximation provided by the central limit theorem.}\label{fig:renfemeanCLT}
\end{figure}

We draw \(B=1000\) different samples, each of size \(n=5\), from two millions records, and calculate the sample mean in each of them. The top right panel of \ref{fig:renfemeanCLT} shows the result for \(n=5\), but also for \(n=20\) (bottom left). The last graph of Figure \ref{fig:renfemeanCLT} shows the impact of the increase in sample size: whereas the normal approximation is okay-ish for \(n=5\), it is indistinguishable from the normal approximation for \(n=20\). As \(n\) increases and the sample size gets bigger, the quality of the approximation improves and the curve becomes more concentrated around the true mean. Even if the distribution of the travel time is discrete, the mean is approximately normal.

We considered a single distribution in the example, but you could play with other distributions and vary the sample size to see when the central limit theorem kicks in usng this \href{http://195.134.76.37/applets/AppletCentralLimit/Appl_CentralLimit2.html}{applet}.

The central limit theorem underlies why scaled test statistics which have sample mean zero and sample variance 1 have a standard null distribution in large sample: this is what guarantees the validity of our inference!

  \bibliography{book.bib,packages.bib,notes60604.bib}

\end{document}
